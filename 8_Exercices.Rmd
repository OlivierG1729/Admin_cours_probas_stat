
---
output:
  pdf_document: default
  html_document: default
---
# Exercices

## Dénombrement et probabilités

\noindent On commence par quelques exercices de dénombrement, très classiques lorqu'on découvre le calcul des probabilités. Si ce type d'exercices a en soi très peu de chances de tomber au concours, on peut toujours imaginer un sujet dans lequel quelques questions isolées nécessitent des réflexes d'analyse combinatoire. 

:::: {.methbox .meth data-latex="important"}
**Exercice 2.1.** Dans un jeu de 52 cartes, on tire 5 cartes sans remise. Quelle est la probabilité de tirer :

\noindent 

**1.** 5 coeurs ?

\noindent 

**2.** 2 piques et 3 coeurs ?

\noindent 

**3.** 5 trèfles ou 5 coeurs ?

\noindent 

**4.** 5 cartes de la même couleur (pique, coeur, carreau, trèfle) ?

\noindent 

**5.** 3 cartes d’une couleur et 2 d’une autre ?

\noindent 

**6.** les 4 as et une autre carte ?
::::

\noindent

**Solution.** Le nombre de main à $5$ cartes dans un jeu de $52$ cartes est $\binom{52}{5}$.

\noindent

**1.** Il y a $13$ coeurs dans un jeu de $52$ cartes, donc il y a $\binom{13}{5}$ mains à $5$ coeurs. La probabilité de tirer $5$ coeurs est donc $\frac{\binom{13}{5}}{\binom{52}{5}}$.

\noindent

**2.** Le nombre de façons de tirer $2$ piques et $3$ coeurs est $\binom{8}{2}\times\binom{8}{3}$. La probabilité d'un tel tirage est donc $\frac{\binom{8}{2}\times\binom{8}{3}}{\binom{52}{5}}$.

\noindent 

**3.** Les événements *obtenir 5 trèfles* et *obtenir 5 coeurs* étant incompatibles, la probabilité que l'un ou l'autre soit réalisé est la somme des probabilités, soit $\frac{\binom{13}{5}}{\binom{52}{5}}+\frac{\binom{13}{5}}{\binom{52}{5}}=\frac{2\,\binom{13}{5}}{\binom{52}{5}}$.

\noindent

**4.** Avec le même raisonnement qu'à la question 3., la probabilité d'avoir $5$ cartes de la même couleur est $\frac{4\,\binom{13}{5}}{\binom{52}{5}}$.

\noindent

**5.** 

- choisir deux couleurs parmi quatre : $A_4^2$ possibilités (l'ordre des couleurs compte, puisque les nombres de cartes de chaque couleur sont différents) ;

- pour une couleur choisir $3$ cartes : $\binom{13}{3}$ possibilités ;

- pour l'autre couleur choisir $2$ cartes : $\binom{13}{2}$ possibilités.

\noindent Donc, en vertu du principe mulitplicatif, il y a $\binom{4}{2}\times\binom{13}{3}\times\binom{13}{2}$ mains de $5$ cartes, avec $3$ cartes d'une couleur et $2$ cartes d'une autre. La probabilité d'un tel tirage est donc égale à $\frac{\binom{4}{2}\times\binom{13}{3}\times\binom{13}{2}}{\binom{52}{5}}$.

\noindent 

**6.** 

- choisir les quatre as : une seule possibilité ;

- choisir n'importe quelle carte parmi les $48$ cartes restantes : $48$ possibilités.

Donc, la probabilité de tirer les quatre as et une autre carte (quelconque) est égale à $\frac{48}{\binom{13}{5}}$.



:::: {.methbox .meth data-latex="important"}
**Exercice 2.2.** On s'intéresse à une famille et on considère les événements suivants :

- $A$ : la famille a des enfants des deux sexes
- $B$ : la famille a au plus un garçon

\noindent 

**1.** Démontrer que si la famille a deux enfants, alors $A$ et $B$ sont dépendants.

\noindent

**2.** Démontrer que si la famille a trois enfants, alors $A$ et $B$ sont indépendants. 
::::

\noindent

**Solution.** On suppose que la probabilité d'avoir une fille et celle d'avoir un garçon sont égales à $\frac{1}{2}$. On note 

- $G_i$ l'événement : l'enfant numéro $i$ est un garçon ;
- $F_i$ l'événement : l'enfant numéro $i$ est une fille.


\noindent 

**1.** On vérifie facilement que $A\cap B=A=(F_1\cap G_2)\cup (F_2\cap G_1)$. Il s'agit d'une réunion disjointe de deux événements de probabilité commune $\frac{1}{4}$, donc $\mathbb{P}(A\cap B)=\mathbb{P}(A)=\frac{1}{2}$. Comme $\mathbb{P}(B)>0$, on en déduit que $\mathbb{P}(A\cap B)\neq\mathbb{P}(A)\mathbb{P}(B)$, et donc $A$ et $B$ sont dépendants.

**2.** $\overline{A}=(F_1\cap F_2\cap F_3)\cup(G_1\cap G_2\cap G_3)$ donc

$$\mathbb{P}(A)=1-\mathbb{P}(F_1\cap F_2\cap F_3)-\mathbb{P}(G_1\cap G_2\cap G_3)=1-\frac{1}{8}-\frac{1}{8}=\frac{3}{4}$$

\noindent $\overline{B}=(F_1\cap F_2\cap F_3)\cup(G_1\cap F_2\cap F_3)\cup (G_1\cap F_2\cap F_3)\cup (G_1\cap F_2\cap F_3)$.Il s'agit d'une réunion disjointe de quatre événements de probabilité commune $\frac{1}{8}$ donc $\mathbb{P}(\overline{B})=\frac{4}{8}=\frac{1}{2}$. D'où $\mathbb{P}(B)=\frac{1}{2}$.

\noindent $A\cap B=(G_1\cap F_2\cap F_3)\cup (F_1\cap G_2\cap F_3)\cup (F_1\cap F_2\cap G_3)$. Il s'agit d'une réunion disjointe de trois événements de probabilité commune $\frac{1}{8}$. Donc, $\mathbb{P}(A\cap B)=\frac{3}{8}=\mathbb{P}(A)\mathbb{P}(B)$. Les événements $A$ et $B$ sont donc indépendants.




:::: {.methbox .meth data-latex="important"}
**Exercice 2.3 (coefficients binomiaux). 1.** Soit $1\leq p\leq n$. On considère $n$ boules et deux boîtes $A$ et $B$ vides. Un échantillon est constitué d'une boule que l'on met dans la boîte $A$ et $p-1$ boules que l'on met dans la boîte $B$. En dénombrant de deux façons différentes l'ensemble de tous les échantillons que l'on peut ainsi obtenir, établir la formule 

$$n\binom{n-1}{p-1}=p\binom{n}{p}$$
\noindent Démontrer cette formule par le calcul.

\noindent

**2.** Démontrer que pour $1\leq p\leq n$ on a la formule

$$\sum\limits_{k=p}^n\binom{k}{p}=\binom{n+1}{p+1}$$

\noindent 

**3. (Extension de la formule de Pascal).** Soient $m, p, q$ des entiers naturels tels que $q\leq p\leq m$. Démontrer par un dénombrement la formule :

$$\sum\limits_{j=0}^q \binom{q}{j}\binom{m-q}{p-j}=\binom{m}{p}$$

**4.** Soit $n$ un entier tel que $n\geq 1$. Démontrer par un dénombrement la formule :

$$\sum\limits_{k=0}^n \binom{n}{k}^2=\binom{2n}{n}$$
::::

\noindent

**Solution. 1.**

\noindent

**Première méthode de dénombrement des échantillons :**

- choisir une boule à mettre dans la boîte $A$ : $n$ choix possibles ;
- choisir $p-1$ boules parmi les $n-1$ boules restantes, à mettre dans la boîte $B$ : $\binom{n-1}{p-1}$ choix possibles.

\noindent En appliquant le principe multiplicatif, on en déduit qu'il y a donc $n\times\binom{n-1}{p-1}$ échantillons.

\noindent

**Deuxième méthode de dénombrement des échantillons :**

- choisir $p$ boules parmi $n$ boules à mettre dans l'ensemble des deux boîtes $A$ et $B$ : $\binom{n}{p}$ choix possibles ;
- choisir une boule parmi ces $p$ boules à mettre dans la boîte $A$ (les autres vont ditrectement dans la boîte $B$) : $p$ choix possibles.

\noindent En appliquant le principe multiplicatif, on en déduit qu'il y a donc $p\times\binom{n}{p}$ échantillons.

\noindent Ces deux méthodes comptent le même nombre d'échantillons. On en déduit que

$$n\times\binom{n-1}{p-1}=p\times\binom{n}{p}$$

\noindent

**Approche calculatoire.** On peut démontrer cette formule par le calcul (c'est plus simple mais moins élégant) :

\begin{align}
n\times\binom{n-1}{p-1} &= n\frac{(n-1)!}{(p-1)!(n-p)!} \\
&= \frac{n!}{(p-1)!(n-p)!} \\
&=p\frac{n!}{p!(n-p)!} \\
&=p\times\binom{n}{p} \\
\end{align}

\noindent 

**2.** On choisit $p+1$ nombres parmi les entiers $1,2,\dots,n+1$ : $\binom{n+1}{p+1}$ choix possibles.

\noindent On peut aussi dénombrer le nombre de choix possibles de la manière suivante :

- fixer le maximum $k$ des $p+1$ nombres tirés : ce maximum est nécessairement compris entre $p+1$ et $n+1$ (inclus) ;
- choisir les $p$ nombres restants parmi $1,2,\dots, k-1$ : à $k$ fixé, on a $\binom{p}{k-1}$ choix possibles.

On en déduit que le nombre de façons de choisir $p+1$ nombres parmi les entiers $1,2,\dots n+1$ est égal à  $\sum\limits_{k=p+1}^{n+1}\binom{p}{k-1}=\sum\limits_{k=p}^n\binom{p}{k}$. 

\noindent Ces deux méthodes de dénombrement doivent mener au même résultat, donc 

$$\sum\limits_{k=p}^n\binom{p}{k}=\binom{n+1}{p+1}$$
\noindent

**3.** On choisit $p$ entiers parmi les entiers $1,2,\dots,m$ : $\binom{m}{p}$ choix possbles.

\noindent Pour choisir ces $p$ entiers on peut aussi procéder de la manière suivante :

- on choisit $j$ entiers parmi les entiers $1,2,\dots,q$. Cela n'est évidemment possible que pour $0\leq j\leq q$, et pour chaque entier $j$ vérifiant cet encadrement on a $\binom{q}{j}$ façons différentes de choisir ces $j$ entiers.

- on complète notre sélection en choisissant les $p-j$ entiers restants parmi $q+1,q+2,\dots, m$ : $\binom{m-q}{p-j}$ choix possibles.

\noindent En vertu du principe multiplicatif, à $j$ fixé compris entre $0$ et $q$ inclus, on a donc $\binom{q}{j}\binom{m-q}{p-j}$ façons de choisir $p$ entiers parmi $1,2,\dots m$ de sorte que $j$ de ces entiers soient compris entre $1$ et $q$ et les $p-j$ entiers restants soient compris entre $q+1$ et $m$.

\noindent Par ailleurs, l'entier $j$ peut prendre n'importe quelle valeur comprise entre $0$ et $q$, et pour deux valeurs de $j$ distinctes on a deux sélections d'entiers distinctes. En vertu du principe additif, le nombre de façons de choisir $p$ entiers parmi $1,2,\dots, q$ est donc finalement égal à $\sum\limits_{j=0}^q \binom{q}{j}\binom{m-q}{p-j}$.

\noindent On a dénombré de deux façons différentes le même ensemble, ces deux approches doivent aboutir au même résultat. On a donc

$$\sum\limits_{j=0}^q \binom{q}{j}\binom{m-q}{p-j}=\binom{m}{p}$$

\noindent 

**4.** On applique la formule de la question précédente avec $q=n$, $p=n$ et $m=2n$ :

$$\sum\limits_{k=0}^n \binom{n}{k}\binom{n}{n-k}=\binom{2n}{n}$$

\noindent Par la formule de symétrie des coefficients binomiaux $\binom{n}{n-k}=\binom{n}{k}$, on en déduit que

$$\sum\limits_{k=0}^n \binom{n}{k}^2=\binom{2n}{n}$$


:::: {.methbox .meth data-latex="important"}
**Exercice 2.4 (Interne, 2023). 1.** Montrer que pour tout entier naturel $n$, on a $\sum\limits_{k=0}^n\binom{2n+1}{k}=\sum\limits_{k=0}^n\binom{2n+1}{n+k+1}$, et calculer la valeur commune de ces sommes.

\noindent

**2.** Montrer que pour tout entier naturel non nul $n$, on a $\sum\limits_{k=0}^{n-1}\binom{2n}{k}=\sum\limits_{k=1}^n\binom{2n}{n+k}=2^{2n-1}-\frac{1}{2}\binom{2n}{n}$.

\noindent

**3.** Déduire de ce qui précède le résultat suivant : $\forall n\in\mathbb{N}^{*}, \sum\limits_{k=1}^n k\binom{2n}{n+k}=\frac{n}{2}\binom{2n}{n}$.
::::

\noindent

**Solution. 1.** 

\begin{align}
\sum\limits_{k=0}^n\binom{2n+1}{k}&=\sum\limits_{k=0}^n\binom{2n+1}{n-k} \\
& \text{(changement de variable } k\mapsto n-k \text{)} \\
&=\sum\limits_{k=0}^n\binom{2n+1}{(2n+1)-(n-k)} \\
& \text{(symétrie des coefficients binomiaux : } \binom{n}{k}=\binom{n}{n-k} \text{)} \\
&=\sum\limits_{k=0}^n\binom{2n+1}{n+k+1} \\
\end{align}

\noindent On pose $S_n:=\sum\limits_{k=0}^n\binom{2n+1}{k}$. On a donc

\begin{align}
2S_n&=\sum\limits_{k=0}^n\binom{2n+1}{k}+\sum\limits_{k=0}^n\binom{2n+1}{n+k+1} \\
&=\sum\limits_{k=0}^{2n+1}\binom{2n+1}{k} \\
&=2^{2n+1} \\
\end{align}

\noindent D'où :

$$S_n=2^{2n}$$

\noindent

**2.** L'égalité $\sum\limits_{k=0}^{n-1}\binom{2n}{k}=\sum\limits_{k=1}^n\binom{2n}{n+k}$ se démontre de façon tout à fait analogue. On note $S'_n$ la valeur commune de ces deux sommes. Alors

\begin{align}
2S'_n &= \sum\limits_{k=0}^{n-1}\binom{2n}{k}+\sum\limits_{k=1}^n\binom{2n}{n+k} \\
&= \sum\limits_{k=0}^{2n}\binom{2n}{k}-\binom{2n}{n} \\
&= 2^{2n}-\binom{2n}{n} \\
\end{align}

\noindent On en déduit que

$$S'_n=2^{2n-1}-\frac{1}{2}\binom{2n}{n}$$

\noindent

**3.** Pour tout réel $x$, on a d'après la formule du binôme de Newton $(1+x)^{2n}=\sum\limits_{k=0}^{2n}\binom{2n}{k}x^k$. En dérivant par rapport à $x$ on obtient donc

$$2n(1+x)^{2n-1}=\sum\limits_{k=1}^{2n}k\binom{2n}{k}x^{k-1}$$

\noindent On évalue cette égalité en $x=1$ :

$$n2^{2n}=\sum\limits_{k=1}^{2n}k\binom{2n}{k}$$

\noindent D'où

\begin{align}
n2^{2n}&=\sum\limits_{k=1}^{n}k\binom{2n}{k}+\sum\limits_{k=n+1}^{2n}k\binom{2n}{k} \\
&=\sum\limits_{k=1}^{n}k\binom{2n}{k}+\sum\limits_{k=1}^{n}(n+k)\binom{2n}{n+k} \\
&=\sum\limits_{k=1}^{n}k\binom{2n}{k}+n\sum\limits_{k=1}^{n}\binom{2n}{n+k}+\sum\limits_{k=1}^{n}k\binom{2n}{n+k} \\
&=\sum\limits_{k=1}^{n}k\binom{2n}{k}+n\left(2^{2n-1}-\frac{1}{2}\binom{2n}{n}\right)+\sum\limits_{k=1}^{n}k\binom{2n}{n+k}
\end{align}

\noindent d'après 2.

\noindent Par ailleurs, on a facilement

$$k\binom{2n}{k}=2n\binom{2n-1}{k-1}$$

\noindent D'où

\begin{align}
\sum\limits_{k=1}^n k\binom{2n}{k}&=2n\sum\limits_{k=1}^n
\binom{2n-1}{k-1} \\
&= 2n\sum\limits_{k=0}^{n-1}\binom{2n-1}{k} \\
&= n2^{2n-1} \\
\end{align}

\noindent Ainsi

$$n2^{2n}=n2^{2n-1}+n2^{2n-1}-\frac{n}{2}\binom{2n}{n}+\sum\limits_{k=1}^n k\binom{2n}{n+k}$$

\noindent et donc

$$\sum\limits_{k=1}^n k\binom{2n}{n+k}=\frac{n}{2}\binom{2n}{n}$$


\noindent 

:::: {.methbox .meth data-latex="important"}
**Exercice 2.5 (Interne, 2012).** On considère trois événements $A, B, C$ d'un espace probabilisé tels que

$$\mathbb{P}(A)=0,4 \hspace{1cm} \mathbb{P}(B)=0,2 \hspace{1cm} \mathbb{P}(C)=0,3$$
$$\mathbb{P}(A\cap B)=0,2 \hspace{1cm} \mathbb{P}(A\cap C)=0,12 \hspace{1cm} \mathbb{P}(B\cap C)=0$$      


\noindent

**1.** Parmi les événements $A, B$ et $C$, quels sont les deux événements indépendants ?

\noindent

**2.** La réalisation de l'un des trois événements implique la réalisation de l'un des deux autres. De quel événement s'agit-il ?

\noindent

**3.** Déterminer la probabilité $\mathbb{P}(A\cap B\cap C)$.

\noindent

**4.** Calculer la probabilité qu'au moins l'un des événements $A, B, C$ se réalise.

\noindent

**5.** On considère maintenant l'expérience aléatoire pour laquelle les événements $A, B$ ou $C$, et seulement eux, peuvent se réaliser. 

\noindent

On note $X$ le nombre d'événements qui se sont réalisés, parmi $A, B, C$. Pour quelles valeurs de $i$ a-t-on $\mathbb{P}(X=i)>0$ ? Pour chacune de ces valeurs $i$, déterminer $\mathbb{P}(X=i)$. 
::::

\

\noindent

**Solution. 1.** $\mathbb{P}(A\cap C)=\mathbb{P}(A)\mathbb{P}(C)=0,12$, donc $A$ et $C$ sont indépendants.

\noindent

**2.** $\mathbb{P}(A\vert B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}=\frac{0,2}{0,2}=1$, donc sachant que $B$ est réalisé, $A$ est réalisé avec probabilité $1$.

\noindent

**3.** $A\cap B\cap C\subset B\cap C$, et $\mathbb{P}(B\cap C)=0$. D'après la propriété de croissance des probabilités, on en déduit que $\mathbb{P}(A\cap B\cap C)=0$.

\noindent

**4.** On veut calculer $\mathbb{P}(A\cup B\cup C)$. On peut utiliser la formule de Poincaré :

\begin{align}
\mathbb{P}(A\cup B\cup C)&=\mathbb{P}(A)+\mathbb{P}(B)+\mathbb{P}(C)-\mathbb{P}(A\cap B)-\mathbb{P}(A\cap C)-\mathbb{P}(B\cap C)+\mathbb{P}(A\cap B\cap C) \\
&=0,4+0,2+0,3-0,2-0,12-0+0 \\
&=0,58 \\
\end{align}

\noindent

**5.** Remarquons d'abord qu'*a priori* $X\in\{0,1,2,3\}$. Par ailleurs :

$$A\cup B\cup C=(X\geq 1)$$
$$A\cap B\cap C=(X=3)$$

\noindent On en déduit que 

$$\mathbb{P}(X=0\vert A\cup B\cup C)=\mathbb{P}(X=0\vert X\geq 1)=0$$

\noindent et, d'après 3 :

$$\mathbb{P}(X=3\vert A\cup B\cup C)=\frac{\mathbb{P}(X=3, X\geq 1)}{\mathbb{P}(X\geq 1)}=\frac{\mathbb{P}(X=3)}{\mathbb{P}(X\geq 1)}=\frac{\mathbb{P}(A\cap B\cap C)}{\mathbb{P}(X\geq 1)}=0$$

\noindent De plus

\begin{align}
\mathbb{P}(X=2\vert A\cup B\cup C)&=\frac{\mathbb{P}(X=2\vert  X\geq 1)}{\mathbb{P}(X\geq 1)} \\
&=\frac{\mathbb{P}(X=2, X\geq 1)}{\mathbb{P}(X\geq 1)} \\
&=\frac{\mathbb{P}(X=2)}{\mathbb{P}(X\geq 1)} \text{ ;  car } (X=2)\subset (X\geq 1) \\
&=\frac{\mathbb{P}(A\cap B\cap\overline{C})+\mathbb{P}(A\cap \overline{B}\cap C)+\mathbb{P}(\overline{A}\cap B\cap C)}{\mathbb{P}(X\geq 1)} \\
&=\frac{\mathbb{P}(A\cap B)+\mathbb{P}(A\cap C)+\mathbb{P}(B\cap C)}{\mathbb{P}(X\geq 1)} \text{ ; car } \mathbb{P}(A\cap B\cap C)=0 \\
&=\frac{0,32}{0,58} \\
&=\frac{16}{29} \\
\end{align}

\noindent On en déduit enfin que

\begin{align}
\mathbb{P}(X=1\vert A\cup B\cup C)&=\mathbb{P}(X\geq 1\vert A\cup B\cup C)-\mathbb{P}(X=2\vert A\cup B\cup C)-\mathbb{P}(X=3\vert A\cup B\cup C) \\
&= \frac{0,58}{0,29}-\frac{0,32}{0,29}-0 \\
&= \frac{0,26}{0,39} \\
&=\frac{2}{3} \\
\end{align}

\noindent 

**Commentaire sur cette dernière question.** *Il s'agit typiquement du genre de question qui peut facilement devenir assez chronophage si l'on n'est pas un peu astucieux. Il y a plusieurs astuces / points à ne pas manquer pour s'économiser du temps de calcul :*

- *bien traduire $(X\geq 1)$ par $A\cup B\cup C$ et $X=3$ par $A\cap B\cap C$. *

- *constater que le calcul de $\mathbb{P}(X=1\vert A\cup B\cup C)$ et $\mathbb{P}(X=2\vert A\cup B\cup C)$ se ramène en fait au calcul d'un seul de ces deux termes. Pour voir cela, il faut penser à utiliser l'égalité $\mathbb{P}(X\geq 1\vert A\cup B\cup C)=\mathbb{P}(X=1\vert A\cup B\cup C)+\mathbb{P}(X=2\vert A\cup B\cup C)$ qui résulte de $\mathbb{P}(X=3\vert A\cup B\cup C)=0$.*

- *remarquer que $\mathbb{P}(X=1\vert A\cup B\cup C)$ est plus rapide à calculer que $\mathbb{P}(X=2\vert A\cup B\cup C)$. Pour s'en convaincre, il suffit de calculer directement $\mathbb{P}(X=2\vert A\cup B\cup C)$ : cela n'a rien de difficile, mais c'est plus long.* 



:::: {.methbox .meth data-latex="important"}
**Exercice 2.6 (Interne, 2011). 1.** Soit $\Omega$ un ensemble fondamental tel que $\Omega=A_1\cup A_2$, où les $A_i$ ($i=1,2$) sont des événements donnés. On suppose $\Omega$ muni d'une probabilité $\mathbb{P}$ telle que $\mathbb{P}(A_1)=0,8$ et $\mathbb{P}(A_2)=0,5$. Donner la valeur de $\mathbb{P}(A_1\cap A_2)$.

\noindent

**2.** Un individu oublie sa carte bancaire 10 \% du temps, son chéquier 5 \% du temps et ces deux instruments de paiement 2 \% du temps. Calculer :

- la probabilité pour que, à un moment donné, il détienne ces deux instruments sur lui ;
- la probabilité qu'il trouve sa carte bancaire dans ses poches sachant qu'il a déjà trouvé son chéquier.

\noindent

**3.** Une automobile est classée comme étant "de luxe" si son moteur possède au moins 6 cylindres ou une cylindrée d'au moins 3 litres. On observe que 15 \% des véhicules ont au moins 6 cylindres, que 10 \% au moins 3 litres de cylindrées et que 80 \% des véhicules d'au moins 3 litres ont au moins 6 cylindres. Calculer la proportion de véhicules de luxe dans le parc automobile.

\noindent

**4.** Soient $\Omega$ un ensemble fondamental et $A$ et $B$ deux parties de $\Omega$. Montrer que

$$\mathbb{P}(A\cap B)\leq\mathbb{P}(A)\leq\mathbb{P}(A\cup B)\leq\mathbb{P}(A)+\mathbb{P}(B)$$

\noindent

**5.** Soient $A$ et $B$ deux événements indépendants en probabilité et tels que $\mathbb{P}(B)=2\,\mathbb{P}(A)$ et $\mathbb{P}(A\cup B)=5/8$. Calculer $\mathbb{P}(A)$ et $\mathbb{P}(B)$.

\noindent

**6.** Chaque jour ouvrable de la semaine, le professeur Statys reçoit du courrier au collège avec une probabilité de $1/3$. Par ailleurs, il a annoncé qu'il viendra aujourd'hui avec une probabilité de $2/5$. Or, on observe que sa boite aux lettres est vide. Sachant cela, calculer la probabilité que Statys soit venu aujourd'hui.

\noindent

**7.a.** Démontrer que l'on peut généraliser (une récurrence est indiquée) la formule de base

$$\mathbb{P}(A_1\cup A_2)=\mathbb{P}(A_1)+\mathbb{P}(A_2)-\mathbb{P}(A_1\cap A_2)$$

\noindent au cas de $N$ événements $A_n$ ($n=1\dots N$), autrement dit

\begin{align}
\mathbb{P}(A_1\cup\dots A_n)&=\sum\limits_{n=1}^N\mathbb{P}(A_n)-\sum\limits_{i<j}\mathbb{P}(A_i\cap A_j)+\sum\limits_{i<j<k}\mathbb{P}(A_i\cap A_j\cap A_k) \\

& +\dots+(-1)^{N-1}\mathbb{P}\left(\bigcap\limits_{n=1}^N A_n\right) \\
\end{align}

\noindent 

**b.** Que se passe-t-il :

- si la suite $(A_n)_{n=1\dots N}$ est emboitée de façon décroissante ($A_1\supset A_2\supset\dots\supset A_N$) ?

- si la suite $(A_n)_{n=1\dots N}$ est emboitée de façon croissante ($A_1\subset A_2\subset\dots\subset A_N$) ?
::::

\noindent

**Solution. 1.** $\mathbb{P}(A_1\cup A_2)=\mathbb{P}(A_1)+\mathbb{P}(A_2)-\mathbb{P}(A_1\cap A_2)=0,8+0,5-1=0,3$.

\noindent

**2.** On note $A$ et $B$ les événements

- $A$ : l'individu a sa carte bancaire
- $B$ : l'individu a son chéquier

\noindent On doit donc déterminer $\mathbb{P}(A\cap B)$ et $\mathbb{P}(A\vert B)$. On a :

\begin{align}
\mathbb{P}(A\cap B)&=1-\mathbb{P}(\overline{A}\cup\overline{B}) \\
&=1-\left(\mathbb{P}(\overline{A})+\mathbb{P}(\overline{B})-\mathbb{P}(\overline{A}\cap\overline{B})\right) \\
&=0,87
\end{align}

\noindent et 

\begin{align}
\mathbb{P}(A\vert B)&=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)} \\
&= \frac{\mathbb{P}(A\cap B)}{1-\mathbb{P}(\overline{B})} \\
&= \frac{0,87}{1-0,05} \\
&\approx 0,92 \\
\end{align}

\noindent 

**3.** On note :

- $(N\geq 6)$ l'événement : la voiture a au moins $6$ cylindres ;
- $(C\geq 3)$ l'événement : la voiture a une cylindrée d'au moins $3$ litres ;
- $L$ l'événement : la voiture est un véhicule de luxe.

\noindent On a donc $L=(N\geq 6)\cup (C\geq 3)$, et 

\begin{align}
\mathbb{P}(L)&=\mathbb{P}\left((N\geq 6)\cup (C\geq 3)\right) \\
&= \mathbb{P}(N\geq 6)+\mathbb{P}(C\geq 3)-\mathbb{P}(N\geq 6, C\geq 3) \\
&= \mathbb{P}(N\geq 6)+\mathbb{P}(C\geq 3)-\mathbb{P}(N\geq 6\vert C\geq 3)\mathbb{P}(C\geq 3) \\
&= 0,15+0,10-0,80\times 0,10 \\
&= 0,17 \\
\end{align} 

\noindent La proportion de véhicules de luxe dans le parc automobile est donc égale à $17\,\%$.

\noindent

**4.** $(A\cap B)\subset A\subset (A\cup B)$ donc $\mathbb{P}(A\cap B)\leq\mathbb{P}(A)\leq\mathbb{P}(A\cup B)$. 

\noindent De plus, $\mathbb{P}(A\cup B)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B)\leq\mathbb{P}(A)+\mathbb{P}(B)$, d'où le résultat.

\noindent 

**5.** Par indépendance de $A$ et $B$ et étant donné la relation $\mathbb{P}(B)=2\mathbb{P}(A)$, on a 

\begin{align}
\frac{5}{8}&= \mathbb{P}(A\cup B) \\
&=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B) \\
&=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A)\mathbb{P}(B) \\
&=3\mathbb{P}(A)-2\mathbb{P}(A)^2 \\
\end{align}

\noindent Posons $x=\mathbb{P}(A)$. Alors, $x$ est solution de l'équation

$$3x-2x^2=\frac{5}{8}$$
\noindent soit

$$16x^2-24x+5=0$$

\noindent Il s'agit d'une équation du second degré, qui admet deux racines réelles :

$$x_1=\frac{1}{4} \text{ et } x_2=\frac{5}{4}$$
\noindent Comme $\frac{5}{4}>1$, on en déduit que $\mathbb{P}(A)=\frac{1}{4}$, et donc $\mathbb{P}(B)=\frac{1}{2}$.

\noindent

**6.** La question a été posée telle quelle lors des écrits du concours interne de 2011. Toutefois, l'énoncé ne semble pas très clair, et en l'état il semble difficile de répondre. 

\noindent On peut commencer par noter $V$ et $C$ les événements

- $V$ : M.Statys vient au collège ;
- $C$ : M.Statys reçoit du courrier.

\noindent On veut calculer $\mathbb{P}(V\vert \overline{C})$ :

\begin{align}
\mathbb{P}(V\vert\overline{C})&=\frac{\mathbb{P}(V\cap\overline{C})}{\mathbb{P}(\overline{C})} \\
&=\frac{\mathbb{P}(V)-\mathbb{P}(V\cap C)}{1-\mathbb{P}(C)} \\
\end{align}

\noindent On connaît $\mathbb{P}(V)$ et $\mathbb{P}(C)$, mais pas $\mathbb{P}(V\cap C)$. Cette probabilité n'est pas donnée dans l'énoncé, et sa détemination semble impossible en l'absence d'hypothèse supplémentaire... Voici deux hypothèses que l'on pourrait envisager, et leurs conséquences sur le calcul de $\mathbb{P}(V\vert\overline{C})$ :

- **Hypothèse 1 : $V$ et $C$ sont indépendants.** Dans ce cas, $V$ et $\overline{C}$ aussi sont indépendants, et l'exercice devient trivial : $\mathbb{P}(V\vert\overline{C})=\mathbb{P}(V)=\frac{2}{5}$.

- **Hypothèse 2 : $C\subset V$.** On peut imaginer - mais c'est un peu tiré par les cheveux - que l'expression *M. Statys reçoit du courrier* comme *M. Statys ouvre son courrier*. Dans ce cas, pas le choix : s'il ouvre son courrier (le jour où il le reçoit), c'est qu'il est présent ! Et donc on a bien $C\subset V$. Alors 

\begin{align}
\mathbb{P}(V\vert\overline{C})&=\frac{\mathbb{P}(V)-\mathbb{P}(V\cap C)}{1-\mathbb{P}(C)} \\
&=\frac{\mathbb{P}(V)-\mathbb{P}(C)}{1-\mathbb{P}(C)} \\
&=\frac{\frac{2}{5}-\frac{1}{3}}{1-\frac{1}{3}} \\
&=\frac{1}{10} \\
\end{align}

\noindent L'hypothèse $2$ me semble plus crédible que l'hypothèse $1$, peut-être est-ce ainsi qu'il fallait interpréter l'énoncé ?

\noindent 

**7.a.** Voir cours.

\noindent 

**b.** Si la suite $(A_n)_{1\leq n\leq N}$ est décroissante, alors $\bigcup\limits_{n=1}^N A_n=A_1$, et donc dans le membre de droite de la formule de Poincaré, toutes les probablités s'annulent après simplification, sauf $\mathbb{P}(A_1)$.

\noindent De même, si la suite $(A_n)_{1\leq n\leq N}$ est croissante, alors $\bigcup\limits_{n=1}^N A_n=A_N$, et donc dans le membre de droite de la formule de Poincaré, toutes les probablités s'annulent après simplification, sauf $\mathbb{P}(A_N)$.


\vspace{1cm}



:::: {.methbox .meth data-latex="important"}
**Exercice 2.7 (Interne, 2012).** On considère une population dont les individus sont susceptibles de présenter une maladie $\mathcal{M}$. Pour chaque individu de la population considérée, on envisage trois facteurs de risque possibles de cette maladie : $A, B$ et $C$.

Grâce à une étude statistique, on dispose des renseignements suivants :

- $\mathbb{P}(A)=0,4 \hspace{1cm} \mathbb{P}(B)=0,06 \hspace{1cm} \mathbb{P}(A\cap B)=0,06 \hspace{1cm} \mathbb{P}(A\cap C)= 0,1$

- $A$ et $C$ sont indépendants
- $B$ et $C$ sont incompatibles


**1.** Déterminer $\mathbb{P}(C)$, $\mathbb{P}(B\cap C)$ et $\mathbb{P}(A\cap B\cap C)$.

**2.** Calculer la probabilité pour qu'au moins un individu présente un facteur de risque.

**3.** On note $X$ le nombre de facteurs de risque présents chez un individu. Quelles valeurs $X$ peut-elle prendre ? Pour chacune de ces valeurs $i$, calculer $\mathbb{P}(X=i)$.

**4.** On note $M$ l'événement *L'individu est atteint de la maladie * $\mathcal{M}$. On donne les probabilités conditionelles suivantes :

$$\mathbb{P}_{\overline{A}}(M)=0 \hspace{1cm} \mathbb{P}_{A\cap B}(M)=0,06 \hspace{1cm} \mathbb{P}_{A\cap C}(M)=0,05 \hspace{1cm} \mathbb{P}_{A\cap \overline{B}\cap\overline{C}}(M)=0$$
\noindent 

**a.** Interpréter ces quatre valeurs.

**b.** Calculer la probabilité $\mathbb{P}(M)$.
::::

\noindent

**Solution.** *Remarquons d'abord que cet exercice, posé lors des épreuves écrites du concours interne de 2012, est très proche dans l'esprit de l'exercice 5, lui aussi posé lors de ces épreuves ! Ce type d'exercices, de niveau lycée, est peu présent dans les sujets des précédentes années.*

\noindent 

**1.** - Comme $A$ et $C$ sont indépendants, $\mathbb{P}(C)=\frac{\mathbb{P}(A\cap C)}{\mathbb{P}(A)}=\frac{1}{4}$.

- $B\cap C=\emptyset$, donc $\mathbb{P}(B\cap C)=0$.

- $(A\cap B\cap C)\subset (B\cap C)=\emptyset$, donc $A\cap B\cap C=\emptyset$, d'où $\mathbb{P}(A\cap B\cap C)=0$.

\noindent 

**2.** On calcule $\mathbb{P}(A\cup B\cup C)$. On utilise la formule de Poincaré :

\begin{align}
\mathbb{P}(A\cup B\cup C) &= \mathbb{P}(A)+\mathbb{P}(B)+\mathbb{P}(C) \\
& \hspace{0.2cm} -\mathbb{P}(A\cap B)-\mathbb{P}(A\cap C)-\mathbb{P}(B\cap C) \\
& \hspace{0.2cm} + \mathbb{P}(A\cap B\cap C) \\
&= 0,4+0,06+0,25-0,06-0,1-0+0 \\
&= 0,55 \\
\end{align}

\noindent

**3.** On a *a priori* $X\in\{0, 1, 2, 3\}$. On a 

- $\mathbb{P}(X=3)=\mathbb{P}(A\cap B\cap C)=0$.

- $\mathbb{P}(X=0)=1-\mathbb{P}(X\geq 1)=1-\mathbb{P}(A\cup B\cup C)=1-0,55=0,45$.

- Il est ensuite plus facile de calculer d'abord $\mathbb{P}(X=2)$ :

\begin{align}
\mathbb{P}(X=2)&=\mathbb{P}(\overline{A}\cap B\cap C)+\mathbb{P}(A\cap \overline{B}\cap C)+\mathbb{P}(A\cap B\cap \overline{C}) \\
&= 0+\mathbb{P}(A\cap C)+\mathbb{P}(A\cap B) \\
&= 0,16 \\
\end{align}

- et d'en déduire $\mathbb{P}(X=1)$ :

\begin{align}
\mathbb{P}(X=1)&=\mathbb{P}(X\geq 1)-\mathbb{P}(X=2) \text{ ; car } $\mathbb{P}(X=3)=0$ \\
&=0,55-0,16 \\
&=0,39 \\
\end{align}

\noindent

**4.a.** 

- $\mathbb{P}_{\overline{A}}(M)=0$ : si l'individu ne présente pas le facteur de risque $A$, alors la probabilité qu'il soit malade est nulle.

- $\mathbb{P}_{A\cap B}(M)=0,06$ : la présence simultanée des facteurs de risque $A$ et $B$ implique une probabilité d'être malade de $6\,\%$.

- $\mathbb{P}_{A\cap C}(M)=0,05$ : la présence simultanée des facteurs de risque $A$ et $C$ implique une probabilité d'être malade de $5\,\%$.

- $\mathbb{P}_{A\cap \overline{B}\cap\overline{C}}(M)=0$ : si un individu présente le facteur de risque $A$, mais pas les facteurs de risque $B$ et $C$, alors la probabilité qu'il soit malade est nulle.

\noindent 

**b.** Comme $B$ et $C$ sont incompatibles, $\{A\cap B, A\cap C, A\cap\overline{B}\cap\overline{C}, \overline{A}\}$ est un système complet d'événements. D'après la formule des probabilités totales on a alors

\begin{align}
\mathbb{P}(M)&=\mathbb{P}_{\overline{A}}(M)\mathbb{P}(\overline{A})+\mathbb{P}_{A\cap B}(M)\mathbb{P}(A\cap B)+\mathbb{P}_{A\cap C}(M)\mathbb{P}(A\cap C)+\mathbb{P}_{A\cap\overline{B}\cap\overline{C}}(M)\mathbb{P}(A\cap\overline{B}\cap\overline{C}) \\
&= 0+0,06\times 0,06+0,05\times 0,1+0 \\
&= 0,0086 \\
\end{align}



:::: {.methbox .meth data-latex="important"}
**Exercice 2.8 (propagation d'une rumeur).** Une information binaire (du type vrai/faux) se propage au sein d'une population. Lorsqu'une personne reçoit l'information :

- elle la retransmet telle quelle à la personne suivante avec probabilité $p$ ;
- elle la retransmet de façon erronée à la personne suivante avec probabilité $1-p$.

\noindent On note $p_n$ la probabilité pour que, après $n$ transmissions, l'information soit correcte. On suppose que $p_0=1$.

\noindent

**1.** Donner une relation de récurrence entre $p_{n+1}$ et $p_n$.

\noindent

**2.** En déduire une expression de $p_n$ en fonction de $p$ et de $n$.

\noindent

**3.** En déduire la valeur de $\lim_{n\to\infty} p_n$. 
::::

\noindent 

**Solution.** On note $I_n$ : *l'information est correcte après $n$ transmissions*. On a donc $p_n=\mathbb{P}(I_n)$. 

\noindent 

**1.** D'après la formule des probabilités totales : $\mathbb{P}(I_{n+1})=\mathbb{P}(I_{n+1}\vert I_n)\mathbb{P}(I_n)+\mathbb{P}(I_{n+1}\vert\overline{I_n})\mathbb{P}(\overline{I_n})$, autrement dit

$$p_{n+1}=pp_n+(1-p)(1-p_n)$$

soit encore

$$p_{n+1}=(2p-1)p_n+1-p$$

\noindent 

**2.** On reconnait une suite arithmético-géométrique $p_{n+1}=ap_n+b$, avec $a=2p-1$ et $b=1-p$. 

- si $2p-1=1$, autrement dit si $p=1$, alors $(p_n)_{n}$ est une suite constante, et donc pour tout entier naturel $n$ on a $p_n=p_0=1$.

- sinon, la suite $(p_n)_n$ converge vers une limite $l$ telle que $l=al+b$, soit $l=\frac{b}{1-a}=\frac{1-p}{2(1-p)}=\frac{1}{2}$. On a alors

$$p_{n+1}-\frac{1}{2}=(2p-1)\left(p_n-\frac{1}{2}\right)$$

\noindent donc la suite $(p_n-l)_n$ est géométrique de raison $2p-1$. D'où $p_n-l=(2p-1)^n(p_0-l)$, et donc

$$p_n=\frac{1}{2}+\frac{1}{2}(2p-1)^n$$

\noindent

**Commentaire.** *Le jour du concours, vous avez tout à fait le droit d'utiliser directement la formule explicitant le terme générique d'une suite arithmético-géométrique. A minima, si (comme moi...) vous ne le connaissez pas par coeur, il faut savoir la redémontrer rapidement en utilisant la méthode ci-dessus.*

\noindent

**3.** - si $p=1$, alors la suite $(p_n)_n$ est constante égale à 1, donc $\lim\limits_{n\to\infty}p_n=1$.

- si $p=0$, alors $p_n=\frac{1}{2}(1+(-1)^n)$. Les suites extraites $(p_{2n})_n$ et $(p_{2n+1})_n$ convergent donc respectivement vers $1$ et $0$ (elles sont mêmes constantes), donc la suite $(p_n)_n$ admet deux valeurs d'adhérence distictes : elle ne converge pas.

- si $0<p<1$, alors $-1<2p-1<1$, et donc $\lim\limits_{n\to\infty}(2p-1)^n=0$ : on en déduit que $\lim\limits_{n\to\infty}p_n=\frac{1}{2}$.

\noindent

**Remarque.** Ce dernier résultat nous dit que si d'une date à la date suivante l'information n'est ni *presque sûrement* transmise correctement, ni *presque sûrement* transmise de façon erronée, alors à une date asymptotitque il y a une chance sur deux qu'elle soit vraie. Un tel résultat était prévisible, car les événements $I_n$ et $\overline{I_n}$ jouent des rôles parfaitement symétriques, et il n'y a donc aucune raison que l'un des deux soit asymptotiquement plus probable que l'autre.




:::: {.methbox .meth data-latex="important"}
**Exercice 2.9.** Le gérant d'un magasin d'informatique a reçu un lot de clés USB. 5 \% des boites sont abîmées. Le gérant estime que :

- 60 \% des boites abîmées contiennent au moins une clé défectueuse ;
- 98 \% des boites non abîmées ne contiennent aucune clé défectueuse.

\noindent Un client achète une boite du lot. On désigne par 

- $A$ l'événement : la boite est abîmée ;
- $D$ l'événement : la boite achetée contient au moins une clé défectueuse.

\noindent

**1.** Donner les probabilités $\mathbb{P}(A)$, $\mathbb{P}(\overline{A})$, $\mathbb{P}(D\vert A)$, $\mathbb{P}(D\vert\overline{A})$, $\mathbb{P}(\overline{D}\vert A)$ et $\mathbb{P}(\overline{D}\vert\overline{A})$. En déduire $\mathbb{P}(D)$.

\noindent

**2.** Le client constate qu'une des clés achetées est défectueuse. Quelle est la probabilité pour qu'il ait acheté une boite abîmée ?
::::

\noindent

**Solution. 1.** 

- $\mathbb{P}(A)=0,05$ ;

- $\mathbb{P}(\overline{A})=0,95$ d'après ce qui précède ;

- $\mathbb{P}(D\vert A)=0,6$ ;

- $\mathbb{P}(\overline{D}\vert A)=0,4$ d'après ce qui précède ;

- $\mathbb{P}(\overline{D}\vert\overline{A})=0,98$ ;

- On en déduit, d'après la formule des probabilités totales et la formule de Bayes, que 

\begin{align}
\mathbb{P}(D)&=\mathbb{P}(D\vert A)\mathbb{P}(A)+\mathbb{P}(D\vert\overline{A})\mathbb{P}(\overline{A}) \\
&=0,6\times 0,05+0,02\times 0,95 \\
&\approx 0,049 \\
\end{align}

\noindent 

**2.** On veut déterminer $\mathbb{P}(A\vert D)$. D'après la formule de Bayes :

\begin{align}
\mathbb{P}(A\vert D)&=\frac{\mathbb{P}(D\vert A)\mathbb{P}(A)}{\mathbb{P}(D)} \\
&\approx\frac{0,6\times 0,05}{0,049} \\
&\approx 0,61 \\
\end{align}

:::: {.methbox .meth data-latex="important"}
**Exercice 2.10 (test de dépistage).** Une maladie est présente dans la population, avec une prévalence d'une personne malade sur $10\,000$. Un nouveau test de dépistage vient d'être mis au point. On constate que :

- lorsqu'une personne est malade, le test est positif dans $99\,\%$ des cas ;

- lorsqu'une personne n'est pas malade, le test est positif dans $0,1\,\%$ des cas.

\noindent Selon vous, ce test doit-il être commercialisé ?
::::

\noindent

**Solution.** On note $M$ et $P$ les événements :

- $M$ : *la personne est malade* ;
- $P$ : *le test est positif*.

\noindent On calcule $\mathbb{P}(M\vert P)$ avec la fomrule de Bayes :

\begin{align}
\mathbb{P}(M\vert P)&=\frac{\mathbb{P}(P\vert M)\mathbb{P}(M)}{\mathbb{P}(P\vert M)\mathbb{P}(M)+\mathbb{P}(P\vert\overline{M})\mathbb{P}(\overline{M})} \\
&= \frac{0,99\times 0,0001}{0,99\times 0,0001+0,001\times 0,9999} \\
&\approx 0,09
\end{align}

\noindent La probabilité qu'une personne dont le test est positif soit malade est donc seulement égale à $9\,\%$. Il faut donc éviter de commercialiser ce test.

\noindent

**Remarque.** Ce résultat qui peut paraître surprenant s'explique par le fait que la maladie étudiée est très rare ($1$ cas sur $10\,000$), et donc même en cas de test positif la probabilité que la personne soit vraiment malade reste limitée.

## Variables aléatoires discrètes

:::: {.methbox .meth data-latex="important"}
**Exercice 3.1. 1.** Soit $X$ une variable aléatoire discrète de support $X(\Omega)=\mathbb{N}^*$ et telle que pour tout entier naturel $n\geq 1$ on ait 

$$\mathbb{P}(X=n)=\frac{1}{2^n}$$

**a.** Vérifier que l'on définit bien une loi de probabilité.

**b.** $X$ admet-elle une espérance ? Si oui, la calculer.

**c.** $X$ admet-elle une variance ? Si oui, la calculer.


**2.** Soient $\alpha$ un réel et $Y$ une variable aléatoire discrète de support $Y(\Omega)=\mathbb{N}$ et telle que, pour tout entier naturel $n$ on ait 

$$\mathbb{P}(Y=n)=\frac{\alpha}{n!}$$

**a.** Déterminer la valeur de $\alpha$ pour qu'on ait bien ainsi défini une loi de probabilité.

**b.** $Y$ admet-elle une espérance ? Si oui, la calculer.


**3.** $Z$ une variable aléatoire discrète de support $Y(\Omega)=\mathbb{N}^*$ et telle que, pour tout entier naturel $n\geq 1$ on ait 

$$\mathbb{P}(Z=n)=\frac{1}{n(n+1)}$$

**a.** Vérifier que l'on définit bien une loi de probabilité.

**b.** $Z$ admet-elle une espérance ? Si oui, la calculer.
::::

**Solution. 1.a.** La série $\sum\limits_n \frac{1}{2^n}$ est convergente (série géométrique de raison $\frac{1}{2}\in\left]-1\,;\,1\right[$) et
\begin{align}
\sum\limits_{n=1}^{+\infty}\frac{1}{2^n}&=\sum\limits_{n=0}^{+\infty}\frac{1}{2^n}-1 \\
&=\frac{1}{1-\frac{1}{2}}-1 \\
&=1
\end{align}

\noindent On a donc bien défini une loi de probabilité.

\noindent

**b.** On commence par quelques rappels d'analyse, utiles pour cette question et la suivante :

:::: {.thmbox .thm data-latex="important"}
**Rappels d'analyse.** Pour cette question, comme pour la question suivante, on va passer par les séries dérivées de la série géométrique $\sum\limits_{n}x^n$. Cette série est convergente sur $]-1\,;\,1[$ et sa somme est indéfiniment dérivable sur cet intervalle. Les dérivées successives de cette dernière s'obtiennent alors par dérivations successives du terme général. Autrement dit, en posant, pour tout réel $x$ dans $]-1\,;\,1[$ :

$$f(x)=\sum\limits_{n=0}^{+\infty}x^n$$
\noindent on a 

\begin{align}
f(x)&=\sum\limits_{n=0}^{+\infty}x^n=\frac{1}{1-x} \\
f'(x)&=\sum\limits_{n=0}^{+\infty}n\,x^{n-1}=\frac{1}{(1-x)^2} \\
f''(x)&=\sum\limits_{n=0}^{+\infty}n(n-1)\,x^{n-2}=\frac{2}{(1-x)^3} \\
\end{align}

::::

\noindent On montre maintenant que $X$ admet une espérance. Cela revient à montrer que la série $\sum\limits_{n}\frac{n}{2^{n}}$ est convergente. Or, avec ce qui précède :

\begin{align}
f'\left(\frac{1}{2}\right)&=\sum\limits_{n=0}^{+\infty}\frac{n}{2^{n-1}} \\
&=\sum\limits_{n=1}^{+\infty}\frac{n}{2^{n-1}} \\
&=\frac{1}{\left(1-\frac{1}{2}\right)^2} \\
&=4 \\
\end{align}

\noindent 

Donc, la série $\sum\limits_{n}\frac{n}{2^{n}}$ est bien convergente, et 

\begin{align}
\sum\limits_{n=0}\frac{n}{2^{n}}&=\frac{1}{2}\sum\limits_{n=0}^{+\infty}\frac{n}{2^{n-1}} \\
&=2
\end{align}

**Ainsi, $X$ admet une espérance et $\mathbb{E}(X)=2$.**

\noindent  

**c.** On obtient de même :

\begin{align}
f''\left(\frac{1}{2}\right)&=\sum\limits_{n=2}^{+\infty}\frac{n(n-1)}{2^{n-2}} \\
&=\frac{2}{\left(1-\frac{1}{2}\right)^2} \\
&= 16
\end{align}

\noindent La série $\sum\limits_{n}\frac{n}{2^{n-2}}=2\sum\limits_{n}\frac{n}{2^{n-1}}$ converge et la somme $\sum\limits_{n=2}\frac{n}{2^{n-2}}$ vaut $2\left(\sum\limits_{n=1}\frac{n}{2^{n-1}}-1\right)=6$ (question précédente). Par conséquent, la série $\sum\limits_{n}\frac{n^2}{2^{n}}$ converge et 

\begin{align}
\sum\limits_{n=1}^{+\infty}\frac{n^2}{2^{n}}&=\frac{1}{2}+\sum\limits_{n=2}^{+\infty}\frac{n^2}{2^{n}} \\
&=\frac{1}{2}+\frac{1}{4}\sum\limits_{n=2}^{+\infty}\frac{n^2}{2^{n-2}} \\
&=\frac{1}{2}+\frac{1}{4}\left(\sum\limits_{n=2}^{+\infty}\frac{n(n-1)}{2^{n-2}}+\sum\limits_{n=2}^{+\infty}\frac{n}{2^{n-2}}\right) \\
&=\frac{1}{2}+\frac{1}{4}(16+6) \\
&=6 \\
\end{align}

\noindent

Donc, $X$ admet un moment d'ordre $2$, et donc une variance, et $\mathbb{E}(X^2)=6$. On en déduit sa variance par la formule de Koenig-Huygens :

\begin{align}
\mathbb{V}(X)&=\mathbb{E}(X^2)-\mathbb{E}(X)^2 \\
&=6-4 \\
&=2
\end{align}

\noindent

**Ainsi, $X$ admet une variance et $\mathbb{V}(X)=2$.**

**2.a.** La série $\sum\limits_{n}\frac{\alpha}{n!}$ converge et $\sum\limits_{n=0}^{+\infty}\frac{\alpha}{n!}=\alpha\,e$. Pour qu'on ait bien défini une loi de probabilité, cette somme doit être égale à $1$, ce qui impose de poser $\alpha=\frac{1}{e}$.

**b.** $Y$ admet une espérance si et seulement si la série $\sum\limits_{n}n\,\frac{1}{e\,n!}$ est convergente. 

\noindent Or, pour tout entier naturel $N$ :

\begin{align}
\sum\limits_{n=0}^N n\,\frac{1}{e\,n!}&=\sum\limits_{n=1}^N\frac{1}{e\,(n-1)!} \\
&=\frac{1}{e}\sum\limits_{n=0}^{N-1}\frac{1}{n!} \\
&\underset{N\to +\infty}{\longrightarrow}  1\\
\end{align}

\noindent 

**$Y$ admet donc une espérance et $\mathbb{E}(Y)=1$.**

\noindent 

**3.a.** Soit $N$ un entier naturel tel que $N\geq 1$. On a :

\begin{align}
\sum\limits_{n=1}^N\frac{1}{n(n+1)}&=\sum\limits_{n=1}^N\left(\frac{1}{n}-\frac{1}{n+1}\right) \\
&=\sum\limits_{n=1}^N\frac{1}{n}-\sum\limits_{n=1}^N\frac{1}{n+1} \\
&=\sum\limits_{n=1}^N\frac{1}{n}-\sum\limits_{n=2}^{N+1}\frac{1}{n} \\
&=1-\frac{1}{N+1} \\
&\underset{N\to +\infty}{\longrightarrow} 1 \\
\end{align}

\noindent 

ce qui permet de conclure.

\noindent

**Commentaires. i.** *La première égalité résulte d'une décomposition en éléments simples. Une fois qu'on l'a écrite, on peut se contenter, le jour du concours, d'évoquer une somme télescopique pour justifier directement le passage à l'expression $1-\frac{1}{N+1}$, sans écrire, donc, les deux lignes intermédiaires. Pour rappel, une somme télescopique est - à un changement éventuel de signe près - une somme du type $\sum\limits_{n=p}^q (a_{n+1}-a_{n})$. Dans une telle somme, tous les termes s'annulent deux à deux à l'exception de ceux situés aux extremités. On a donc* $$\sum\limits_{n=p}^{q} (a_{n+1}-a_{n})=a_{q+1}-a_{p}$$

\noindent

**ii. Un exemple d'approche incorrecte.** *Il peut être tentant de dire que la série $\sum\limits_{n}\frac{1}{n(n+1)}$ est convergente (par exemple, son terme général est positif et majoré par $\frac{1}{n^2}$, qui est celui d'une série convergente) et d'écrire ensuite*

\begin{align}
\sum\limits_{n=1}^{+\infty}\frac{1}{n(n+1)}&=\sum\limits_{n=1}^{+\infty}\left(\frac{1}{n}-\frac{1}{n+1}\right) \\
&=\sum\limits_{n=1}^{+\infty}\frac{1}{n}-\sum\limits_{n=1}^{+\infty}\frac{1}{n+1} \\
\end{align}

\noindent

*La première égalité est correcte, puisqu'on a juste remplacé $\frac{1}{n(n+1)}$ par $\frac{1}{n}-\frac{1}{n+1}$, qui sont deux termes égaux.* 

\noindent 

*En revanche, l'égalité $\sum\limits_{n=1}^{+\infty}\left(\frac{1}{n}-\frac{1}{n+1}\right)=\sum\limits_{n=1}^{+\infty}\frac{1}{n}-\sum\limits_{n=1}^{+\infty}\frac{1}{n+1}$ est incorrecte à cause du terme de droite. Celui-ci correspond à la différence entre les sommes de deux séries divergentes (autrement dit, quelque chose du type $(+\infty)-(+\infty))$). En l'état, il n'a donc a priori pas de sens et ne peut être écrit tel quel.*

*On a ici utilisé (à tort) une formule de linéarité $$\sum\limits_{n=p}^{+\infty}(u_n+v_n)=\sum\limits_{n=p}^{+\infty}u_n+\sum\limits_{n=p}^{+\infty}v_n$$ mais l'utilisation d'une telle formule suppose de vérifier au préalable que tous les termes en jeu sont bien définis. Autrement dit, on doit non seulement s'assurer de la convergence de la série $\sum\limits_n(u_n+v_n)$, mais aussi de celle des séries $\sum\limits_n u_n$ et $\sum\limits_n v_n$, et c'est justement la convergence de ces deux séries qui pose problème dans notre exemple.*

*La seule approche correcte dans cette question est donc bien de se placer d'abord dans le cas fini en sommant de $1$ à $N$, d'utiliser ensuite la linéarité (qui est licite dans le cas fini), puis de terminer en faisant tendre $N$ vers l'infini.*

**b.** $Z$ admet une espérance (finie) si et seulement si la série $\sum\limits_n\frac{n}{n(n+1)}=\sum\limits_n\frac{1}{n+1}$ est convergente. Or, cette série diverge (série harmonique), donc $Z$ n'admet pas d'espérance (finie).





:::: {.methbox .meth data-latex="important"}
**Exercice 3.2 (à retenir).** Soit $X$ une variable aléatoire discrète à valeurs dans $\mathbb{N}$ et d'espérance finie.

**1.** Démontrer que, pour tout entier naturel $n$, on a 

$$\sum\limits_{k=0}^n\mathbb{P}(X>k)=\sum\limits_{k=0}^n k\,\mathbb{P}(X=k)+(n+1)\,\mathbb{P}(X>n)$$

**2.** En déduire que 

$$\mathbb{E}(X)=\sum\limits_{k=0}^{\infty}\mathbb{P}(X>k)$$

**Commentaire.** *Cette formule donne donc une autre expression de l'espérance pour une variable aléatoire \textbf{positive} (dans le cas général, elle peut être fausse). On peut presque la considérer comme du cours, donc il faut penser à l'utiliser.*
::::

\noindent

**Solution. 1.** 

\noindent

On procède par récurrence sur $n\in\mathbb{N}$.

- pour $n=0$ :

\begin{align}
\sum\limits_{k=0}^0 k\,\mathbb{P}(X=k)+(0+1)\,\mathbb{P}(X>0)&=0.\mathbb{P}(X=0)+\mathbb{P}(X>0) \\
&=\mathbb{P}(X>0) \\
&=\sum\limits_{k=0}^0\mathbb{P}(X>k)
\end{align}

\noindent donc l'égalité est vraie au rang $n=0$.

- supposons maintenant cette égalité vraie au rang $n$, pour un entier naturel $n$ donné, et démontrons qu'elle est alors vraie au rang $n+1$. On a donc

\begin{align}
\sum\limits_{k=0}^{n+1}\mathbb{P}(X>k)&=\sum\limits_{k=0}^{n}\mathbb{P}(X>k)+\mathbb{P}(X>n+1) \\
&=\sum\limits_{k=0}^{n}k\,\mathbb{P}(X=k)+(n+1)\mathbb{P}(X>n)+\mathbb{P}(X>n+1) \\
&\text{(par hypothèse de réccurence)} \\
&=\sum\limits_{k=0}^{n}k\,\mathbb{P}(X=k)+(n+1)\mathbb{P}(X=n+1)+(n+1)\mathbb{P}(X>n+1)+\mathbb{P}(X>n+1) \\
&=\sum\limits_{k=0}^{n+1}k\,\mathbb{P}(X=k)+(n+2)\mathbb{P}(X>n+1)
\end{align}

\noindent ce qui permet de conclure.

\noindent

**2.** D'après la question précédente, il suffit de démontrer que $\lim\limits_{n\to +\infty}(n+1)\,\mathbb{P}(X>n)=0$. Or, on a

\begin{align}
0&\leq (n+1)\,\mathbb{P}(X>n) \\
&=\sum\limits_{k=n+1}^{+\infty}(n+1)\mathbb{P}(X=k) \\
&\leq\sum\limits_{k=n+1}^{+\infty}\mathbb{P}(X=k) \\
\end{align}

Or, ce dernier terme est le reste de la série $\sum\limits_{n}n\,\mathbb{P}(X=n)$, qui est une série convergente de somme $\mathbb{E}(X)$. Par conséquent, ce terme tend vers $0$ lorsque $n$ tend vers l'infini, et on conclut en appliquant le théorème des gendarmes.


\noindent

**Commentaires.** *L'égalité de la question 2 a été établie à partir d'une égalité entre sommes sur des ensembles d'indices finis. Cette égalité a été démontrée par récurrence dans la question 1. Comme souvent, une démonstration par récurrence présente l'avantage de réduire essentiellement la question en un simple jeu d'écriture, ce qui peut s'avérer plus efficace le jour du concours.*

*On présente dans ce qui suit une approche complètement différente, qui demande davantage d'intuition et est plus visuelle. On commence par présenter son heuristique, avant de formaliser l'idée principale en un raisonnement plus rigoureux.*


**Méthode 2 (calcul direct). Heuristique.** On peut partir de la définition de l'espérance  
$$\mathbb{E}(X)=\sum\limits_{i=1}^{+\infty}i\,\mathbb{P}(X=i)$$

et écrire le terme général de cette série comme somme de $k$ termes égaux :

$$i\,\mathbb{P}(X=i)=\mathbb{P}(X=i)+\mathbb{P}(X=i)+\dots\mathbb{P}(X=i)$$

\noindent 

On construit alors un tableau infini, dont la ligne numéro $i$ contient exactement $i$ termes non nuls, tous égaux à $\mathbb{P}(X=i)$ :

<center>
```{r, echo = FALSE, out.width="80%", out.height="80%"}
knitr::include_graphics("C:/Users/olivier.guin/Travail/Formation_Administrateur/Cours/Cours_probabilités_statistique/images/tableau_formule_esperance.PNG") 
```
</center>

\noindent

Ainsi :

- en sommant tous les éléments de la ligne $i$, on obtient $i\,\mathbb{P}(X=i)$ ;

- en sommant toutes ces sommes par ligne, on obtient $\sum\limits_{i=0}^{+\infty}i\,\mathbb{P}(X=i)$, soit $\mathbb{E}(X)$.

\noindent 

On regarde maintenant ce qui se passe si, plutôt que de sommer les sommes par lignes, on somme les sommes par colonnes :

- la somme des éléments de la colonne $j$ est égale à 
$\mathbb{P}(X=j)+\mathbb{P}(X=j+1)+\dots+\mathbb{P}(X=n)+\dots$, soit encore $\mathbb{P}(X>j-1)$.

- en sommant les sommes par colonnes, on obtient donc $\sum\limits_{j=1}^{+\infty}\mathbb{P}(X>j-1)=\sum\limits_{j=0}^{+\infty}\mathbb{P}(X>j)$.

\noindent 

On s'attend à ce que ces deux façons de calculer amènent au même résultat, ce qui se traduit exactement par l'égalité $\mathbb{E}(X)=\sum\limits_{j=0}^{+\infty}\mathbb{P}(X>j)$. 

\noindent 

Il s'agit maintenant d'écrire tout cela un peu plus rigoureusement. Ce qui précède s'appuie sur une seule idée : pour sommer tous les éléments d'un tableau, on peut soit sommer les sommes par lignes, soit sommer les sommes par colonnes. Notons de manière générale $a_{i,j}$ l'élément situé sur la ligne $i$ et la colonne $j$ du tableau, que l'on suppose dans un premier temps fini (à $p$ lignes et $q$ colonnes).

\noindent La somme des sommes par lignes s'écrit alors $\sum\limits_{i=1}^p\sum\limits_{j=1}^q a_{i,j}$ et la somme des sommes par colonnes s'écrit $\sum\limits_{j=1}^q\sum\limits_{i=1}^p a_{i,j}$, si bien que dans le cas d'un tableau fini, l'équivalence entre ces deux procédés de calcul se traduit par l'égalité

$$\sum\limits_{i=1}^p\sum\limits_{j=1}^q a_{i,j}=\sum\limits_{j=1}^q\sum\limits_{i=1}^p a_{i,j}$$
\noindent Autrement dit, passer d'une sommation de sommes par lignes à une sommation de sommes par colonnes revient exactement à intervertir deux symboles $\Sigma$, ce qui dans le cas où les ensembles d'indices sur lesquels on somme sont tous **finis** est trivialement vrai (c'est une conséquence directe des propriétés de commutativité et d'associativité de l'addition dans $\mathbb{R}$).

\noindent 

Dans le cas infini, il faut être un peu plus prudent : l'égalité $$\sum\limits_{i\in I}\sum\limits_{j\in J}a_{ij}=\sum\limits_{j\in J}\sum\limits_{i\in I}a_{ij}$$
n'est pas nécessairement vraie si l'un au moins des ensembles $I$ et $J$ est infini. Cependant, pour $I=J=\mathbb{N}$ on a le résultat suivant :

:::: {.thmbox .thm data-latex="important"}
**Théorème de Fubini pour les séries.** Soit $(a_{m,n})_{(m,n)\in\mathbb{N}^2}$ une suite à double indice. Si 

$$\sum\limits_{i=0}^{+\infty}\sum\limits_{j=0}^{+\infty}|a_{i,j}|<+\infty$$
\noindent alors 

$$\sum\limits_{i=0}^{+\infty}\sum\limits_{j=0}^{+\infty}a_{i,j}=\sum\limits_{i=0}^{+\infty}\sum\limits_{j=0}^{+\infty}a_{i,j}$$
(étant entendu que toutes les séries en jeu sont convergentes)
::::

\noindent

Avec ce résultat, on peut maintenant rédiger une solution plus formalisée des idées précédentes.

**Solution formalisée.** On pose $a_{i,j}=\mathbb{P}(X=i).1_{j\leq i}$. On a

\begin{align}
\sum\limits_{i=1}^{+\infty}\sum\limits_{j=1}^{+\infty}|a_{i,j}|&=\sum\limits_{i=1}^{+\infty}\sum\limits_{j=1}^{+\infty}\mathbb{P}(X=i).1_{j\leq i} \\
&=\sum\limits_{i=1}^{+\infty}\sum\limits_{j=1}^{i}\mathbb{P}(X=i) \\
&=\sum\limits_{i=1}^{+\infty}i\,\mathbb{P}(X=i) \\
&=\mathbb{E}(X) \\
&<+\infty
\end{align}

\noindent 

D'après le théorème de Fubini, la série double $\sum\limits_j\sum\limits_i\mathbb{P}(X=i).1_{j\leq i}$ est donc convergente et

\begin{align}
\mathbb{E}(X)&=\sum\limits_{i=1}^{+\infty}\sum\limits_{j=1}^{+\infty}\mathbb{P}(X=i).1_{j\leq i} \\
&=\sum\limits_{j=1}^{+\infty}\sum\limits_{i=1}^{+\infty}\mathbb{P}(X=i).1_{j\leq i} \\
&=\sum\limits_{j=1}^{+\infty}\sum\limits_{i=j}^{+\infty}\mathbb{P}(X=i) \\
&=\sum\limits_{j=1}^{+\infty}\mathbb{P}(X\geq j) \\
&=\sum\limits_{j=0}^{+\infty}\mathbb{P}(X> j) \\
\end{align}
::::



:::: {.methbox .meth data-latex="important"}
**Exercice 3.3.** Soient $n\in\mathbb{N}^*$ et $a\in\mathbb{R}$. On considère une variable aléatoire discrète de support $\{0,1,\dots, n\}$ et telle que, pour tout entier $0\leq k\leq n$ :

$$\mathbb{P}(X=k)=\frac{a}{k+1}\binom{n}{k}$$

**1.** Déterminer le réel $a$.

**2.a.** Calculer les espérance $\mathbb{E}(X+1)$ et $\mathbb{E}(X(X+1))$.

**b.** En déduire les valeurs de $\mathbb{E}(X)$ et $\mathbb{V}(X)$.
::::

\noindent

**Solution. 1.** Pour déterminer le réel $a$, on utilise l'égalité $$\sum\limits_{k=0}^n\mathbb{P}(X=k)=1$$

soit encore

$$a\sum\limits_{k=0}^n\frac{\binom{n}{k}}{k+1}=1$$

\noindent 

Or, on a l'égalité

$$\frac{\binom{n}{k}}{k+1}=\frac{\binom{n+1}{k+1}}{n+1}$$
\noindent

Celle-ci a déjà été démontrée dans l'exercice 2.3. sur les coefficients binomiaux (question 1).

\noindent

On en déduit que

\begin{align}
a&=\frac{1}{\sum\limits_{k=0}^n\frac{\binom{n}{k}}{k+1}} \\
&=\frac{n+1}{\sum\limits_{k=0}^n\binom{n+1}{k+1}} \\
&=\frac{n+1}{\sum\limits_{k=0}^{n+1}\binom{n+1}{k}-1} \\
&=\frac{n+1}{2^{n+1}-1}
\end{align}

\noindent

**2.a.** D'après le théorème de transfert, on a 

\begin{align}
\mathbb{E}(X+1)&=\sum\limits_{k=0}^n (k+1)\,\mathbb{P}(X=k) \\
&=\sum\limits_{k=0}^n (k+1)\,\frac{a}{k+1}\binom{n}{k} \\
&=a\sum\limits_{k=0}^n\binom{n}{k} \\
&=a\,2^n \\
&=\frac{n+1}{2^{n+1}-1}\,2^n
\end{align}

\noindent De façon analogue, on a

\begin{align}
\mathbb{E}(X(X+1))&=\sum\limits_{k=0}^n (k+1)\,\mathbb{P}(X=k) \\
&=\sum\limits_{k=0}^n (k+1)\frac{a}{k+1}\binom{n}{k} \\
&=a\sum\limits_{k=0}^n k\,\binom{n}{k} \\
&=a\sum\limits_{k=1}^n k\,\binom{n}{k} \\
&=a\,n\sum\limits_{k=1}^n \binom{n-1}{k-1} \\
&=a\,n\sum\limits_{k=0}^{n-1} \binom{n-1}{k} \\
&=a\, n2^{n-1} \\
&=\frac{n(n+1)}{2^{n+1}-1}\,2^{n-1} \\
\end{align}

\noindent

**b.** On en déduit que

\begin{align}
\mathbb{E}(X)&=\mathbb{E}(X+1)-1 \\
&=\frac{n+1}{2^{n+1}-1}\,2^n-1
\end{align}

\noindent

et

\begin{align}
\mathbb{V}(X)&=\mathbb{E}(X^2)-\mathbb{E}(X)^2 \\
&=\mathbb{E}(X(X+1))-\mathbb{E}(X)-\mathbb{E}(X)^2 \\
&=\mathbb{E}(X(X+1))-\mathbb{E}(X).(\mathbb{E}(X)+1) \\
&=\frac{n(n+1)}{2^{n+1}-1}\,2^{n+1}-\left(\frac{n+1}{2^{n+1}-1}2^n-1\right)\frac{n+1}{2^{n+1}-1}2^n
\end{align}

\noindent

**Commentaire.** *Pour calculer l'espérance et la variance de $X$, on décide donc dans cet exercice de commencer par calculer les espérances de $X$ et $X(X+1)$. Ces calculs sont en effet plus simples car ils permettent, via le théorème de transfert, d'annuler le terme $\frac{1}{k+1}$ de la probabilité $\mathbb{P}(X=k)$. Une fois qu'on a calculé $\mathbb{E}(X)$ et $\mathbb{E}(X(X+1))$, on en déduit facilement, par linéarité de l'espérance, $\mathbb{E}(X)$ et $\mathbb{V}(X)$. Ici, l'exercice est guidé, mais on pourrait très bien imaginer une question d'un sujet du concours dans laquelle il est directement demandé de calculer ces deux quantités, sans les indications fournies par la question 2. Dans ce cas, c'est au candidat de prendre l'initiative d'introduire les variables $X+1$ et $X(X+1)$. De façon générale, retenir que pour calculer l'espérance (ou la variance) d'une variable aléatoire, il est parfois avantageux de calculer au préalable l'espérance (ou la variance) d'une transformation (simple) de cette variable.*


:::: {.methbox .meth data-latex="important"}
**Exercice 3.4 (Somme de deux VA).** Soient $X$ et $Y$ deux variables aléatoires indépendantes, à valeurs dans $\mathbb{N}$ et telles que pour tout entier naturel $k$ :

$$\mathbb{P}(X=k)=\mathbb{P}(Y=k)=pq^k$$

\noindent où $q=1-p$ et $p\in ]0\,;\,1[$.

**1.** Calculer $\mathbb{P}(X=Y)$.

**2.** Calculer $\mathbb{P}(X<Y)$.

**3.** Déterminer la loi de $X+Y$.
::::

\noindent

**1.** 

\begin{align}
\mathbb{P}(X=Y)&=\sum\limits_{k=0}^{+\infty}\mathbb{P}(X=k, Y=k) \\
&=\sum\limits_{k=0}^{+\infty}\mathbb{P}(X=k)\,\mathbb{P}(Y=k) \\
&\text{(par indépendance de } X \text{ et } Y \text{)} \\
&=\sum\limits_{k=0}^{+\infty}p^2 q^{2k} \\
&=p^2\,\frac{1}{1-q^2} \\
&=\frac{p^2}{(1-q)(1+q)} \\
&=\frac{p^2}{p(1+q)} \\
&=\frac{p}{1+q} \\
\end{align}

\noindent

**2.** $X$ et $Y$ jouent des rôles symétriques, donc $\mathbb{P}(X<Y)=\mathbb{P}(Y>X)$, et ces deux probabilités sont alors égales à $\frac{1-\mathbb{P}(X=Y)}{2}$, soit :

$$\mathbb{P}(X<Y)=\mathbb{P}(Y<X)=\frac{q}{1+q}$$

\noindent 

**3.** $X$ et $Y$ étant indépendantes, la loi de $X+Y$ s'obtient par produit de convolution. Son support est $(X+Y)(\Omega)=\mathbb{N}$, et pour tout entier naturel $k$, on a

\begin{align}
\mathbb{P}(X+Y=k)&=\sum\limits_{i=0}^k\mathbb{P}(X=i)\,\mathbb{P}(Y=k-i) \\
&=\sum\limits_{i=0}^k pq^i pq^{k-i} \\
&=p^2\sum\limits_{i=0}^k q^k \\
&=(k+1)p^2 q^k \\
\end{align}

\noindent

**Commentaire.** *Bien faire attention ici aux supports des variables $X$ et $Y$. Comme $Y$ est positive, les événements $(X=k)$ et $(Y=k-i)$ ne peuvent être réalisés que si $i\geq 0$ et $k-i\geq 0$, soit $0\leq i\leq k$.*

:::: {.methbox .meth data-latex="important"}
**Exercice 3.5.** Soit $X$ une variable aléatoire suivant la loi binomiale $\mathcal{B}(n,p)$, avec $n\geq 2$ et $0<p<1$.
\noindent Soit $Y$ une variable aléatoire telle que :

- pour tout entier $k$ tel que $1\leq k\leq n$, la réalisation de l'événement $(X=k)$ entraîne celle de l'événement $(Y=k)$ ;

- la loi de $Y$ sachant $(X=0)$ est la loi uniforme sur $\{1,2,\dots, n\}$.


**1.** Déterminer la loi de probabilité de $Y$.

**2.** Calculer $\mathbb{E}(Y)$.

**3.** Déterminer la loi de probabilité conditionnelle $\mathcal{L}(Y|X\neq 0)$.
::::

\noindent

**Solution. 1.** On a 

$$\left \{
\begin{array}{rcl}
Y|X=0\sim\mathcal{U}_{[\![1;n]\!]} \\
\forall k\in [\![1;n]\!],\, (X=k)\subset (Y=k) \\
\end{array}
\right.$$

\noindent Soit $k\in $[\![1;n]\!]$, alors, d'après la formule des probabilités totales :

\begin{align}
\mathbb{P}(Y=k)&=\sum\limits_{i=0}^n\mathbb{P}(Y=k, X=i) \\
&=\underbrace{\mathbb{P}(Y=k|X=0)}_{=\frac{1}{n}}\underbrace{\mathbb{P}(X=0)}_{=(1-p)^n}+\sum\limits_{i=1}^n\underbrace{\mathbb{P}(Y=k, X=i)}_{=\left \{
\begin{array}{rcl}
0&;\, \text{ si } i\neq k \\
\mathbb{P}(X=k)&;\,\text{ si } i=k \\
\end{array}
\right.} \\
&=\frac{1}{n}(1-p)^n+\mathbb{P}(X=k) \\
&=\frac{1}{n}(1-p)^n+\binom{n}{k}p^k(1-p)^{n-k} \\
\end{align}

\noindent Montrons enfin que $Y$ a pour support $Y(\Omega)[\![1;n]\!]=$ :
 

\begin{align}
\sum\limits_{k=1}^n\mathbb{P}(Y=k)&=(1-p)^n+\sum\limits_{k=1}^n\binom{n}{k}p^k (1-p)^{n-k} \\
&=\sum\limits_{k=0}^n\binom{n}{k}p^k (1-p)^{n-k} \\
&=1 \\
\end{align}

\noindent car on a sommé les probabilités $\mathbb{P}(Z=k)$, $k\in [\![0;n]\!]$, où $Z\sim\mathcal{B}(n\,;\,p)$.

\noindent On a donc $Y(\Omega)=[\![1;n]\!]$, ce qui permet de conclure cette question.

\noindent


**Commentaire :** *Pour traiter rigoureusement cette question, il est important de ne pas éluder la question du support. On peut, comme on l'a fait, calculer $\sum\limits_{k=1}^n\mathbb{P}(Y=k)$, et conclure, si cette somme est bien égale à $1$, que le support est égal à $[\![1;n]\!]$. Une autre approche aurait consisté à démontrer que $\mathbb{P}(Y=x)=0$ pour $x\not\in [\![1;n]\!]$.*

\noindent

**2.** On peut écrire, pour tout $k\in [\![1;n]\!]$, $\mathbb{P}(Y=k)=\frac{1}{n}(1-p)^n+\mathbb{P}(Z=k)$, avec $Z\sim\mathcal{B}(n\,;\,p)$.

\noindent Donc :

\begin{align}
\sum\limits_{k=1}^n k\mathbb{P}(Y=k)&=\frac{1}{n}(1-p)^n\sum\limits_{k=1}^n k+\sum\limits_{k=1}^n k\mathbb{P}(Z=k) \\
&=\frac{n+1}{2}(1-p)^n+\mathbb{E}(Z) \\
&=\frac{n+1}{2}(1-p)^n+np
\end{align}

\noindent

**3.** Conditionnellement à $(X\neq 0)$, $Y$ prend ses valeurs dans $[\![1;n]\!]$, et pour tout $k\in [\![1;n]\!]$ on a :

\begin{align}
\mathbb{P}(Y=k|X\neq 0)&=\frac{\mathbb{P}(Y=k, X\neq 0)}{\mathbb{P}(X\neq 0)} \\
&=\frac{\mathbb{P}(Y=k)-\mathbb{P}(Y=k, X=0)}{1-\mathbb{P}(X=0)} \\
&=\frac{\mathbb{P}(Y=k)-\mathbb{P}(Y=k| X=0)\mathbb{P}(X=0)}{1-\mathbb{P}(X=0)} \\
&=\frac{\frac{1}{n}(1-p)^n+\binom{n}{k}p^k (1-p)^{n-k}-(1-p)^n\frac{1}{n}}{1-(1-p)^n} \\ 
&=\frac{\binom{n}{k}p^k (1-p)^{n-k}}{1-(1-p)^n}
\end{align}





:::: {.methbox .meth data-latex="important"}
**Exercice 3.6.** Une urne contient des boules numérotées $1$, $2$ et $3$. On tire $n$ boules une à une avec remise. Soient $X$ la variable aléatoire représentant le plus petit des numéros obtenus et $Y$ la variable aléatoire représentant le plus grand.

**1.** Calculer $\mathbb{P}(X\geq i)$ pour $i\in\{1,2,3\}$. En déduire la loi de $X$.

**2.** Calculer $\mathbb{P}(Y\leq j)$ pour tout $j\in\{1,2,3\}$. En déduire la loi de $Y$.

**3.** Les variables aléatoires $X$ et $Y$ sont-elles indépendantes ?

\vspace{0.5cm}
::::

\noindent

**Commentaire général sur cet exercice.** *Les questions 1 et 2 sont ultra classiques dans les sujets de probas-stat du concours. Il s'agit de calculer la probabilité que le $\max$ ou le $\min$ de $n$ variables aléatoires indépendantes soient inférieures ou supérieures à une certaine valeur. Dans ce cas, le schéma est toujours le même :*

- *on exprime l'événement sur le max ou le min comme une intersection d'événements ;* 
- *on utilise l'indépendance pour justifier que la probabilité de l'intersection est le produit des probabilités ;*
- *si de plus les variables aléatoires sont identiquement distribuées (elles sont alors i.i.d.) on utilise le fait que toutes ces probabilités sont égales..*

**Solution. 1.** Pour $k\in [\![1;n]\!]$, on note $X_k\in\{1,2,3\}$ le numéro obtenu lors du tirage numéro $k$. Ces tirages étant effectués avec remise, les variables $X_k$ sont indépendantes.

\noindent On a alors

$$\left \{
\begin{array}{rcl}
X&=\min(X_1,\dots, X_n) \\
Y&=\max(X_1,\dots X_n)\\
\end{array}
\right.$$

\noindent $X$ et $Y$ sont à valeurs dans $\{1,2,3\}$.

\noindent Pour $i\in\{1,2,3\}$, on a

\begin{align}
\mathbb{P}(X\geq i)&=\mathbb{P}\left(\min(X_1,\dots X_n)\geq i \right) \\
&=\mathbb{P}\left(\bigcap_{k=1}^n (X_k\geq i) \right) \\
&=\prod\limits_{k=1}^n\mathbb{P}(X_k\geq i) \\
&\text{(par indépendance)} \\
&=\left(1-\frac{i-1}{3}\right)^n \\
&=\left(\frac{4-i}{3}\right)^n \\
\end{align}

\noindent On a alors

\begin{align}
\mathbb{P}(X=1)&=\mathbb{P}(X\geq 1)-\mathbb{P}(X\geq 2) \\
&=1-\left(\frac{2}{3}\right)^n
\end{align}

\begin{align}
\mathbb{P}(X=2)&=\mathbb{P}(X\geq 2)-\mathbb{P}(X\geq 3) \\
&=\left(\frac{2}{3}\right)^n-\frac{1}{3^n} \\
&=\frac{2^n-1}{3^n} \\
\end{align}

\begin{align}
\mathbb{P}(X=3)&=\mathbb{P}(X\geq 3) \\
&=\frac{1}{3^n}
\end{align}

\noindent On vérifie de façon immédiate qu'on a bien $\sum\limits_{k=1}^3\mathbb{P}(X=k)=1$.

\noindent

**Commentaire.** *On pourrait aussi calculer $\mathbb{P}(X=3)$ en utilisant la formule $\mathbb{P}(X=3)=1-\mathbb{P}(X=1)-\mathbb{P}(X=2)$. Cependant, dans cet exemple une telle approche ne rendrait pas le calcul plus rapide et présenterait de plus l'inconvénient de masquer d'éventuelles erreurs sur les calculs préalables de $\mathbb{P}(X=1)$ et $\mathbb{P}(X=2)$.*


 \noindent
 
 **2.** Soit $j\in [\![1;3]\!]$. On a 
 
\begin{align}
\mathbb{P}(Y\leq j)&=\mathbb{P}\left(\max(X_1,\dots X_n)\leq j \right) \\
&=\mathbb{P}\left(\bigcap_{k=1}^n (X_k\leq j) \right) \\
&=\prod\limits_{k=1}^n\mathbb{P}(X_k\leq j) \\
&\text{(par indépendance)} \\
&=\left(\frac{j}{3}\right)^n \\
\end{align}

\noindent pour tout $j\in[\![1;3]\!]$ on a donc

\begin{align}
\mathbb{P}(Y=j)&=\mathbb{P}(Y\leq j)-\mathbb{P}(Y\leq j-1) \\
&=\left(\frac{j}{3}\right)^n-\left(\frac{j-1}{3}\right)^n
\end{align}


\noindent

**Commentaires. i.** *Si on veut être un tout petit peu plus explicite, on peut préciser que cette dernière expression est valable pour $j=2,3$, mais qu'elle l'est aussi a posteriori pour $j=1$ car $\mathbb{P}(Y\leq 1-1)=0=\left(\frac{1-1}{3}\right)^n$.*

\noindent

**ii.**  *Dans la question précédente, on aurait très bien pu écrire le résultat de $\mathbb{P}(X=i)$ sous une forme générique comme on le fait ici.*

**iii.** *Modulo un échange entre les symboles $\min$ et $\max$ d'une part, et $\geq$ et $\leq$ d'autre part, les questions 1 et 2 sont identiques. Dès lors qu'on a traité la question 1 soigneusement, on peut certainement se permettre de gagner un peu de temps et donner directement (si on le voit !) le résultat de la question 2, en évoquant par exemple un "raisonnement analogue" à celui de la question 1. De façon générale, il faut se dire qu'une rédaction efficace doit :*

*- convaincre le correcteur qu'on a bien compris ce qu'on faisait et qu'on sait rédiger de façon rigoureuse (l'un impliquant généralement l'autre) ;*

*- éviter les redites afin de gagner un peu de temps, sans trop sacrifier le point précédent. Sur un type de raisonnement que l'on a déjà rédigé proprement lors d'une question précédente, on peut probablement aller un peu plus vite sans s'attirer les foudres de la personne qui corrige la copie...*

\noindent

**3.** On a $X\leq Y$, donc $\mathbb{P}(X=2, Y=1)=0\neq\mathbb{P}(X=2)\mathbb{P}(Y=1)$. Les variables $X$ et $Y$ ne sont donc pas indépendantes.


:::: {.methbox .meth data-latex="important"}
**Exercice 3.7.** Soient $X$ et $Y$ deux variables aléatoires de Bernoulli indépendantes, de même paramètre $p\in ]0\,;\,1[$. On note $U=X+Y$ et $V=X-Y$.

**1.** Déterminer la loi du couple $(U,V)$.

**2.** Calculer le coefficient de corrélation $\rho_{U,V}$.

**3.** $U$ et $V$ sont-elles indépendantes ? Que peut-on conclure ?
::::

\noindent

**Solution. 1.** $U(\Omega)=[\![0;2]\!]$ et $V(\Omega)=[\![-1;1]\!]$. Par ailleurs :

$$(U,V)(\Omega)=\left\{(0;0),\, (1;-1),\,(1;0),\,(2;0)\right\}$$
\noindent et, par indépendance de $X$ et $Y$ :

\begin{align}
\mathbb{P}(U=0,V=0)&=\mathbb{P}(X=0,Y=0) \\
&=\mathbb{P}(X=0)\mathbb{P}(Y=0) \\
&=(1-p)^2 \\
\end{align}

\begin{align}
\mathbb{P}(U=1,V=-1)&=\mathbb{P}(X=0,Y=1) \\
&=\mathbb{P}(X=0)\mathbb{P}(Y=1) \\
&=(1-p)p\\
\end{align}

\begin{align}
\mathbb{P}(U=1,V=1)&=\mathbb{P}(X=1,Y=0) \\
&=\mathbb{P}(X=1)\mathbb{P}(Y=0) \\
&=p(1-p) \\
\end{align}

\begin{align}
\mathbb{P}(U=2,V=0)&=\mathbb{P}(X=1,Y=1) \\
&=\mathbb{P}(X=1)\mathbb{P}(Y=1) \\
&=p^2 \\
\end{align}

\noindent

**2.** 

\noindent

**Commentaire.** *La question précédente incite à utiliser la loi du couple $(U,V)$ pour :*

- *calculer $\mathbb{E}(UV)$ ;*
- *calculer les lois marginales de $U$ et $V$ et en déduire $\mathbb{E}(U)$,  $\mathbb{E}(V)$, $\sigma_U$ et $\sigma_V$ ;*
- *en déduire enfin  $cov(U,V)$, puis $\rho_{U,V}$.*

\noindent

*Mais tout cela serait assez long, et il est bien plus efficace ici d'utiliser les propriétés de bilinéarité et de symétrie de la covariance...*

\noindent 

\begin{align}
\text{cov}(U,V)&=\text{cov}(X+Y,X-Y) \\
&=\mathbb{V}(X)-\text{cov}(X,Y)+\text{cov}(Y,X)-\mathbb{V}(Y) \\
&=\mathbb{V}(X)-\mathbb{V}(Y) \\
&=0
\end{align}

\noindent et donc

$$\rho_{U,V}=0$$

\noindent 

**3.** $\mathbb{P}(U=2,V=1)=0$, car $(2,1)\not\in (U,V)(\Omega)$. Mais $\mathbb{P}(U=2)\mathbb{P}(V=1)\neq 0$ car aucun de ces facteurs n'est nul : 

\begin{align}
\mathbb{P}(U=2)&=\mathbb{P}(U=2,V=0) \\
&=p^2 \\
&\neq 0 \text{ car } p>0 \\
\end{align}

\begin{align}
\mathbb{P}(V=1)&=\mathbb{P}(U=1, V=1) \\
&=p(1-p) \\
&\neq 0 \text{ car } p\not\in\{0,1\} \\
\end{align}

\noindent Les variables $U$ et $V$ ne sont donc pas indépendantes.

\noindent On a donc construit un exemple de deux variables aléatoires non corrélées mais non indépendantes.





:::: {.methbox .meth data-latex="important"}
**Exercice 3.8.** Soient $X$ et $Y$ deux variables aléatoires à valeurs dans $\mathbb{N}$. On donne la loi du couple $(X,Y)$ :

$$\forall (i,j)\in\mathbb{N}^2, \, \mathbb{P}(X=i, Y=j)=\frac{(i+j)\lambda^{i+j}}{e\,i!\,j!}$$

**1.** Déterminer $\lambda$. Donner la loi de $X$.

**2.** Les variables $X$ et $Y$ sont-elles indépendantes ?
::::


\noindent

**Solution. 1. Heuristique.** Pour déterminer $\lambda$, on doit résoudre l'équation

$$\sum\limits_{i=0}^{+\infty}\sum\limits_{j=0}^{+\infty}\frac{(i+j)\lambda^{i+j}}{e\,i!j!}=1$$

\noindent Résoudre cette équation suppose d'étudier la convergence de la série double $\sum\limits_i\sum\limits_j\frac{(i+j)\lambda^{i+j}}{e\,i! j!}$. Comme souvent quand on a à montrer la convergence d'une série, le plus simple est de manipuler celle-ci au brouillon comme si on savait déjà qu'elle convergeait. De telles manipulations amènent souvent à voir émerger les conditions de sa convergence, et il ne reste plus alors qu'à transformer la suite de calculs informels en une démonstration rigoureuse.

\noindent Ici, on a envie d'écrire :

$$\sum\limits_{i=0}\sum\limits_{j=0}\frac{(i+j)\lambda^{i+j}}{e\,i!j!}=\frac{1}{e}\left(\sum\limits_{i=0}^{+\infty}\frac{i\lambda^i}{i!}\sum\limits_{j=0}^{+\infty}\frac{\lambda^{j}}{j!}+\sum\limits_{i=0}^{+\infty}\frac{\lambda^i}{i!}\sum\limits_{j=0}^{+\infty}\frac{j\lambda^{j}}{j!}\right)$$

\noindent soit encore

$$\sum\limits_{i=0}\sum\limits_{j=0}\frac{(i+j)\lambda^{i+j}}{e\,i!j!}=\frac{1}{e}\left(\sum\limits_{i=1}^{+\infty}\frac{\lambda^i}{(i-1)!}\sum\limits_{j=0}^{+\infty}\frac{\lambda^{j}}{j!}+\sum\limits_{i=0}^{+\infty}\frac{\lambda^i}{i!}\sum\limits_{j=1}^{+\infty}\frac{\lambda^{j}}{(j-1)!}\right)$$

\noindent On ne peut pas écrire ces égalités telles quelles, justement car on ignore si la série relative au membre de gauche est convergente. En revanche, toutes les séries relatives au membre de droite sont bien convergentes pour tout réel $\lambda$, car toutes sont égales, à un éventuel facteur près, à la série $\sum\limits_i\frac{\lambda^i}{i!}$, dont la somme vaut $\sum\limits_{=0}\frac{\lambda^i}{i!}=e^{\lambda}$. Mais puisque les séries du membre de droite sont convergentes, leur somme l'est aussi, autrement dit le membre de gauche est convergent, et donc par là même les égalités ci-dessus sont bien vraies.

\noindent On rédige maintenant notre solution.

**Solution rédigée.** Pour tout réel $\lambda$, la série $\sum\limits_i\frac{\lambda^i}{i!}$ converge et $\sum\limits_{i=0}\frac{\lambda^i}{i!}=e^{\lambda}$. Donc, la série

$$\sum\limits_i\sum\limits_j\frac{(i+j)\lambda^{i+j}}{e\,i!j!}=\frac{1}{e}\left(\sum\limits_{i}^{+\infty}\frac{\lambda^i}{(i-1)!}\sum\limits_{j}^{+\infty}\frac{\lambda^{j}}{j!}+\sum\limits_{i=0}^{+\infty}\frac{\lambda^i}{i!}\sum\limits_{j=1}^{+\infty}\frac{\lambda^{j}}{(j-1)!}\right)$$
\noindent est convergente, et

\begin{align}
\sum\limits_{i=0}\sum\limits_{j=0}\frac{(i+j)\lambda^{i+j}}{e\,i!j!}&=\frac{1}{e}\left(\sum\limits_{i=1}^{+\infty}\frac{\lambda^i}{(i-1)!}\sum\limits_{j=0}^{+\infty}\frac{\lambda^{j}}{j!}+\sum\limits_{i=0}^{+\infty}\frac{\lambda^i}{i!}\sum\limits_{j=1}^{+\infty}\frac{\lambda^{j}}{(j-1)!}\right) \\
&=\frac{1}{e}2\lambda e^{2\lambda}
\end{align}

\noindent On cherche donc un réel $\lambda$ tel que

$$2\lambda e^{2\lambda}=e$$


\noindent La fonction $x\mapsto x e^x$ est dérivable sur $\mathbb{R}$, et pour tout réel $x$ on a $f'(x)=(1+x)e^x$. On en déduit que $f'<0$ - et donc $f$ est strictement décroissante - sur $]-\infty\,;\,-1[$ et $f'>0$ - et donc $f$ est strictement croissante - sur $]-1\,;\,+\infty[$. Comme par ailleurs $\lim\limits_{x\to -\infty}f(x)=0$ et $\lim\limits_{x\to +\infty}f(x)=+\infty$, l'équation $f(x)=e$ admet une unique solution sur $\mathbb{R}$. Or, $f(1)=e$, et donc cette unique solution est $\lambda=\frac{1}{2}$.

\noindent Déterminons maintenant la loi de $X$. Soit $i\in\mathbb{N}$. On a

\begin{align}
\mathbb{P}(X=i)&=\sum\limits_{j=0}^{+\infty}\mathbb{P}(X=i, Y=j) \\
&=\sum\limits_{j=0}^{+\infty}\frac{i+j}{e\,i!j!}\frac{1}{2^{i+j}} \\
&=\frac{i}{e\,i!2^i}\sum\limits_{j=0}^{+\infty}\frac{1}{j!}\frac{1}{2^j}+\frac{1}{e\,i!2^i}\sum\limits_{j=0}^{+\infty}\frac{j}{j!}\frac{1}{2^j} \\
&=\frac{i}{e\,i! 2^i}e^{\frac{1}{2}}+\frac{1}{e\,i!2^i.2}e^{\frac{1}{2}} \\
&=\frac{2i+1}{\sqrt{e}\,2^{i+1}\,i!} \\
\end{align}

\noindent

**2.** Par symétrie des rôles joués par $X$ et $Y$, on a

$$\forall j\in\mathbb{N}, \, \mathbb{P}(Y=j)=\frac{2j+1}{\sqrt{e}\,2^{j+1}\,j!}$$
\noindent On en déduit que

$$\mathbb{P}(X=0)\mathbb{P}(Y=0)=\frac{1}{4e}$$
\noindent alors que

$$\mathbb{P}(X=0, Y=0)=0$$

\noindent Ainsi, $\mathbb{P}(X=0, Y=0)\neq\mathbb{P}(X=0)\mathbb{P}(Y=0)$, et donc les variables aléatoires $X$ et $Y$ ne sont pas indépendantes.












:::: {.methbox .meth data-latex="important"}
**Exercice 3.9.** Soient $X$ et $Y$ deux variables aléatoires indepéndantes de loi de Poisson de paramètres respectifs $\lambda$ et $\mu$. Déterminer la loi conditionnelle de $X$ lorsque la somme $S=X+Y$ a une valeur finie $S=s$. En déduire l'expression de la fonction de régression de $X$ sur $S$ puis la valeur de $\mathbb{E}(\mathbb{E}(X|S))$.
::::

\noindent

**Solution.** $S(\Omega)=\mathbb{N}$, on cherche donc la loi $\mathcal{L}(X|X+Y=s)$ pour $s\in\mathbb{N}$. Conditionnellement à l'événement $(X+Y=s)$, $X$ est à valeurs dans $[\![0\,;\,s]\!]$.

\noindent Soit $k\in[\![0\,;\,s]\!]$, calculons $\mathbb{P}(X=k|X+Y=s)$. On a :

\begin{align}
\mathbb{P}(X=k|X+Y=s)&=\frac{\mathbb{P}(X=k, X+Y=s)}{\mathbb{P}(X+Y=s)} \\
&=\frac{\mathbb{P}(X=k, Y=s-k)}{\mathbb{P}(X+Y=s)} \\
&=\frac{\mathbb{P}(X=k)\mathbb{P}(Y=s-k)}{\mathbb{P}(X+Y=s)} \\
&\text{(car } X \text{ et } Y \text{ sont indépendantes)} \\
\end{align}

\noindent On utilise le résultat classique suivant :

:::: {.methbox .meth data-latex="important"}
**Somme de deux lois de Poisson indépendantes.** Soient $\lambda$ et $\mu$ deux réels strictement positifs et $X$ et $Y$ deux variables indépendantes telles que 

$$X\sim\mathcal{P}(\lambda)$$
$$Y\sim\mathcal{P}(\mu)$$
\noindent Alors :

$$X+Y\sim\mathcal{P}(\lambda+\mu)$$
::::

\noindent 

**Démonstration.** $X+Y(\Omega)=\mathbb{N}$. Comme $X$ et $Y$ sont indépendantes, la loi de $X+Y$ est donnée par le produit de convolution entre $\mathbb{P}_X$ et $\mathbb{P}_Y$ :

$$\mathbb{P}_{X+Y}=\mathbb{P}_X*\mathbb{P}_Y$$

Soit $k\in\mathbb{N}$. On a donc

\begin{align}
\mathbb{P}(X+Y=k)&=\sum\limits_{i\in\mathbb{N}}\mathbb{P}(X=i)\,\mathbb{P}(Y=k-i) \\
&=\sum\limits_{i=0}^k\mathbb{P}(X=i)\,\mathbb{P}(Y=k-i) \\
&=\sum\limits_{i=0}^k\frac{\lambda^i}{i!}e^{-\lambda}\frac{\mu^{k-i}}{(k-i)!}e^{-\mu} \\
&=e^{-(\lambda+\mu)}\mu^k\sum\limits_{i=0}^k\left(\frac{\lambda}{\mu}\right)^i\frac{1}{i!}\frac{1}{(k-i)!} \\
&=e^{-(\lambda+\mu)}\frac{\mu^k}{k!}\sum\limits_{i=0}^k\binom{k}{i}\left(\frac{\lambda}{\mu}\right)^i \\
&=e^{-(\lambda+\mu)}\frac{\mu^k}{k!}\left(1+\frac{\lambda}{\mu}\right)^k \\
&=e^{-(\lambda+\mu)}\frac{(\lambda+\mu)^k}{k!} \\
\end{align}

\noindent et on donc on a bien $X+Y\sim\mathcal{P}(\lambda+\mu)$.

$\square$

\noindent

**Commentaire.** *Ce résultat peut être considéré comme du cours. En pratique, il faut donc le connaître, mais aussi savoir le démontrer.*

\noindent On revient au calcul de $\mathbb{P}(X=k|X+Y=s)$. On a donc :

\begin{align}
\mathbb{P}(X=k|X+Y=s)&=\frac{\mathbb{P}(X=k)\mathbb{P}(Y=s-k)}{\mathbb{P}(X+Y=s)} \\
&=\frac{\lambda^k e^{-\lambda}\,\mu^{s-k} e^{-\mu}}{k!\,(s-k)!\,\frac{(\lambda+\mu)^s}{s!}e^{-(\lambda+\mu)}} \\
&=\frac{s!}{k!\,(s-k)!}\,\frac{\lambda^k\,\mu^{s-k}}{(\lambda+\mu)^s} \\
&=\binom{s}{k}\left(\frac{\lambda}{\lambda+\mu}\right)^k\,\left(\frac{\mu}{\lambda+\mu}\right)^{s-k} \\
&=\binom{s}{k}p^k\,(1-p)^{s-k}\\
\end{align}

\noindent où $p=\frac{\lambda}{\lambda+\mu}$.

\noindent Ainsi, on a

$$X|X+Y=s\sim\mathcal{B}\left(s\,;\,\frac{\lambda}{\lambda+\mu}\right)$$
\noindent On en déduit que

$$\forall s\in\mathbb{N},\,\mathbb{E}(X|X+Y=s)=\frac{\lambda}{\lambda+\mu}s$$

\noindent Par conséquent, $\mathbb{E}(X|S)$ est la variable aléatoire

$$\mathbb{E}(X|S)=\frac{\lambda}{\lambda+\mu}S$$
\noindent Elle admet une espérance, et

\begin{align}
\mathbb{E}\left(\mathbb{E}(X|S)\right)&=\frac{\lambda}{\lambda+\mu}\mathbb{E}(S) \\
&=\frac{\lambda}{\lambda+\mu}(\lambda+\mu) \\
&=\lambda
\end{align}




## Variables à densité


:::: {.methbox .meth data-latex="important"}
**Exercice 4.1.** Soit $X$ une variable aléatoire suivant une loi exponentielle de paramètre $\frac{1}{2}$.

**1.a.** Déterminer une densité de $Y=\sqrt{X}$.

**b.** Déterminer l'espérance et la variance de $Y$.

**2.** Déterminer une densité de $Z=X^2$.
::::

\noindent

**Solution.** Dans tout cet exercice, pour toute variable aléatoire $U$ à densité, on notera $f_U$ sa fonction de densité et $F_U$ sa fonction de répartition.

\noindent 

**1.a.** $X$ a pour densité la fonction $f_X$ définie par

$$\forall x\in\mathbb{R},\,f_X(x)=\frac{1}{2}e^{-\frac{x}{2}}.\mathbb{1}_{\mathbb{R}_+}(x)$$
\noindent et pour fonction de répartition la fonction $F_X$ définie par

$$\forall x\in\mathbb{R},\,F_x(x)=\left(1-e^{-\frac{x}{2}}\right).\mathbb{1}_{\mathbb{R}_+}(x)$$

\noindent Soit $y$ un réel. 

\noindent Si $y<0$ alors $F_Y(y)=0$. 

\noindent Si $y\geq 0$, alors

\begin{align}
F_Y(y)&=\mathbb{P}(Y\leq y) \\
&=\mathbb{P}(\sqrt{X}\leq y) \\
&=\mathbb{P}(0\leq X\leq y^2) \\
&=F_X(y^2) \text{ car } F_X(0)=0 \\
&=1-e^{-\frac{y^2}{2}} \\
\end{align}

\noindent $F_Y$ est dérivable sur $\mathbb{R}_+^*$ et donc $Y$ admet une densité donnée par $f_Y=F_Y'$ :

\begin{align}
f_Y(y)&=F_Y'(y) \\
&=y.e^{-\frac{y^2}{2}}
\end{align}

\noindent D'où l'expression générale de $f_Y(y)$, quel que soit le réel $y$ :

$$f_y(y)=y.e^{-\frac{y^2}{2}}.\mathbb{1}_{\mathbb{R}_+}(y)$$

\noindent

**b.** $Y$ admet une espérance si et seulement si l'intégrale $\int_0^{+\infty}y^2e^{-\frac{y^2}{2}}\,dy$ est convergente. Or, si $U$ suit une loi normale standard, la variance de $U$ s'écrit comme l'intégrale $\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}y^2e^{-\frac{y^2}{2}}\,dy$, autrement dit cette intégrale vaut $1$. On en déduit que $Y$ admet une espérance et que

$$\mathbb{E}(Y)=\sqrt{\frac{\pi}{2}}$$

\noindent De même, $Y$ admet un moment d'ordre $2$, et donc une variance, si et seulement si l'intégrale $\int_{0}^{+\infty}y^3e^{-\frac{y^2}{2}}\,dy$ est convergente.

\noindent Soit $A>0$. On a 

$$\int_0^A y^3e^{-\frac{y^2}{2}}\,dy=\int_0^A y^2.\left(ye^{-\frac{y^2}{2}}\right)\,dy$$

\noindent Les fonctions $y\mapsto y^2$ et $y\mapsto ye^{-\frac{y^2}{2}}$ sont de classe $\mathcal{C}^1$ sur le segment $[0\,;\,A]$, ce qui va nous permettre d'effectuer une intégration par parties :

- on dérive $u:y\mapsto y^2$ : $u'(y)=2y$ ;
- on intègre $v':y\mapsto ye^{-\frac{y^2}{2}}$ en posant $v(y)=-e^{-\frac{y^2}{2}}$.

\noindent On en déduit que

\begin{align}
\int_0^A y^3e^{-\frac{y^2}{2}}\,dy&=\left[-y^2e^{-\frac{y^2}{2}}\right]_0^A+2\int_0^A ye^{-\frac{y^2}{2}}\, dy \\
&=-A^2e^{-\frac{A^2}{2}}+2-2e^{-\frac{A^2}{2}} \\
&\longrightarrow_{A\to +\infty} 2
\end{align}

\noindent Ainsi, $Y$ admet un moment d'ordre $2$ et 

$$\mathbb{E}(Y^2)=2$$

\noindent D'après la formule de König-Huygens, on a alors

\begin{align}
\mathbb{V}(Y)&=\mathbb{E}(Y^2)-\mathbb{E}(Y)^2 \\
&=2-\frac{\pi}{2} \\
\end{align}

\noindent

\noindent 

**Commentaire.** *Dans le cadre du programme, l'intégration par parties s'applique sur un segment. Pour une intégrale impropre comme celle de cette question, on fixe donc une (ou deux) borne(s) finie(s), on transforme l'intégrale sur le segment constitué par ces deux bornes par intégration par parties, et seulement après on fait tendre cette (ou ces) borne(s) vers la (les) valeur(s) souhaitée(s).*

**2.** $Z(\Omega)=\mathbb{R}_+$ et, pour $z\geq 0$ on a 

\begin{align}
F_Z(z)&=\mathbb{P}(Z\leq z) \\
&=\mathbb{P}(X^2\leq z) \\
&=\mathbb{P}\left(0\leq X\leq\sqrt{Z}\right) \\
&=F_X\left(\sqrt{z}\right) \, \text{ car } F_X(0)=0 \\
&=1-e^{-\frac{\sqrt{z}}{2}} \\
\end{align}

$F_Z$ est dérivable sur $\mathbb{R}_+^*$ et donc $Z$ admet une densité, donnée par $f_Z=F_Z'$. Pour tout $z\geq 0$ on a 

\begin{align}
f_Z(z)&=F_Z'(z) \\
&=\frac{1}{4\sqrt{z}}e^{-\frac{\sqrt{z}}{2}} \\
\end{align}

\noindent On a donc

$$f_Z(z)=\frac{1}{4\sqrt{z}}e^{-\frac{\sqrt{z}}{2}}.1_{\mathbb{R}_+}(z)$$

\noindent

**Commentaire sur cet exercice.** *Dans la question 1.a., tout comme dans la question 2., on cherche la densité d'une transformation $\varphi(X)$ d'une variable aléatoire $X$ dont on connaît la loi. Ce type de question revient très fréquemment dans les sujets de concours. La stratégie qui fonctionne souvent le mieux est de calculer d'abord la fonction de répartition, puis de la dériver afin d'obtenir la fonction de densité.* 

*Pour pouvoir appliquer cette méthode, on doit :*

- *s'assurer que $\varphi$ est strictement monotone sur $X(\Omega)$ : si c'est bien le cas, alors en particulier elle réalise une bijection de $\Omega$ sur $X(\Omega)$ ;*

- *calculer son inverse $\varphi^{-1}$ sur $X(\Omega)$.*

*En pratique, ces deux points sont assez souvent immédiats, comme c'est le cas dans cet exercice. Alors, on peut transformer le calcul de la probabilité $\mathbb{P}(\varphi(X)\leq a)$ en*

- *$\mathbb{P}(X\leq\varphi^{-1}(\{a\}))$ si $X$ est strictement croissante sur $X(\Omega)$* ;

- *$\mathbb{P}(X\geq\varphi^{-1}(\{a\}))$ si $X$ est strictement décroissante sur $X(\Omega)$.*







:::: {.methbox .meth data-latex="important"}
**Exercice 4.2.** Soient $X$ et $U$ deux variables aléatoires indépendantes, où $X\sim\mathcal{N}(0,1)$ et $U\sim\mathcal{U}_{\{-1,1\}}$. On définit $Y=UX$.

**1.** Démontrer que, pour tout réel $x$ :

$$\mathbb{P}(Y\leq x)=\frac{1}{2}\mathbb{P}(X\leq x)+\frac{1}{2}\mathbb{P}(X\geq -x)$$

**2.** En déduire que $Y$ suit une loi normale centrée réduite.

**3.** Calculer l'espérance de $U$ puis démontrer que $\mathbb{E}(XY)=0$.

**4.** En déduire que $\text{Cov}(X,Y)=0$.
::::



:::: {.methbox .meth data-latex="important"}
**Exercice 4.3. 1.** Soient $X$ et $Y$ deux variables aléatoires indépendantes de loi commune $\mathcal{E}(\lambda)$, avec $\lambda>0$. Déterminer une densité de $X+Y$.

**2.** Plus généralement, soient $X_1,\dots,X_n$ des variables aléatoires indépendantes de même loi $\mathcal{E}(\lambda)$. Pour tout entier $n\geq 1$, on note

$$S_n=\sum\limits_{k=1}^n X_k$$

\noindent Démontrer qu'une densité de $S_n$ est donnée par

$$f_n(x)=\left \{
\begin{array}{c @{} c}
    \frac{\lambda^n}{(n-1)!}\,x^{n-1}\,e^{-\lambda x} & \text{ si } x\geq 0 \\
    0 & \text{ si }\lambda<0
\end{array}
\right.$$
::::


:::: {.methbox .meth data-latex="important"}
**Exercice 4.4.** Soient $X$ et $Y$ deux variables indépendantes de loi normale centrée réduite (de densité $\varphi$ et de fonction de répartition $\Phi$). On pose

$$Z=\text{sup}(X,Y)$$

**1.a.** Démontrer que $Z$ est une variable à densité.

**b.** Vérifier que $Z$ admet pour densité $f$ définie, pour tout réel $x$, par $f(x)=2\,\varphi(x)\,\Phi(x)$.

**2.a.** Justifier la convergence et donner la valeur de $\int_{-\infty}^{+\infty}e^{-t^2}\,dt$.

**b.** En remarquant que $\varphi'(x)=-x\,\varphi(x)$, démontrer que

$$\int_{0}^{+\infty}x\,f(x)\,dx=\frac{1}{\sqrt{2\pi}}+\frac{1}{\pi}\int_{0}^{+\infty}e^{-x^2}\,dx$$
    
**c.** Vérifier de même que

$$\int_{-\infty}^{0}x\,f(x)\,dx=-\frac{1}{\sqrt{2\pi}}+\frac{1}{\pi}\int_{-\infty}^{0}e^{-x^2}\,dx$$

En déduire que $Z$ admet une espérance et donner sa valeur.

**3.a.** Démontrer que $X^2$ et $Z^2$ suivent la même loi.

**b.** Déterminer $\mathbb{E}(X^2)$ puis donner la valeur de $\mathbb{V}(Z)$.
::::



:::: {.methbox .meth data-latex="important"}
**Exercice 4.5.** Soit $(X,Y)$ un couple de variables aléatoires, de densité 

$$f(x,y)=\left \{
\begin{array}{c @{} c}
    \frac{k}{\sqrt{xy}} & \text{ si } 0<x\leq y<1 \\
    0 & \text{ sinon }
\end{array}
\right.$$

**1.** Déterminer la valeur de $k$ ainsi que la fonction de répartition du couple $(X,Y)$.

**2.** Déterminer les lois marginales de $X$ et $Y$. Ces variables sont-elles indépendantes ?

**3.** Déterminer les lois conditionnelles de $X|Y=y$ et de $Y|X=x$. En déduire l'expression de la fonction de régression $x\mapsto\mathbb{E}(Y|X=x)$, puis calculer $\mathbb{E}(\mathbb{E}(Y|X))$.
::::



## Annales des oraux

### Interne, 2016

<center>
```{r, echo = FALSE}
knitr::include_graphics("C:/Users/olivier.guin/Travail/Formation_Administrateur/Cours/Cours_probabilités_statistique/images/oral_2016_probas_01.PNG") 
```
</center>

\noindent 

**Solution. 1.** 
\begin{align}
\mathbb{P}(Y>X) &= \mathbb{E}(1_{Y>X}) \\
&= \int_{\mathbb{R}_+^2} 1_{y>x}\lambda e^{-\lambda x}\lambda e^{-\lambda y} \, \text{dy dx  (par indépendance)} \\
&= \lambda\int_{0}^{+\infty}e^{-\lambda x}\int_{x}^{+\infty}\lambda e^{-\lambda y}\,\text{dy dx} \\
&= \int_{0}^{+\infty} e^{-\lambda x}\,\lambda e^{-\lambda x}\,\text{dx} \\
&= \int_{0}^{+\infty} e^{-\lambda y} f_{\lambda}(y)\text{ dy} \text{ (où } f_{\lambda} \text{ est la densité de la loi } \mathcal{E}(\lambda) \text{)} \\
&= \mathbb{E}(e^{-\lambda X}) \\
\end{align}


