# Convergence

\noindent Dans tout ce qui suit, toutes les variables aléatoires considérées sont définies sur un espace probabilisé $(\Omega, \mathcal{A}, \mathbb{P})$ donné, et sont à valeurs dans $\mathbb{R}$ (ou éventuellement $\mathbb{R}^d$, avec $d\in\mathbb{R}^{*}$).

## Différents modes de convergence

### Convergence en probabilité

:::: {.defbox .def data-latex="important"}
<center>**Convergence en probabilité**</center>

Soient $(X_n)_{n\in\mathbb{N}}$ une suite de variables aléatoires. On dit que $(X_n)_n$ **converge en probabilité** vers une variable aléatoire $X$ lorsque :

$$\forall\varepsilon >0,\,\lim\limits_{n\to +\infty}\mathbb{P}\left(|X_n-X|\geq\varepsilon\right)=0$$

\noindent 
**Notation :** $$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$$
::::

\noindent

**Remarques.** De façon équivalente, $(X_n)$ converge en probabilité vers $X$ lorsque

$$\forall\varepsilon >0,\,\lim\limits_{n\to +\infty}\mathbb{P}\left(|X_n-X|<\varepsilon\right)=1$$

\noindent

**Exemple.** Soit $X:\Omega\longrightarrow\mathbb{R}$ une variable aléatoire. On considère la suite de variables aléatoires $(X_n)_n$ définies par $X_n=\frac{E(nX)}{n}$, où $E(.)$ désigne la fonction partie entière. Démontrons que $(X_n)_n$ converge en probabilité vers $X$.

\noindent On a

\begin{align*}
\left|X-\frac{E(nX)}{n}\right|&=\left|nX-\frac{E(nX)}{n}\right| \\
&\leq\frac{1}{n}
\end{align*}

\noindent car tout réel $x$ est à une distance au plus $1$ de sa partie entière : $\forall x\in\mathbb{R}, |x-E(x)|\leq 1$.

\noindent Soit $\varepsilon >0$. Comme $\lim\limits_{n\to +\infty}\frac{1}{n}=0$, il existe un entier naturel $n_0$ non nul tel que

$$\forall n\in\mathbb{N}, n\geq n_0\Rightarrow\frac{1}{n}<\varepsilon$$
\noindent et donc en particulier

$$\forall n\in\mathbb{N}, n\geq n_0\Rightarrow\left|X-\frac{E(nX)}{n}\right|<\varepsilon$$
\noindent d'où

$$\forall n\in\mathbb{N}, n\geq n_0\Rightarrow\mathbb{P}\left(\left|X-\frac{E(nX)}{n}\right|<\varepsilon\right)=1$$
\noindent et donc finalement

$$\lim\limits_{n\to +\infty}\mathbb{P}\left(\left|X-\frac{E(nX)}{n}\right|<\varepsilon\right)=1$$
\noindent autrement dit

$$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$$

$\square$

\noindent Les inégalités de concentration constituent souvent un outil efficace pour démontrer une convergence en probabilité :

:::: {.methbox .def data-latex="important"}
<center>**Inégalité de Markov pour démontrer que $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$**</center>

\noindent On suppose que :

- chacune des variables $X_n$ et $X$ admet une espérance
- $\mathbb{E}(|X_n-X|^p)\underset{n \to +\infty}{\overset{}{\longrightarrow}}0$, pour un certain $p>0$

\noindent Le premier point permet d'écrire l'inégalité de Markov (appliquée à la variable aléatoire positive $|X_n-X|^p$) :

\begin{align*}
\mathbb{P}\left(|X_n-X|\geq\varepsilon\right)&=\mathbb{P}\left(|X_n-X|^p\geq\varepsilon^p\right) \\
&\leq\frac{\mathbb{E}\left(|X_n-X|^p\right)}{\varepsilon^p}
\end{align*}

\noindent la première égalité venant du fait que l'application $x\mapsto x^p$ est strictement croissante sur $\mathbb{R}_+$.

\noindent Le deuxième point permet alors de conclure que 

$$\forall\varepsilon >0,\, \mathbb{P}(|X_n-m|\geq\varepsilon) \underset{n \to +\infty}{\overset{}{\longrightarrow}}0$$ 

\noindent autrement dit que

$$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$$
::::

\noindent 

**Remarque.** On vient de démontrer que la convergence dans $L^p$ (ce mode de convergence est défini juste après) pour $p>0$ implique la convergence en probabilité.

\noindent

**Exemple. [Max de lois uniformes indépendantes].** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. de loi uniforme $\mathcal{U}([0\,;\,1])$. On pose

$$Y_n=\max(X_1,\dots,X_n)$$
\noindent Démontrons que $(Y_n)_n$ converge en probabilité vers $1$ en utilisant l'inégalité de Markov appliquée à la variable aléatoire positive $1-Y_n$.

\noindent Pour cela, on a besoin de l'espérance de $1-Y_n$, et donc de l'espérance de $Y_n$. Pour calculer $\mathbb{E}(Y_n)$ on commence par calculer la fonction de répartition $F_{Y_n}$ de $Y_n$. Soit $y$ un réel.

\noindent Si $y\leq 0$ on a $F_{Y_n}(y)=0$ et si $y>1$ on a $F_{Y_n}(y)=0$.

\noindent Soit maintenant $y\in [0,1]$. Alors, on a

\begin{align*}
F_{Y_n}(y)&=\mathbb{P}(Y_n\leq y) \\
&=\mathbb{P}(\max(X_1,\dots, X_n)\leq y) \\
&=\mathbb{P}\left(\bigcap\limits_{i=1}^n \{X_i\leq y\} \right) \\
&=\prod\limits_{i=1}^n\mathbb{P}(X_i\leq x) \\
&=\prod\limits_{i=1}^n x \\
&=x^n
\end{align*}

\noindent donc la densité $f_{Y_n}$ de $Y_n$ est donnée par

$$f_{Y_n}(y)=nx^{n-1}.1_{[0,1]}(y)$$

\noindent D'où l'espérance de $Y_n$ :

\begin{align*}
\mathbb{E}(Y_n)&=\int_0^1 y.f_{Y_n}(y)\,dy \\
&=n\int_0^1 y^n\,dy \\
&=\frac{n}{n+1}
\end{align*}

\noindent Soit $\varepsilon >0$. L'inégalité de Markov appliquée à la variable $|Y_n-1|$ donne

\begin{align*}
\mathbb{P}(|Y_n-1|\geq\varepsilon)\leq\frac{\mathbb{E}(1-Y_n)}{\varepsilon^2} \\
&=\frac{1-\frac{n}{n+1}}{\varepsilon^2} \\
&=\frac{1}{(n+1)\varepsilon^2}
\end{align*}

\noindent Or, $\frac{1}{(n+1)\varepsilon^2}\underset{n \to +\infty}{\overset{}{\longrightarrow}}0$, ce qui permet de conclure que $(Y_n)_n$ converge bien en probabilité vers $1$.

$\square$


:::: {.methbox .def data-latex="important"}
<center>**Inégalité de Bienaymé-Tchebytchev pour démontrer que $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}m$**</center>

\vspace{0.5cm}

\noindent Cette inégalité peut être utilisée pour démontrer la convergence en probabilité vers une constante. On suppose que :

-  les variables aléatoires $X_n$ admettent une espérance commune $\mathbb{E}(X_n)=m$

- chacune d'elles admet également une variance $\mathbb{V}(X_n)$

- $\lim\limits_{n\to +\infty}\mathbb{V}(X_n)=0$.

\noindent Les deux premiers points permettent d'écrire l'inégalité de Bienaymé-Tchebytchev :

$$\forall\varepsilon >0, \,\mathbb{P}\left(|X_n-m|\geq\varepsilon\right)\leq\frac{\mathbb{V}(X_n)}{\varepsilon^2}$$
\noindent Le dernier point permet de conclure que

$$\forall\varepsilon >0,\, \mathbb{P}(|X_n-m|\geq\varepsilon) \underset{n \to +\infty}{\overset{}{\longrightarrow}}0$$ 

\noindent autrement dit que

$$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}m$$
::::

\noindent

**Exemple. [Convergence en probabilité de la moyenne empirique].** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. de loi $\mathcal{E}(\lambda)$, où $\lambda>0$. On considère, pour tout entier naturel $n\geq 1$ la moyenne empirique

$$\overline{X_n}=\frac{1}{n}\sum\limits_{i=1}^n X_i$$
\noindent Démontrons que $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\frac{1}{\lambda}$. Ce résultat est une conséquence immédiate de la loi faible des grands nombres (que l'on énonce un peu plus loin), mais ici nous allons procéder autrement en utilisant l'inégalité de Bienaymé-Tchebytchev.

\noindent Tout d'abord, la limite supposée $\frac{1}{\lambda}$ est une constante, ce qui incite à penser à cette inégalité. Ensuite, les $(X_n)_n$ admettent toutes une espérance et une variance - ce qui confirme que nous pouvons utiliser cette inégalité - et

- $\forall n\in\mathbb{N}^*,\,\mathbb{E}(X_n)=\frac{1}{\lambda}$

- $\forall n\in\mathbb{N}^*, \mathbb{V}(X_n)=\frac{1}{\lambda^2}$

\noindent On en déduit par linéarité que $\mathbb{E}(\overline{X_n})=m$ et par indépendance des $X_n$ que

\begin{align*}
\mathbb{V}\left(\overline{X_n}\right)&=\frac{1}{n^2}\sum\limits_{i=1}^n \mathbb{V}(X_i) \\
&=\frac{1}{n^2}\sum\limits_{i=1}^n\frac{1}{\lambda^2} \\
&=\frac{1}{n\lambda^2}
\end{align*}

\noindent L'utilisation de l'inégalité de Bienaymé-Tchebytchev donne donc, pour tout $\varepsilon >0$ :

\begin{align}
\mathbb{P}\left(\left|\overline{X_n}-\frac{1}{\lambda}\right|\geq\varepsilon\right)&\leq\frac{\mathbb{V}\left(\overline{X_n}\right)}{\varepsilon^2} \\
&=\frac{1}{\varepsilon^2 n\lambda^2\varepsilon^2}
\end{align}

\noindent et donc

$$\lim\limits_{n\to +\infty}\mathbb{P}\left(\left|\overline{X_n}-\frac{1}{\lambda}\right|\geq\varepsilon\right)=0$$
\noindent ce qui permet de conclure que $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\frac{1}{\lambda}$.


\noindent Le résultat suivant fournit un critère simple pour démontrer une convergence en probabilité. Il s'agit d'une condition suffisante, qui fonctionne assez souvent en pratique.


:::: {.thmbox .thm data-latex="important"}
<center>**Convergence en probabilité : conditions suffisantes**</center>

\noindent Soient $a$ un nombre réel et $(X_n)_{n}$ une suite de variables aléatoires admettant une espérance et une variance. 

\noindent Si

$$\mathbb{E}(X_n) \underset{n \to +\infty}{\overset{}{\longrightarrow}}a$$

et 

$$\mathbb{V}(X_n) \underset{n \to +\infty}{\overset{}{\longrightarrow}}0$$
alors

$$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}a$$
::::

\noindent

**Démonstration.** 

\noindent

**Exemple.** 



**Parler de Markov et de Bieanymé-Tchebytchev**



### Convergence dans les espaces $L^1$ et $L^2$


\noindent A nouveau, sans que cela ne soit plus mentionné par la suite, nous supposons ici que toutes les variables aléatoires évoquées sont définies sur un même espace probabilisé $(\Omega, \mathcal{A}, \mathbb{P})$. 

\noindent Par ailleurs, ces variables sont à valeurs dans $\mathbb{R}$. On pourrait toutefois facilement étendre les définitions qui suivent à des variables à valeurs dans $\mathbb{R}^d$ en remplaçant les valeurs absolues $|X_n-X|$ par des normes $||X_n-X||$ définies dans $\mathbb{R}^d$.


#### Les ensembles $L^1(\Omega)$ et $L^2(\Omega)$

:::: {.defbox .def data-latex="important"}
<center>**Espaces $L^1(\Omega)$ et $L^2(\Omega)$**</center>

\noindent Soit $X$ une variable aléatoire définie sur $(\Omega, \mathcal{A}, \mathbb{P})$ et à valeurs dans $\mathbb{R}$.

\vspace{0.5cm}

\noindent 

<center>**Espace $L^1(\Omega)$**</center>


\noindent On dit que $X\in L^1(\Omega)$, ou encore que $X\in L^1(\Omega, \mathcal{A}, \mathbb{P})$ lorsque

$$\mathbb{E}(|X|)<+\infty$$

\noindent ce qui revient à dire que $X$ admet une espérance. Selon le cas (discret fini, discret infini, à densité), cette condition s'écrit aussi sous l'une des formes suivantes :

$$\sum\limits_{n=0}^N \mathbb{P}(X=x_n).|x_n|<+\infty$$
$$\sum\limits_{n=0}^{+\infty} \mathbb{P}(X=x_n).|x_n|<+\infty$$

$$\int_{-\infty}^{+\infty}|x|.f(x)\,dx<+\infty$$
\noindent avec $f$ la densité de $X$ dans le cas à densité.


\vspace{1cm}


<center>**Espace $L^2(\Omega)$**</center>


\noindent On dit que $X\in L^2(\Omega)$, ou encore que $X\in L^2(\Omega, \mathcal{A}, \mathbb{P})$ lorsque

$$\mathbb{E}(X^2)<+\infty$$

\noindent ce qui revient à dire que $X$ admet un moment d'ordre 2. Selon le cas (discret fini, discret infini, à densité), cette condition s'écrit aussi sous l'une des formes suivantes :

$$\sum\limits_{n=0}^N \mathbb{P}(X=x_n).x_n^2<+\infty$$
$$\sum\limits_{n=0}^{+\infty} \mathbb{P}(X=x_n).x_n^2<+\infty$$

$$\int_{-\infty}^{+\infty}x^2.f(x)\,dx<+\infty$$

\noindent avec $f$ la densité de $X$ dans le cas à densité.


::::

\noindent 

**Remarques. i.** Cette définition est suffisante pour le concours


\noindent 

**ii.** Plus généralement, pour tout $p>0$ on peut définir l'ensemble $L^p(\Omega)$ comme l'ensemble des variables aléatoires réelles définies sur $(\Omega, \mathcal{A}, \mathbb{P})$ telles que

$$\mathbb{E}(|X|^p)<+\infty$$
et cette condition s'écrit, selon les cas, sous l'une des formes suivantes : 

$$\sum\limits_{n=0}^N \mathbb{P}(X=x_n).|x_n|^p<+\infty$$
$$\sum\limits_{n=0}^{+\infty} \mathbb{P}(X=x_n).|x_n|^p<+\infty$$

$$\int_{-\infty}^{+\infty}|x|^p.f(x)\,dx<+\infty$$
\noindent


\noindent Le résultat qui suit est très classique :

:::: {.thmbox .thm data-latex="important"}

\noindent 

**Théorème.** On a l'inclusion

$$L^2(\Omega)\subset L^1(\Omega)$$
\noindent autrement dit dès qu'une variable aléatoire admet un moment d'ordre $1$ (une espérance), elle admet aussi un moment d'ordre $2$ et donc une variance.
::::

\noindent 

**Démonstration.** Soit $X\in L^2(\Omega)$, i.e. $\mathbb{E}(X^2)<+\infty$. On a 

$$(|X|-1)^2\geq 0$$
\noindent d'où 

$$|X|\leq\frac{1+X^2}{2}$$
\noindent On déduit facilement de cette dernière inégalité que $\mathbb{E}(|X|)<+\infty$, i.e. que $X\in L^1(\Omega)$.

$\square$

\noindent 

**Remarque.** On peut aussi définir une notion d'espace $L^p(E, \mathcal{A}, \mu)$ pour $(E, \mathcal{A}, \mu)$ un espace mesuré (hors-programme), mais dans ce cas l'inclusion précédente n'est plus vraie en toute généralité, autrement dit l'inclusion

$$L^2(E, \mathcal{A}, \mu)\subset L^1(E, \mathcal{A}, \mu)$$
\noindent n'est pas automatique pour des espaces mesurés qui ne sont pas des espaces probabilisés (en fait, elle devient fausse si la mesure n'est pas finie, i.e. si $\mu(E)=+\infty$).


\noindent On peut démontrer que $L^1(\Omega)$ et $L^2(\Omega)$ sont des espaces-vectoriels :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème.** $L^1(\Omega)$ et $L^2(\Omega)$ sont des $\mathbb{R}$-espaces-vectoriels.
::::

\noindent

**Démonstration.** Il suffit de démontrer que $L^1(\Omega)$ et $L^2(\Omega)$ sont des sous-espaces-vectoriels du $\mathbb{R}$-espace-vectoriel $\mathbb{R}^{\Omega}$ des applications de $\Omega$ dans $\mathbb{R}$.

\noindent Tout d'abord, l'application $0$ qui à tout $\omega$ dans $\Omega$ associe le réel $0$ admet clairement une espérance et un moment d'ordre $2$ (tous les deux nuls), donc elle appartient à $L^1(\Omega)$ et à $L^2(\Omega)$.

\noindent Soient $X,Y\in L^1(\Omega)$ et $\lambda\in\mathbb{R}$. Alors :

\begin{align}
\mathbb{E}\left(|X+\lambda Y|\right) &\leq \mathbb{E}\left(|X|+|Y|\right) \\
&=\mathbb{E}(|X|)+\mathbb{E}(|Y|) \\
&<+\infty
\end{align}

\noindent donc $X+\lambda Y\in L^1(\Omega)$ et $L^1(\Omega)$ est un sous-espace vectoriel de $\mathbb{R}^{\Omega}$.

\noindent Soient $X,Y\in L^2(\Omega)$ et $\lambda\in\mathbb{R}$. Alors :

\begin{align}
\mathbb{E}\left((X+\lambda Y)^2\right) &= \mathbb{E}\left(X^2+2\lambda XY+\lambda^2 Y^2 \right) \\
&=\mathbb{E}(X^2)+2\lambda\mathbb{E}(XY)+\lambda^2\mathbb{E}(Y^2) \\
&\leq\mathbb{E}(X^2)+2\lambda\sqrt{\mathbb{E}(X^2).\mathbb{E}(Y^2)}+\lambda^2\mathbb{E}(Y^2) \\
& \text{(d'après l'inégalité de Cauchy-Schwarz)} \\
&<+\infty
\end{align}

\noindent donc $X+\lambda Y\in L^2(\Omega)$, ce qui permet de conclure que $L^2(\Omega)$ est un sous-espace vectoriel de $\mathbb{R}^{\Omega}$.

:::: {.defbox .def data-latex="important"}
<center>**Convergence en moyenne d'ordre $1$, en moyenne d'ordre $2$**</center>

Soient $X$ une variable aléatoire et $(X_n)_{n\in\mathbb{N}}$ une suite de variables aléatoires.

**Convergence dans $L^1$.** On suppose que $X$ et les $X_n$ admettent des espérances : $\mathbb{E}(|X_n|)<+\infty$ pour tout entier naturel $n$, et $\mathbb{E}(|X|)<+\infty$. On dit alors que $(X_n)_n$ **converge vers $X$ dans $L^1$**, ou que **$(X_n)$ converge en moyenne d'ordre $1$ vers $X$**, lorsque :

$$\lim\limits_{n\to +\infty}\mathbb{E}\left(|X_n-X|\right)=0$$

\noindent 
**Notation :** $$X_n \underset{n \to +\infty}{\overset{L^1}{\longrightarrow}}X$$

\noindent

**Convergence dans $L^2$.** On suppose que $X$ et les $X_n$ admettent des moments d'ordre $2$ : $\mathbb{E}(X_n^2)<+\infty$ pour tout entier naturel $n$, et $\mathbb{E}(X^2)<+\infty$. On dit alors que **$(X_n)_n$ converge vers $X$ dans $L^2$**, ou que **$(X_n)$ en moyenne d'ordre $2$ vers $X$**, lorsque :

$$\lim\limits_{n\to +\infty}\mathbb{E}\left((X_n-X)^2\right)=0$$

\noindent 
**Notation :** $$X_n \underset{n \to +\infty}{\overset{L^2}{\longrightarrow}}X$$
::::



**Exemples. i.** Soit $X_n$ une variable aléatoire telle que

$$\mathbb{P}(X_n=0)=1-\frac{1}{n^2}$$
$$\mathbb{P}(X_n=n)=\frac{1}{n^2}$$
\noindent Alors, $(X_n)_n$ converge vers $0$ dans $L^1(\Omega)$. En effet :

\begin{align*}
\mathbb{E}(|X_n|)&=\mathbb{E}(X_n) \\
&=0.\mathbb{P}(X_n=0)+n.\mathbb{P}(X_n=n) \\
&=n\mathbb{P}(X_n=n) \\
&=n.\frac{1}{n^2} \\
&=\frac{1}{n}
\end{align*}

\noindent et donc 

$$\lim\limits_{n\to +\infty}\mathbb{E}(|X_n|)=0$$

**ii.** Soient $(p_n)_n$ une suite de nombres réels de $[0\,;\,1]$ et $(X_n)_n$ une suite de variables aléatoires telles que

$$\mathbb{P}(X_n=0)=1-p_n$$
$$\mathbb{P}(X_n=\sqrt{n})=p_n$$

\noindent Déterminons une condition nécessaire et suffisante pour que $(X_n)_n$ converge vers $0$ dans $L^2(\Omega)$, autrement dit pour que $\mathbb{E}(X_n^2)\underset{n \to +\infty}{\overset{}{\longrightarrow}}0$.

\noindent Tout d'abord, $X_n$ a un univers fini, donc elle est dans $L^2(\Omega)$. Ensuite :

\begin{align*}
\mathbb{E}(X_n^2)&=(\sqrt{n})^2.p_n \\
&=np_n
\end{align*}

\noindent Donc, $(X_n)_n$ converge vers $0$ dans $L^2(\Omega)$ si et seulement si $\lim\limits_{n\to +\infty} np_n=0$, autrement dit si $p_n=o\left(\frac{1}{n}\right)$ lorsque $n$ tend vers l'infini.

$\square$



\noindent Le théorème suivant donne un lien entre convergence en moyenne quadratique (dans $L^2$) ey convergence en moyenne d'ordre $1$ :

:::: {.thmbox .thm data-latex="important"}
\noindent 
**Théorème (CV dans $L^2$ implique CV dans $L^1$).** Soit $(X_n)_n$ une suite de variables aléatoires réelles et $X$ une variable aléatoire réelle, toutes dans $L^2(\Omega)$.

\noindent Si

$$X_n \underset{n \to +\infty}{\overset{L^2}{\longrightarrow}}X$$
\noindent alors

$$X_n \underset{n \to +\infty}{\overset{L^1}{\longrightarrow}}X$$
:::: 


\noindent 

**Démonstration.** La fonction $x\in\mathbb{R}\mapsto x^2$ est convexe, donc d'après l'inégalité de Jensen, on a 

$$\left(\mathbb{E}|X_n-X|\right)^2\leq\mathbb{E}((X_n-X)^2)$$
\noindent donc 

$$0\leq \mathbb{E}(|X_n-X|)\leq\sqrt{\mathbb{E}\left((X_n-X)^2\right)}$$
\noindent Si $X_n \underset{n \to +\infty}{\overset{L^2}{\longrightarrow}}X$ alors le membre de droite de l'encadrement ci-dessus tend vers $0$ lorsque $n$ tend vers l'infini. D'après le théorème d'encadrement, on a donc $\lim\limits_{n\to +\infty}\mathbb{E}\left(|X_n-X|\right)=0$, et donc $X_n \underset{n \to +\infty}{\overset{L^1}{\longrightarrow}}X$.

$\square$




**Remarque.** Il existe plus généralement une notion de convergence dans $L^p$, ou convergence en moyenne d'ordre $p$, pour $p>0$, mais seuls les cas $p=1$ et $p=2$ sont au programme.


:::: {.defbox .def data-latex="important"}
<center>**Convergence en moyenne d'ordre $p$ (hors-programme pour $p\not\in\{1,2\}$)**</center>

Soit $p$ un réel strictement positif. Soient $X$ une variable aléatoire et $(X_n)_{n\in\mathbb{N}}$ une suite de variables aléatoires.

On suppose que $X$ et les $X_n$ admettent des moments d'ordre $p$ : $\mathbb{E}(|X_n|^p)<+\infty$ pour tout entier naturel $n$, et $\mathbb{E}(|X|^p)<+\infty$. On dit alors que **$(X_n)_n$ converge vers $X$ dans $L^p$**, ou que **$(X_n)$ en moyenne d'ordre $p$ vers $X$**, lorsque :

$$\lim\limits_{n\to +\infty}\mathbb{E}\left(|X_n-X|^p\right)=0$$

\noindent 

**Notation :** $$X_n \underset{n \to +\infty}{\overset{L^p}{\longrightarrow}}X$$
::::





### Convergence en loi

:::: {.defbox .def data-latex="important"}
<center>**Convergence en loi**</center>

Soient $X$ une variable aléatoire de fonction de répartition $F$, et $(X_n)_{n\in\mathbb{N}}$ une suite de variables aléatoires de fonctions de répartition $F_n$. 

\noindent On dit que $(X_n)_n$ **converge en loi vers $X$** lorsque, en tout point $x$ en lequel $F$ est continue, on a 

$$\lim\limits_{n\to +\infty}F_n(x)=F(x)$$

\noindent 
**Notation :** $$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}X$$
::::


\noindent

**Exemples. i.** Pour tout entier naturel $n$, on note $X_n$ une variable aléatoire positive de densité $f_n$ définie par $f_n(x)=ne^{-nx}$ pour $x>0$. Démontrons que $(X_n)_n$ converge en loi vers $0$. 

\noindent On note $F_n$ la fonction de répartition de $X_n$. On obtient alors

$$F_n(x)=(1-e^{-nx}).1_{\mathbb{R}_+^{*}}(x)$$
\noindent La suite de fonctions $(F_n)_n$ converge simplement vers 

$$F(x)=1_{\mathbb{R}_+^{*}}(x)$$
\noindent qui est la fonction de répartition de la variable aléatoire certaine égale à $0$. Ainsi

$$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}0$$
\noindent

**ii.** Pour $n$ entier naturel, on considère la variable aléatoire certaine $X_n=n$. Etudions la convergence en loi de la suite $(X_n)_n$. 

\noindent On note $F_n$ la fonction de répartition de $X_n$. On a 

$$F_n(x)=1_{[n\,;\,+\infty[}(x)$$
\noindent et donc la suite $(F_n)_n$ converge simplement vers la fonction constante égale à $0$. Mais cette fonction n'est pas une fonction de répartition, car $\lim\limits_{x\to +\infty} F(x)\neq 1$.

\noindent Donc, la suite $(X_n)_n$ ne converge pas en loi.

\noindent 

**Remarque.** La convergence simple de la suite des fonctions de répartition ne suffit pas pour établir une convergence en loi. Il faut, en plus, s'assurer que la limite est bien une fonction de répartition.

\noindent Il existe différents critères de convergence en loi. Certaine s'appliquent uniquement à des variables discrètes avec limite discrète, d'autres à des variables absolument continues avec limite absolument continue, et d'autres enfin s'appliquent à toutes les variables aléatoires.


:::: {.thmbox .def data-latex="important"}
\noindent
**Théorème (Quelques critères de convergence en loi.)** 

\noindent

**i. Espérance contre des fonctions tests.** La suite $(X_n)_n$ converge en loi vers $X$ si et seulement si, pour toute fonction **continue** et **bornée** $f$, on a 

$$\mathbb{E}(f(X_n))\underset{n \to +\infty}{\overset{}{\longrightarrow}}\mathbb{E}(f(X))$$

\noindent

**ii. Espérance contre des fonctions tests (variante).** La suite $(X_n)_n$ converge en loi vers $X$ si et seulement si, pour toute fonction **uniformémént continue** et **bornée** $f$, on a 

$$\mathbb{E}(f(X_n))\underset{n \to +\infty}{\overset{}{\longrightarrow}}\mathbb{E}(f(X))$$

\noindent

**iii. Par les fonctions caractéristiques.** Soit $d\in\mathbb{N}^*$. On suppose $\mathbb{R}^d$ muni d'un produit scalaire $<.,.>$. Pour toute variable aléatoire $X$ à valeurs dans $\mathbb{R}^d$, on note $\varphi_X$ la **fonction caractéristique de $X$** :

$$\forall t\in\mathbb{R},\,\varphi_X(t)=\mathbb{E}(e^{i<t,X>})$$
\noindent Cette fonction caractérise la loi de $X$ (i.e. $X$ et $Y$ ont la même loi si et seulement si $\varphi_X=\varphi_Y$), et elle caractérise la convergence en loi :

$$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}X\text{ si et seulement si } \varphi_{X_n}\underset{n \to +\infty}{\overset{}{\longrightarrow}}\varphi_X$$
\noindent
**iv. Convergence en loi des variables aléatoires discrètes.** si $(X_n)_n$ et $X$ sont des variables aléatoires réelles discrètes, à valeurs dans un même ensemble dénombrable \textbf{discret} 

$$E=\{x_k,\, k\in K\}\hspace{1cm}\text{ avec } K\subset\mathbb{Z}$$

\vspace{0.3cm} 

\noindent alors 

$$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}X$$

\vspace{0.3cm}

\noindent si, et seulement si

$$\forall k\in K,\,\mathbb{P}(X_n=x_k) \underset{n \to +\infty}{\overset{}{\longrightarrow}}\mathbb{P}(X=x_k)$$

\noindent
**v. Convergence en loi des variables aléatoires à densité.**  Soient $(X_n)_n$ et $X$ des variables aléatoires à densité. On note $f_n$ et $f$ ces densités, définies sur $\mathbb{R}$. 

\vspace{0.3cm}

\noindent Si

$$\forall x\in\mathbb{R},\, f_n(x) \underset{n \to +\infty}{\overset{}{\longrightarrow}}f(x)$$

\noindent alors, $(X_n)_n$ converge en loi vers $X$.

::::

\noindent

**Démonstration.** L'assertion i. est démontrée en exercice (feuille de la session 3) pour la condition suffisante, la condition nécessaire est admise. 

\noindent Les assertions ii. sont admises.

\noindent Les assertions iv. et v. sont démontrées en exercices.

$\square$

\noindent

**Exemples.** Voir les exercices 3, 4, 5 et 13 de la feuille de la session 3.



### Convergence presque-sûre (hors-programme ?)

:::: {.defbox .def data-latex="important"}
<center>**Convergence presque sûre**</center>

Soient $X$ une variable aléatoire et $(X_n)_{n\in\mathbb{N}}$ une suite de variables aléatoires.

\noindent On dit que $(X_n)_n$ **converge presque sûrement vers $X$** lorsque :

$$\mathbb{P}\left(\{\omega\in\Omega, X_n(\omega)\longrightarrow X(\omega)\right\})=1$$

\noindent 
**Notation :** $$X_n \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}}X$$
::::

### Liens entre les différents modes de convergence

:::: {.thmbox .thm data-latex="important"}
**Liens entre les différents modes de convergence.** Les liens entre les différents modes de convergence sont donnés par le schéma ci-dessous :

<center>
```{r, echo = FALSE}
knitr::include_graphics("C:/Users/olivier.guin/Travail/Formation_Administrateur/Cours/Cours_probabilités_statistique/images/modes_convergence.PNG") 
```
</center>

::::

\noindent

**Remarque.** D'après le schéma ci-dessus, la convergence en loi est donc le mode de convergence le plus faible. Dit autrement, pour qu'une suite de variables aléatoires converge sous un mode ou un autre, elle doit **a minima** converger en loi.


\noindent Si la convergence en probabilité implique la convergence en loi, la réciproque est fausse en général.

\noindent
**Contre-exemple.** Voir la question 2 de l'exercice 2.

\noindent En revanche, il existe tout de même une réciproque partielle :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème.** Soient $c$ un réel et $(X_n)_n$ une suite de variables aléatoires. Si 

$$X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}c$$

\noindent alors 

$$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}c$$
::::

\noindent
**Démonstration.** Voir la question 1 de l'exercice 2.

$\square$

### Transformation par une application continue

\noindent Souvent, on est amené à étudier la convergence (sous un mode ou un autre) d'une variable aléatoire $Y_n$ s'exprimant comme une fonction d'une autre variable $X_n$ :

$$Y_n=f(X_n)$$

\noindent Si la fonction $f$ est continue, les choses se passent bien en général :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème. [Continuous mapping theorem].** Soient $(X_n)_n$ une suite de variables aléatoires réelles (ou à valeurs dans $\mathbb{R}^d$), $X$  une variable aléatoire réelle (ou à valeurs dans $\mathbb{R}^d$), et $g$ une fonction continue sur $\mathbb{R}$ (ou $\mathbb{R}^d$) et à valeurs dans $\mathbb{R}$.

\vspace{0.3cm}

\noindent 
**i.** Si $X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}X$ alors $g(X_n) \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}g(X)$

\noindent
**ii.** Si $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$ alors $g(X_n) \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}g(X)$.

\noindent 
**iii.** Si $X_n \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}}X$ alors $X_n \underset{n \to +\infty}{\overset{p.s.}{\longrightarrow}}g(X)$

\noindent On résume les points i. ii. et iii. en disant que la convergence en loi, la convergence en probabilité et la convergence presque sûre sont **stables par transformation continue**.
:::

\noindent

**Démonstration.** Voir exercice 6.

$\square$

**Exemple.** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. de loi $\mathcal{E}(\lambda)$. note 

$$\overline{X_n}=\frac{1}{n}\sum\limits_{i=1}^n X_i$$

\noindent leur moyenne empirique, et 

$$Y_n=\cos\left(\frac{1}{1+\overline{X_n}^2}\right)$$
\noindent Démontrons les convergences en probabilité et en loi de la suite $(Y_n)_n$. 

\noindent Le faire directement serait compliqué. Le mieux est de se ramener à une variable dont la convergence (en probabilité, en loi) est facile à établir. Nous avons déjà montré dans un exemple précédent que

$$\overline{X_n} \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\frac{1}{\lambda}$$
\noindent Comme la convergence en probabilité implique la convergence en loi, on en déduit que

$$\overline{X_n} \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}\frac{1}{\lambda}$$
\noindent Par ailleurs, la fonction $f$ définie sur $\mathbb{R}$ par

$$f(x)=\cos\left(\frac{1}{1+x^2}\right)$$
\noindent est continue. Donc, d'après le continuous mapping theorem, on a

$$Y_n\underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\frac{1}{1+\frac{1}{\lambda^2}}=\frac{\lambda^2}{1+\lambda^2}$$
\noindent et

$$Y_n\underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}\frac{1}{1+\frac{1}{\lambda^2}}=\frac{\lambda^2}{1+\lambda^2}$$

$\square$


### Théorème de Slutsky (hors-programme ?)

\noindent Ce résultat est très pratique dans les exercices pour démontrer une convergence en loi. Son appartenance au programme du concours n'est cependant pas certaine (je penche plutôt pour non), donc sauf s'il est explicitement écrit dans le sujet que le résultat est admis (comme cela a déjà le cas) il semble risqué de l'utiliser. Dans tous les cas, comme l'énoncé est simple, ça ne coût pas grand chose de l'avoir en tête.

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème de Slutsky.** Si $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$ et $Y_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}c$, où $c$ est une constante, alors $(X_n, Y_n) \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}(X,c)$.
::::


\noindent

**Démonstration.** Admise.

\noindent

**Remarque.** Comme la convergence en loi **vers une constante** implique la convergence en probabilité vers cette même constante (rappel : ce n'est pas nécessairement vrai si la limite est une variable aléatoire non constante !), on pourrait de manière totalement équivalente énoncer le théorème de Slutsky en disant que si X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$ et $Y_n \underset{n \to +\infty}{\overset{\mathcal{P}}{\longrightarrow}}c$, où $c$ est une constante, alors $(X_n, Y_n) \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}(X,c)$.

\noindent Ce théorème présente tout son intérêt lorsqu'il est couplé avec le continuous mapping theorem, car il permet alors d'en déduire le corollaire suivant :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Corollaire.**  Si $X_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X$ et $Y_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}c$, où $c$ est une constante, alors 

- $X_n+Y_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}X+c$ ;

- $X_n.Y_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}cX$ ;

- et sous réserve d'existence de tous les termes, $\frac{X_n}{Y_n} \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\frac{X}{c}$.
::::

\noindent
**Démonstration.** Avec le théorème de Slutsky, on a

$$(X_n, Y_n) \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}(X,c)$$

\noindent Par ailleurs, les fonctions $(x,y)\mapsto x+y$, $(x,y)\mapsto x.y$ et $(x,y)\mapsto\frac{x}{y}$  sont définies et continues sur $\mathbb{R^2}$ pour les deux premières et sur $\mathbb{R}\times\mathbb{R}^*$ pour la troisième. On en déduit alors le résultat en appliquant le continuous mapping theorem.

$\square$



### Approximations

:::: {.thmbox .def data-latex="important"}
\noindent 
**Théorème. i.** Soit $(X_n)_n$ une suite de variables aléatoires telle que, pour tout entier naturel $n$, on ait $X_n\sim\mathcal{B}(n, p_n)$.

\noindent On suppose que $p_n\underset{n \to +\infty}{\overset{}{\longrightarrow}}0$ et $np_n\underset{n \to +\infty}{\overset{}{\longrightarrow}}\lambda$, où $\lambda$ est un réel. Alors

$$X_n \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}\mathcal{P}(\lambda)$$

\noindent

**ii.** Soit $(X_n)_n$ une suite i.i.d. de variables aléatoires suivant une loi de Bernoulli de paramètre $p\in ]0\,;\,1[$ : 

$$\forall i\in\mathbb{N},\, X_i\sim\mathcal{B}(p)$$

\noindent Pour tout entier naturel $n$, on pose $S_n=\sum\limits_{i=1}^n X_i$. Alors

$$\frac{S_n-np}{\sqrt{npq}} \underset{n \to +\infty}{\overset{\mathcal{L}}{\longrightarrow}}\mathcal{N}(0\,;\,1)$$
::::


\noindent\textbf{Remarques. i.} \textit{Le premier résultat est à l'origine de la dénomination de la loi de Poisson comme la \textbf{loi des événements rares}. En effet, ce résultat nous dit simplement que la loi de Poisson de paramètre $\lambda\approx n p_n$ est presque une loi binomiale de paramètres $(n, p_n)$, donc une loi qui compte le nombre d'occurrences d'un événement donné de probabilité $p_n\approx 0$ très petite. En pratique, cette approximation est considérée comme bonne pour $n\geq 30$ et $np<5$.}

\vspace{0.3cm}

\noindent\textbf{ii.} Le deuxième résultat nous dit que la loi binomiale $\mathcal{B}(n\,;\,p)$ peut être approchée par la loi normale $\mathcal{N}(np\,;\,npq)$. On considère généralement que cette approximation est bonne pour $n\geq 30, np\geq 5$ et $nq\geq 5$. 


\noindent L'exemple suivant montre un intérêt pratique de ces appoximations.

\noindent
**Exemple (contrôle de fabrication).** On souhaite effectuer un contrôle de fabrication sur un lot de $1\,000$ pièces. On sait que la probabilité qu'une pièce soit défectueuse est $p=0,02$. On suppose que les pièces sont défectueuses ou non indépendamment des autres pièces.

\noindent On souhaite évaluer la probabilité que le lot contienne entre $18$ et $22$ pièces défectueuses.

\noindent On note $X$ le nombre de pièces défectueuses. Alors, $X$ suit une loi binomiale de paramètres $n=1\,000$ et $p=0,02$ :

$$X\sim\mathcal{B}(1\,000\,;\,0,02)$$

\noindent Le calcul de $\mathbb{P}(X\in[18\,;\,22])$ est trop compliqué pour être envisagé de manière exacte, on va donc fournir une valeur approchée.

\noindent Nous allons envisager à la fois une approximation par une loi de Poisson et par une loi normale. 

\noindent
**Approximation 1 : par une loi de Poisson.** On a :

- $n\=1\,000\geq 30$ 
- $np=20$, donc $np$ n'est pas inférieur à 5

\noindent Même si dans le cas présent la deuxième condition n'est pas vérifiée, nous calculons tout de même l'approximation par la loi de Poisson $\mathcal{P}(20)$.

\noindent Avec une table de la loi de Poisson, on obtient, en notant $Y$ une variable aléatoire telle que $Y\sim\mathcal{P}(20)$ :

\begin{align}
\mathbb{P}(18\leq X \leq 22) &\approx\mathbb{P}(18\leq Y\leq 22) \\
&\approx 0,42 \\
\end{align}


\noindent 
**Approximation 2 : par une loi normale.** On a :

- $n=1\,000\geq 30$
- $np=20\geq 5$
- $nq=980\geq 5$

\noindent On peut donc approcher la loi de $X$ par la loi normale de paramètres $np=20$ et $npq=19,6$ :

$$X\approx\mathcal{N}(20\,;\,19,6)$$
\noindent Pour calculer $\mathbb{P}(X\in [18\,;\,22])$, on doit se ramener à une loi normale standard $\mathcal{N}(0\,;\,1)$ pour laquelle on dispose de tables. Pour cela, on centre et on réduit $X$ :

$$Y=\frac{X-20}{\sqrt{19,6}}$$
\noindent On a donc

\begin{align*}
\mathbb{P}\left(18\leq X\leq 22\right)&=\mathbb{P}\left(\frac{18-20}{\sqrt{19,6}}\leq Y\leq\frac{22-20}{\sqrt{19,6}}\right) \\
&\approx\mathbb{P}(-0,45\leq X\leq 0,45) \\
&\approx 0,35
\end{align*}

\noindent

**Calcul avec la vraie loi binomiale.** On obtient $\mathbb{P}(18\leq X\leq 22)\approx 0,43$. Dans cet exemple, l'approximation par la loi de Poisson est donc meilleure que celle par la loi normale.




## Lois des grands nombres

\noindent Le programme du concours mentionne deux lois des grands nombres, qui permettent de montrer la convergence de la moyenne empirique vers la moyenne théorique :

- la loi faible des grands nombres, où la convergence est une convergence en probabilités ;

- la loi (forte) des grands nombres dans $L^2$, où la convergence est une convergence presque sûre, donc en un sens plus fort que celle de la loi faible.


:::: {.thmbox .thm data-latex="important"}
**Loi faible des grands nombres (théorème de Khintchine).** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. et admettant une espérance $\mu=\mathbb{E}(X_1)$, i.e. $X_1\in L^1(\Omega, \mathcal{A}, \mathbb{P})$. Alors, la moyenne empirique 

$$\overline{X_n}=\frac{X_1+\dots+X_n}{n}$$
\noindent converge en probabilité vers $\mu=\mathbb{E}(X_1)$. Autrement dit :

$$\forall\varepsilon >0, \mathbb{P}\left(\left|\overline{X_n}-\mu\right|\geq\varepsilon\right)\longrightarrow 0$$
::::

\noindent La loi des grands nombres dans $L^2$ permet d'obtenir une convergence en un sens plus fort (presque sûre plutôt qu'en probabilités), au prix d'une hypothèse plus forte (variables dans $L^2$ plutôt que dans $L^1$) :


:::: {.thmbox .thm data-latex="important"}
**Loi des grands nombres dans $L^2$.** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. telle que $\mathbb{E}(X_1^2)<+\infty$, i.e. $X_1\in L^2(\Omega, \mathcal{A}, \mathbb{P})$. Alors, la moyenne empirique 

$$\overline{X_n}=\frac{X_1+\dots+X_n}{n}$$
\noindent converge presque sûrement vers $\mathbb{E}(X_1)$. 
::::

\noindent On a en fait même un peu mieux. En effet, l'hypothèse d'appartenance à $L^2$ n'est pas nécessaire pour obtenir une convergence presque sûre : une appartenance à $L^1$ est suffisante. Ce resultat est connu usuellement sous le nom de *loi forte des grands nombres*.

:::: {.thmbox .thm data-latex="important"}
**Loi forte des grands nombres (hors-programme ?).** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. telle que $\mathbb{E}(|X_1|)<+\infty$, i.e. $X_1\in L^1(\Omega, \mathcal{A}, \mathbb{P})$. Alors, la moyenne empirique 

$$\overline{X_n}=\frac{X_1+\dots+X_n}{n}$$
\noindent converge presque sûrement vers $\mathbb{E}(X_1)$. 
::::

\noindent Ce résultat n'étant pas mentionné explicitemnt dans la notice du concours, il est donc *a priori* hors-programme. 

\noindent 

**Conséquence :** sauf indication contraire de l'énoncé, pour démontrer une convergence presque sûre de la moyenne empirique vers la moyenne théorique, il faut au préalable s'assurer que les $X_i$ sont dans $L^2$ ($L^1$ ne suffit pas dans ce cadre).



-   énoncé théorique
-   exemples avec illustration en R

## Théorème Central Limite (TCL)

:::: {.thmbox .thm data-latex="important"}
**Théorème central limite.** Soit $(X_n)_n$ une suite de variables aléatoires i.i.d. admettant un moment d'ordre 2. On pose

$$\mu=\mathbb{E(X_1)}$$
$$\sigma^2=\mathbb{V}(X_1)$$
\noindent On pose 

$$S_n=X_1+\dots+X_n$$

\noindent Alors, la variable aléatoire 

$$Z_n=\frac{S_n-n\mu}{\sigma\sqrt{n}}$$
\noindent converge en loi vers la loi normale standard $\mathcal{N}(0,1)$.

::::

-   énoncé théorique
-   exemples avec illustration en R

## Variantes du TCL (hors-programme)
