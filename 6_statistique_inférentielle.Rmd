# Statistique inférentielle

## Estimation

\noindent On s'intéresse à une loi probabiliste $\mathcal{L}_{\theta}$, qui est entièrement décrite par la donnée d'un paramètre inconnu $\theta$. Pour mieux appréhender cette loi, il serait intéressant de connaître la valeur de $\theta$. Plutôt que de chercher à déterminer la valeur exacte de $\theta$, on peut essayer de l'approcher. Dans le cadre de la statistique inférentielle, on suppose qu'on dispose d'un **échantillon i.i.d.** de $\mathcal{L}_{\theta}$, autrement dit d'un certain nombre de réalisations $(Y_1,\dots, Y_n)$ indépendantes et identiquement distribuées de la loi $\mathcal{L}_{\theta}$.

\noindent La donnée d'un tel échantillon constitue un ensemble d'**informations** qui vont nous être utiles pour **estimer** le paramètre $\theta$. on fait donc bien ici de l'*inférence* - ou encore de l'*induction* - dans le sens où on part d'observations particulières (les réalisations $Y_1,\dots, Y_n)$ pour énoncer une règle générale (le fait que ces réalisations sont issues de la loi $\mathcal{L}_{\theta}$).

### Premières définitions


:::: {.defbox .def data-latex="important"}
<center>**Estimateurs**</center>

\vpsace{0.5cm}

\noindent Soit $Y$ une variable aléatoire de loi $\mathcal{L}(Y)$, paramétrée par un réel $\theta$ inconnu. Soit $(Y_1,\dots, Y_n)$ un échantillon i.i.d. de loi $\mathcal{L}(Y)$. On appelle *estimateur* de $\theta$ toute fonction de $Y_1,\dots, Y_n$, i.e. $$\widehat{\theta}_n=S(Y_1,\dots, Y_n)$$
::::


\noindent On veut estimer la moyenne d'une loi normale $\mathcal{N}(\mu\,;\,1)$, à partir d'un échantillon d'observations i.i.d. $(Y_1,\dots, Y_n)$ tirées sous cette loi. Une façon naturelle d'estimer $\mu=\mathbb{E}(Y_1)$ est de poser $\widehat{\mu}_n=\frac{Y_1+\dots Y_n}{n}$. Ici, on estime donc une moyenne théorique par sa contrepartie *empirique*.

:::: {.defbox .def data-latex="important"}
<center>**Biais, erreur quadratique**</center>
\vspace{0.5cm}

\noindent Soit $\widehat{\theta}_n$ un estimateur de $\theta$ admettant un moment d'ordre $1$.

-   On appelle **biais** de $\widehat{\theta}_n$ la quantité $b_{\theta}(\widehat{\theta}_n)=\mathbb{E}(\widehat{\theta}_n)-\theta$.

-   Un estimateur est dit **sans biais** lorsque son biais est nul, i.e. $\mathbb{E}(\widehat{\theta}_n)=\theta$.

-   Il est dit **asymptotiquement sans biais** lorsque son biais tend vers $0$, i.e. $b_{\theta}(\widehat{\theta}_n)\underset{n\to +\infty}{\longrightarrow}0$.

-   Pour un estimateur des moments d'ordre $1$ et $2$, on appelle **erreur quadratique moyenne** la quantité (positive) $\text{EQM}_{\theta}(\widehat{\theta}_n)=\mathbb{E}\left(\left(\widehat{\theta}_n-\theta\right)^2\right)$
::::

\noindent L'erreur quadratique moyenne s'écrit à l'aide de l'espérance et de la variance :

:::: {.thmbox .thm data-latex="important"}
**Théorème :** Soit $\widehat{\theta}_n$ un estimateur de $\theta$ admettant des moments d'ordres $1$ et $2$. Son erreur quadratique moyenne peut se décomposer en biais au carré/variance :

$$\text{EQM}_{\theta}(\widehat{\theta}_n)=b_{\theta}^2(\widehat{\theta}_n)+\mathbb{V}(\widehat{\theta}_n)$$
::::

\noindent Autrement dit, réduire l'erreur (quadratique moyenne) d'un estimateur revient à essayer de réduire son biais et/ou sa variance. En pratique, uune réduction du biais implique souvent une augmentation de la variance (et vice-versa) et il faut trouver un compromis entre les deux, i.e. un estimateur pour lequel la combinaison (biais, variance) implique une faible erreur quadratique moyenne. On parle alors de **compromis biais-variance**.

### Convergence d'un estimateur

:::: {.defbox .def data-latex="important"}
<center>**Estimateurs convergents**</center>
\noindent Un estimateur $\widehat{\theta}_n$ de $\theta$ est dit **convergent** lorsqu'il converge en probabilité vers $\theta$ i.e. lorsque

$$\forall\varepsilon >0, \mathbb{P}\left(|\widehat{\theta}_n-\theta|>\varepsilon\right)\longrightarrow 0$$
::::

\noindent La convergence d'un estimateur sans biais peut se montrer à l'aide du critère pratique suivant :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème (critère pratique de convergence) :** Un estimateur $\widehat{\theta}_n$ sans biais de $\theta$ est **convergent** dès que sa variance tend vers $0$, i.e. $$\left(\mathbb{E}_{\theta}(\widehat{\theta}_n)=0 \text{ et } \mathbb{V}_{\theta}(\widehat{\theta}_n)\longrightarrow 0\right)\Rightarrow \left(\widehat{\theta}_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\theta\right)$$
::::

\noindent 
**Démonstration.** Compte-tenu du fait que $\widehat{\theta}_n$ est un estimateur sans biais pour $\theta$, l'inégalité de Bieanymé-Tchebychev s'écrit $\mathbb{P}(|\widehat{\theta}_n-\theta|>\varepsilon)\leq\frac{\mathbb{V}_{\theta}(\widehat{\theta}_n)}{\varepsilon^2}$, ce qui permet de conclure. $\square$

\noindent On peut même affaiblir un peu l'hypothèse d'absence de biais par une hypothèse de biais asymptotiquement nul :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème (critère pratique de convergence (suite)) :** Un estimateur $\widehat{\theta}_n$ asymtotiquement sans biais de $\theta$ est **convergent** dès que sa variance tend vers $0$, i.e. $$\left(\mathbb{E}_{\theta}(\widehat{\theta}_n)\underset{n\to +\infty}{\longrightarrow}\theta \text{ et } \mathbb{V}_{\theta}(\widehat{\theta}_n)\longrightarrow 0\right)\Rightarrow \left(\widehat{\theta}_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\theta\right)$$
::::

### Exemples classiques

\noindent Quelques exemples très classiques d'estimateurs :

\noindent 
**Exemple 1 : moyenne empirique.** Soit $X_1,\dots X_n$ une suite de $VAR$ i.i.d. de même loi que $X$, admettant une espérance $\mu$. La *moyenne empirique* est l'estimateur $$\overline{X_n}=\frac{X_1+\dots + X_n}{n}$$

:::: {.thmbox .thm data-latex="important"}
**Théorème :** Quelle que soit la loi suivie par $X$, la moyenne empirique $\overline{X_n}$ est un **estimateur sans biais** de l'espérance $\mu=\mathbb{E}(X)$. Si, de plus, $X$ admet une variance $\sigma^2$, alors $\overline{X_n}$ admet également une variance et celle-ci est donnée par $\mathbb{V}(\overline{X_n})=\frac{\sigma^2}{n}$.
::::

\noindent
**Démonstration.** Par linéarité de l'espérance : $\mathbb{E}(\overline{X_n})=\frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i)=\frac{1}{n}\sum\limits_{i=1}^n\mu=\mu$. Si $X$ admet une variance, alors $\overline{X_n}$ aussi et $\mathbb{V}(\overline{X_n})=\frac{1}{n^2}\sum\limits_{i=1}^n\mathbb{V}(X_i)=\frac{\sigma^2}{n}$, par indépendance de $X_1,\dots X_n$. $\square$

:::: {.thmbox .thm data-latex="important"}
**Corollaire :** La moyenne empirique est un estimateur convergent de l'espérance (lorsqu'elle existe).
::::

\noindent
**Démonstration.** L'estimateur $\overline{X_n}$ est sans biais et $\mathbb{V}(\overline{X_n})=\frac{\sigma^2}{n}\longrightarrow 0$. C'est donc un estimateur convergent de l'espérance. $\square$.

\noindent
**Exemple 2 : estimation d'une proportion.** Au sein d'une population, une proportion $p$ d'individus présente une caractéristique. On suppose que la présence de cette caractéristique est distribuée de façon identique et indépendante d'un individu à l'autre suivant la loi de Bernoulli de paramètre $p$. On peut donc estimer la proportion $p$ au niveau population par la proportion $\widehat{p_n}$ au niveau échantillon : cet estimateur est la moyenne empirique, il est sans biais et de variance (inconnue) $\frac{p(1-p)}{n}$.

\noindent 
**Exemple 3 : variance empirique.** Si $X$ admet une variance $\sigma^2$ et $X_1,\dots, X_n$ sont i.i.d. de même loi que $X$, alors un estimateur de $\sigma^2$ est donné par la variance empirique $S_n^{'2}=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X_n})^2$.

:::: {.thmbox .thm data-latex="important"}
**Théorème :** La variance empirique $S_n^{'2}$ est un estimateur biaisé de la variance $\sigma^2$. Plus précisément, on a

$$\mathbb{E}(S_n^{'2})=\frac{n-1}{n}\sigma^2$$
::::

\noindent 
**Démonstration.**

```{=tex}
\begin{align}
\mathbb{E}(S_n^{'2}) &= \frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i^2)-\frac{2\overline{X_n}}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i)+\frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(\overline{X_n}^2) \\
 &= \mathbb{E}(X^2)-\mathbb{E}(\overline{X_n}^2)
\end{align}
```
Par ailleurs :

```{=tex}
\begin{align}
\mathbb{E}(\overline{X_n}^2)&=\frac{1}{n^2}\sum\limits_{i=1}^n\mathbb{E}(X_i^2)+\frac{1}{n^2}\sum\limits_{i\neq j}\mathbb{E}(X_iX_j) \\
&= \frac{1}{n}^2\sum\limits_{i=1}^n\mathbb{E}(X_i^2)+\frac{1}{n^2}\sum\limits_{i\neq j}\mathbb{E}(X_i)\mathbb{E}(X_j) \\
&= \frac{1}{n}\mathbb{E}(X^2)+\frac{n-1}{n}\left(\mathbb{E}(X)\right)^2
\end{align}
```
\noindent Avec la formule de Huygens $\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2$, on en déduit que

$$\mathbb{E}(S_n^{'2})=\frac{n-1}{n}\sigma^2$$ $\square$

\noindent 
**Remarque.** Le biais de l'estimateur $S_n^{'2}$ devient cependant très faible pour $n$ suffisamment grand. Il s'agit d'un estimateur asymptotiquement sans biais de la variance $\sigma^2$ : $\mathbb{E}(S_n^{'2})\longrightarrow \sigma^2$.

\noindent 
**Exemple 4 : variance empirique corrigée.** En modifiant l'estimateur de la variance empirique par un petit facteur correctif, on obtient un estimateur sans biais de la variance. Il suffit de poser

$$S_n^2=\frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\overline{X_n})^2$$

\noindent Cet estimateur s'appelle la **variance empirique corrigée**.

:::: {.thmbox .thm data-latex="important"}
**Théorème :** La variance empirique corrigée $S_n^2=\frac{1}{n-1}\sum\limits_{i=1}^n (X_i-\overline{X_n})^2$ est un estimateur sans biais de la variance $\sigma^2=\mathbb{V}(X)$ :

$$\mathbb{E}(S_n^2)=\sigma^2$$
::::

### Méthodes de construction des estimateurs

\noindent On présente ici deux méthodes classiques de construction des estimateurs ; la **méthode des moments** et la **méthode du maximum de vraisemblance**.

#### La méthode des moments

:::: {.methbox .meth data-latex="important"}
<center>**La méthode des moments**</center>

\vspace{0.5cm}

Soit $X$ une variable aléatoire réelle de loi $\mathcal{L}_{\theta}$, où $\theta$ est un paramètre inconnu. On considère une fonction $f$ de $I\subset\mathbb{R}$ dans $\mathbb{R}$ telle que $f(X)$ admette une espérance. Comme la loi de $X$ dépend de $\theta$, il en est de même de $\mathbb{E}(f(X))$. La méthode des moments suppose qu'on sait expliciter une telle dépendance, i.e. qu'on connaisse une fonction $g$ telle que

$$\mathbb{E}(f(X))=g(\theta)$$

La contrepartie empirique du membre de gauche de cette égalité est $\frac{1}{n}\sum\limits_{i=1}^n f(X_i)$, et la méthode des moments consiste alors à résoudre l'équation en $\widehat{\theta}$ :

$$g(\widehat{\theta})=\frac{1}{n}\sum\limits_{i=1}^n f(X_i)$$
::::

\noindent 
**Exemple 5 : estimation du paramètre d'une loi exponentielle.** Soit $X\sim\mathcal{E}(\lambda)$, où $\lambda>0$ est un paramètre inconnu que l'on veut estimer. La variable aléatoire $X$ admet une espérance, et celle-ci est donnée par $\mathcal{E}(X)=\frac{1}{\lambda}$. La méthode des moments consiste alors à résoudre l'équation

$$\frac{1}{\widehat{\lambda_n}}=\frac{1}{n}\sum\limits_{i=1}^n X_i$$

Cette équation est très simple, elle admet pour solution

$$\widehat{\lambda_n}=\frac{1}{\overline{X_n}}$$

\noindent C'est l'estimateur que l'on obtient par la méthode des moments.

\noindent 
**Remarque.** En reprenant les notations explicitées ci-dessus, on peut identifier les fonctions $f$ et $g$ :

$$f(x)=x$$ $$g(x)=\frac{1}{x}$$ et ici évidemment $\theta=\lambda$. En général, la méthode des moments s'utilise de façon complètement intuitive sans qu'on ait même à expliciter forcément les fonctions $f$ et $g$.

#### La méthode du maximum de vraisemblance

\noindent Une autre méthode de construction d'estimateurs est celle du **maximum de vraisemblance**. L'idée générale de cette méthode est la suivante. On suppose qu'on dispose de réalisations $x_1,\dots x_n$ d'une même variable aléatoire, dont la loi appartient à une famille paramétrique $\left\{\mathcal{L}_{\theta}\,;\,\theta\in\Theta\right\}$ et on cherche à estimer $\theta$. Si par exemple on dispose d'une série de cinq obersations $(0.12, -0.65, 1.35, 1.04, -1.19, 0.08)$ et qu'on veut inférer sur $\theta$ à partir de ces observations, on est enclin à penser que la valeur $\theta=0$ est plus plausible que la valeur $\theta=-10$. La vraisemblance est une formalisation de l'idée intuitive de plausibilité d'un paramètre à partir d'une observation ou d'un ensemble d'observations.

A nouveau, $X$ désigne une variable aléatoire de loi dépendant d'un paramètre inconnu $\theta$, et $x$ une réalisation de $X$.

\noindent La *vraisemblance* $L(x,.)$ est une fonction de $\theta$ définie par

$$L(x;\theta)=\left\{
\begin{array}{lll}
\mathbb{P}_{\theta}(X=x) &\text{; si } X \text{ est discrète} \\
f(x;\theta) &\text{; si } X \text{ est une continue de densité } f(.;\theta) \\
\end{array}
\right.$$

\noindent 
**Remarque :** D'autres notations existent dans la littérature, comme $L(x|\theta), \mathbb{P}(X=x|\theta), f(x|\theta)$. Ces notations viennent de la statistique bayésienne (hors programme du concours) qui envisage $\theta$ comme une variable aléatoire de distribution inconnue. Dans ce cas, la vraisemblance s'interprète comme une probabilité ou une densité de probabilité.

\noindent La définition précédente s'étend au cas d'un échantillon $(X_1,\dots X_n)$ de VA de même loi que $X$. Dans ce cas, on note souvent $L_n(x;\theta)$ la vraisemblance, pour faire apparaître la dépendance en $n$. Un cas particulier important est celui où ces VA sont i.i.d. Dans ce cas, la vraisemblance est définie par

$$L_n(x;\theta)=L_n(x_1,\dots,x_n ; \theta)=\left\{
\begin{array}{lll}
\prod\limits_{i=1}^n\mathbb{P}_{\theta}(X_i=x_i) &\text{; si } X \text{ est discrète} \\
\prod\limits_{i=1}^n f(x_i,\theta) &\text{; si } X \text{ est une continue de densité } f(.;\theta) \\
\end{array}
\right.$$

\noindent La méthode du maximum de vraisemblance consiste juste à dire que si toute l'information dont on dispose sur la variable aléatoire $X$ est l'observation de l'échantillon $(x_1, \dots, x_n)$, alors la meilleure estimation que l'on puisse faire de $\theta$ à partir de cette information est celle qui maximise la fonction de vraisemblance. Autrement dit, on cherche la valeur de $\theta$ qui rend l'observation $(x_1,\dots, x_n)$ la plus plausible. Formellement :

:::: {.methbox .meth data-latex="important"}
<center>**Méthode du maximum de vraisemblance**</center>

\vspace{0.5cm}

\noindent Etant donné une collection de $n$ réalisations $x=(x_1,\dots, x_n)$ des VA $(X_1,\dots X_n)$ de même loi $\mathcal{L}_{\theta}$ de paramètre inconnu $\theta$, on appelle *estimation du maximum de vraisemblance* toute estimation $\widehat{\theta}_n=\widehat{\theta}_n(x_1,\dots,x_n)$ vérifiant

$$\widehat{\theta}_n\in \arg\max\limits_{\theta\in\Theta}L_n(x;\theta)$$ 
\noindent
**Cas particulier :** si la fonction $\theta\mapsto L_n(x;\theta)$ est deux fois dérivable sur $\Theta$, alors on peut chercher à résoudre (en $\theta$) le système

$$\left\{
\begin{array}{lll}
\frac{\partial}{\partial\theta}L_n(x;\theta)=0 \\
\frac{\partial^2}{\partial\theta^2}L_n(x;\theta)<0 \\
\end{array}
\right.$$ 
\noindent Les solutions de ce système fournissent des estimations par maximum de vraisemblance.
::::

\noindent 

**Log-vraisemblance.** Il est souvent plus commode de considérer la log-vraisemblance $l_n(x;\theta)=\ln L_n(x;\theta)=\sum\limits_{i=1}^n \ln L_n(x_i;\theta)$. La fonction $\ln$ étant croissante sur $\mathbb{R}_{+}^*$, maximiser la vraisemblance équivaut à maximiser la log-vraisemblance.

:::: {.methbox .meth data-latex="important"}
<center>**Maximisation de la log-vraisemblance**</center>

\noindent En supposant que $L_n(x;\theta)>0$ pour tout $\theta\in\Theta$, on note $l_n(x;\theta)=\ln L_n(x;\theta)$ la log-vraisemblance. Sous les mêmes hypothèses que ci-dessus, on cherche

$$\widehat{\theta}_n\in\arg\max\limits_{\theta\in\Theta}\left(l_n(x;\theta)\right)$$ 

\noindent 

**Cas particulier :** si la fonction $\theta\mapsto L_n(x;\theta)$ est deux fois dérivable sur $\Theta$, alors la fonction $\theta\mapsto l_n(x;\theta)$ l'est aussi et on peut chercher à résoudre (en $\theta$) le système

$$\left\{
\begin{array}{lll}
\frac{\partial}{\partial\theta}l_n(x;\theta)=0 \\
\frac{\partial^2}{\partial\theta^2}l_n(x;\theta)<0 \\
\end{array}
\right.$$ \noindent Les solutions de ce système fournissent des estimations par maximum de vraisemblance.
::::

\noindent 
**Exemple 6 : loi normale.** $\mathcal{N}(\mu, 1)$. On veut estimer le paramètre inconnu $\mu$ par maximum de vraisemblance. La vraisemblance est donnée par

```{=tex}
\begin{align}
L_n(x;\mu)&=\prod\limits_{i=1}^n\left(\frac{e^{-\frac{(x_i-\mu)^2}{2}}}{\sqrt{2\pi}}\right)\\
&=\frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\sum\limits_{i=1}^n (x_i-\mu)^2}
\end{align}
```
\noindent La log-vraisemblance est plus facile à manipuler :

```{=tex}
\begin{align}
l_n(x;\mu)&=\ln L_n(x;\mu) \\
&= -\frac{n}{2}\ln(2\pi)-\sum\limits_{i=1}^n(x_i-\mu)^2
\end{align}
```
\noindent La fonction $\mu\mapsto l_n(x;\mu)$ est deux fois dérivable sur $\mathbb{R}$ et $\frac{\partial}{\partial\mu}l_n(x;\mu)=2\sum_{i=1}^n(\mu-x_i)$. Une seule valeur de $\mu$ l'annule :

$$\widehat{\mu}_n=\frac{1}{n}\sum\limits_{i=1}^n x_i=\overline{x}_n$$ \noindent Par ailleurs $\frac{\partial^2}{\partial\mu^2}l_n(x;\mu)=2n>0$, et donc $\widehat{\mu}_n\in\arg\max\limits_{\mu\in\mathbb{R}}l(x;\mu)$. Finalement, un estimateur par maximum de vraisemblance est donné par

$$\widehat{\mu}_n=\overline{X}_n=\frac{1}{n}\sum\limits_{i=1}^n X_i$$

\noindent 
**Remarque.** Comme souvent en statistique, on commet un léger abus de notation en désignant par la même lettre l'estimateur $\widehat{\mu}_n=\frac{X_1+\dots+X_n}{n}=\widehat{\mu}_n(X_1,\dots,X_n)$ (qui est une statistique, i.e. une fonction de $(X_1,\dots,X_n$) et l'estimation $\widehat{\mu}_n=\frac{x_1+\dots+x_n}{n}=\widehat{\mu}_n(x_1,\dots, X_n)$ qui en est une réalisation. Conditionnellement à $(X_1,\dots,X_n)$ (i.e. si l'on suppose que l'on observe $(X_1,\dots, X_n)$) ces deux objets sont bien les mêmes.

\noindent 
**Exemple 7 : loi exponentielle.** Soit $(X_1,\dots,X_n)$ un échantillon i.i.d. tiré selon une loi exponentielle $\mathcal{E}(\lambda)$ de paramètre $\lambda>0$ inconnu. La vraisemblance est donnée par \begin{align}
L_n(x;\lambda)&=\prod\limits_{i=1}^n (\lambda e^{-\lambda x_i}\mathbb{1}_{x_i\geq 0}) \\
\end{align}

\noindent Si l'un des $x_i$ est négatif elle vaut $0$. Sinon on calcule la log-vraisemblance

$$l_n(x;\lambda)=n\ln(\lambda)-\lambda\sum_{i=1}^n x_i$$

\noindent La fonction $\lambda\in\mathbb{R}_{+}^*\mapsto l_n(x;\lambda)$ est deux fois dérivable et $\frac{\partial}{\partial\lambda}l_n(x;\lambda)=\frac{n}{\lambda}-\sum\limits_{i=1}^n x_i$, qui s'annule en $\lambda=\frac{n}{\sum\limits_{i=1}^n x_i}=\frac{1}{\overline{x}_n}$. De plus, $\frac{\partial^2}{\partial\lambda^2}l_n(x;\lambda)=-\frac{n}{\lambda^2}<0$ et donc à $x$ fixé, $l_n(x;\lambda)$ atteint son maximum en $\frac{1}{\overline{x_n}}$. L'estimateur du maximum de vraisemblance est donc $\widehat{\lambda}_n=\frac{1}{\overline{X}_n}$. On remarque qu'on retrouve ici le même estimateur que celui obtenu par la méthode des moments.


### Compléments (hors-programme)

\noindent On présente dans cette partie les notions suivantes :

- information de Fisher
- borne de Cramer-Rao
- statistique exhaustive
- famille exponentielle
- amélioration d'un estimateur

\noindent Ces notions ne sont pas au programme du concours, mais elles sont clairement dans sa périphérie immédiate. On les retrouve d'ailleurs dans le sujet d'interne 2022, mais leur connaissance n'est pas requise pour traiter le sujet.

#### Information de Fisher

\noindent Soient $X$ une variable aléatoire (discrète ou continue) à valeurs dans $\mathcal{X}$ de loi $L(x;\theta)>0$, avec $\theta\in\mathbb{R}$. On fait les hypothèses suivantes :

- existence de $\frac{\partial L}{\partial\theta}(x;\theta)$ et de $\frac{\partial^2}{\partial\theta^2}L(x;\theta)$
- on peut échanger tous les opérateurs de dérivation (à l'ordre $1$ et $2$) et d'intégration

\noindent On considère un échantillon i.i.d. $(X_1,\dots,X_n)$ tel que chacun des $X_i$ suit la même loi que $X$. Pour $x=(x_1,\dots,x_n)$ une réalisation de l'échantillon aléatoire $(X_1,\dots, X_n)$, On note $L_n(x;\theta)$ la vraisemblance de $(x_1,\dots, x_n)$ :

$$L_n(x;\theta)=\prod\limits_{i=1}^n L(x_i;\theta)$$

\noindent On appelle alors **score** la quantité (aléatoire) $\frac{\partial}{\partial\theta}\,\ln L_n(X;\theta)=\frac{\partial}{\partial\theta}\, l_n(X;\theta)$, i.e. la dérivée de la log-vraisemblance par rapport à $\theta$.

:::: {.thmbox .thm data-latex="important"}
**Théorème :** On a 

$$\mathbb{E}_{\theta}\left(\frac{\partial }{\partial\theta}\, l_n(X;\theta)\right)=0$$

i.e. le score est d'espérance nulle.
::::

\noindent 
**Démonstration.** On démontre cette égalité dans le cas à densité :
\begin{align}
\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta}\, l_n(X ; \theta)\right) &= \int_{\mathbb{R}^n} \frac{\partial}{\partial\theta} \,l_n(x;\theta)\,L_n(x;\theta)\,dx \\
&=\int_{\mathbb{R}^n}\frac{\frac{\partial L_n}{\partial\theta}(x;\theta)}{L_n(x;\theta)}\,L_n(x;\theta)\,dx \\
&= \int_{\mathbb{R}^n} \frac{\partial L_n}{\partial\theta}(x;\theta)\,dx \\
&=\frac{\partial}{\partial\theta}\int_{\mathbb{R}^n} L_n(x;\theta) \, dx \text{ (on permute intégrale et dérivée)} \\
&= 0 \text{ (car } \int_{\mathbb{R}^n} L_n(x;\theta)\,dx=1\text{)} \\
\end{align}
$\square$

\noindent L'information de Fisher est définie à partir du score de la façon suivante :

:::: {.defbox .def data-latex="important"}

<center>**Information de Fisher**</center>

\vspace{0.5cm}

\noindent L'**information de Fisher** est la quantité définie par 

$$I_n(\theta)\equiv\mathbb{E}_{\theta}\left(\left(\frac{\partial }{\partial\theta}\, l_n(X;\theta)\right)^2\right)=\mathbb{V}_{\theta}\left(\frac{\partial}{\partial\theta}l_n(X;\theta)\right)$$


\noindent Lorsque le domaine de $X$ ne dépend pas de $\theta$, l'information de Fisher est aussi égale à 

$$I_n(\theta)=-\mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2} l_n(X;\theta)\right)$$
\noindent Cette dernière expression est généralement plus facile à calculer.
::::


\vspace{0.5cm}

\noindent
**Interprétation de l'information de Fisher.** On utilise généralement l'information de Fisher lorsqu'on veut inférer sur un paramètre inconnu $\theta$ par maximum de vraisemblance. Par construction, l'estimation $\widehat{\theta}$ que l'on obtient par cette méthode est celle qui maximise la log-vraisemblance $\ln L_n(X;\theta)$, pour une observation de $X$ donnée. L'expression $I_n(\theta)=-\mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2}\ln L_n(X;\theta)\right)$ montre que l'information de Fisher correspond (au signe près) à la courbure de la log-vraisemblance. Plus celle-ci est importante, plus la courbe présente un "pic" autour du maximum, et donc plus la valeur estimée de ce maximum est précise. Au contraire, si la courbure est faible, la courbe est aplatie autour du maximum, et donc l'estimation de $\theta$ sera moins précise. Dit autrement, l'information de Fisher quantifie le niveau d'information que nous apporte l'observation relativement au paramètre $\theta$.

\vspace{0.5cm}

\noindent 
**Démonstration.** Etant donné que $\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right)=0$ on a $\mathbb{V}_{\theta}\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right)=\mathbb{E}_\theta\left(\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta\right)^2\right)$, ce qui démontre la première égalité.
\noindent Pour démontrer la deuxième égalité, on dérive par rapport à $\theta$ l'égalité $\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right)=0$. Pour cela, on utilise la permutation $\frac{\partial}{\partial\theta}\int=\int\frac{\partial}{\partial\theta}$ qui est possible car le domaine de $X$ ne dépend pas de $\theta$. On obtient donc :

\begin{align}
0 &= \frac{\partial}{\partial\theta}\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right) \\
&= \frac{\partial}{\partial\theta}\int_{\mathbb{R}^n}\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right) L_n(x;\theta)\,dx \\
&=\int_{\mathbb{R}^n}\frac{\partial}{\partial\theta}\left(\left(\frac{\partial}{\partial\theta} \,l_n(X;\theta)\right) L_n(x;\theta)\right)\,dx \\
&=\int_{\mathbb{R}^n}\left(\frac{\partial^2}{\partial\theta^2}l_n(x;\theta)\right)L_n(x;\theta)\,dx+\int_{\mathbb{R}^n}\frac{\partial}{\partial\theta}l_n(x;\theta)\frac{\partial}{\partial\theta}L_n(x;\theta)\,dx \\
&= \int_{\mathbb{R}^n}\left(\frac{\partial^2}{\partial\theta^2}l_n(x;\theta)\right)L_n(x;\theta)\,dx+\int_{\mathbb{R}^n}\left(\frac{\partial}{\partial\theta}l_n(x;\theta)\right)^2 L_n(x;\theta)\,dx \\
&= \mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2} l_n(X;\theta)\right)+\mathbb{E}_{\theta}\left(\left(\frac{\partial}{\partial\theta} l_n(X;\theta)\right)^2\right)
\end{align}

\noindent d'où

$$\mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2} l_n(X;\theta)\right)=-\mathbb{E}_{\theta}\left(\left(\frac{\partial}{\partial\theta} l_n(X;\theta)\right)^2\right)$$

\noindent ce qui achève la démonstration.
$\square$

\noindent L'information de Fisher vérifie une propriété d'additivité :

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème (additivité de l'information de Fisher) :** Si l'ensemble $\left\{x\in\mathbb{R}^n, f(x;\theta)>0\right\}$ ne dépend pas de $\theta$, alors l'information de Fisher est additive, i.e.

$$I_n(\theta)=n\,I_1(\theta)$$
:::

\noindent Si le domaine de $X$ ne dépend pas de $\theta$, l'information de Fisher apportée par un échantillon $(X_1,\dots,X_n)$ est donc égale à $n$ fois l'information de Fisher apportée par chacune des observations $X_i$. Cela signifie que chaque observation apporte la même information de Fisher.

#### Borne de Fréchet-Darmois-Cramer-Rao

\noindent Sous certaines hypothèses, on peut montrer que la variance d'un estimateur sans biais ne peut être inférieure à une certaine borne, appelée borne de Fréchet-Darmois-Cramer-Rao, liée à l'information de Fisher :

:::: {.thmbox .thm data-latex="important"}
**Théorème :** On suppose que les hypothèses suivantes, appelées **hypothèses de Cramer-Rao**, sont vérifiées :

- **(H1) :** $\Theta$ est un ouvert sur lequel $f(x;\theta)>0$ et $\theta\mapsto f(x;\theta)$ est dérivable pour tout $x$ 

- **(H2) :** on peut permuter $\int$ et $\frac{\partial}{\partial\theta}$ 

- **(H3) :** $\forall\theta\in\Theta, \, I_n(\theta)>0$

- **(H4) :** $g:\Theta\longrightarrow\mathbb{R}$ est une fonction dérivable 

\noindent Alors, pour tout estimateur **sans biais** $T_n=T_n(X_1,\dots, X_n)$ de $g(\theta)$ on a l'inégalité

$$\mathbb{V}_{\theta}(T_n)\geq\frac{\left(g'(\theta)\right)^2}{I_n(\theta)}$$

\noindent Le nombre $\frac{\left(g'(\theta)\right)^2}{I_n(\theta)}$ s'appelle la **borne de Fréchet-Darmois-Cramer-Rao (FDCR)**.

\noindent Dans le cas particulier où $T_n=\widehat{\theta}_n$ est un estimateur sans biais de $\theta$ (cas où $g(\theta)=\theta$) on a $\mathbb{V}_{\theta}(\widehat{\theta}_n)\geq\frac{1}{I_n(\theta)}$.
:::

\vaspace{0.5cm}

\noindent Ce théorème est admis.

\noindent La borne FDCR n'est pas nécessairement atteinte. Quand elle l'est, l'estimateur qui l'atteint est dit **efficace** :

:::: {.defbox .def data-latex="important"}
<center>**Estimateurs efficaces**</center>

\vspace{0.5cm}

\noindent Un estimateur $T_n$ *sans biais* de $g(\theta)$ tel que $\mathbb{V}_{\theta}(T_n)=\frac{\left(g'(\theta)\right)^2}{I_n(\theta)}$ est appelé un **estimateur efficace**.
:::



#### Statistiques exhaustives

\noindent Tout échantillon $(X_1,\dots, X_n)$ tel que $X_i\sim\mathcal{L}_{\theta}$ apporte de l'information sur le paramètre inconnu $\theta$, et donc sur la loi inconnue $\mathcal{L}_{\theta}$. Plutôt que de faire de l'inférence à partir de l'échantillon $(X_1,\dots, X_n)$ on préfère en général utiliser une statistique $T_n=T(X_1,\dots,X_n)$, qui est une sorte de résumé de l'échantillon tout entier. La contrepartie est qu'en général, le passage de $(X_1,\dots,X_n)$ à son résumé $T_n$ génère une perte d'information sur $\theta$. Une statistique exhaustive est une statistique qui n'engendre pas de telle perte, autrement dit elle contient toute l'information sur $\theta$ contenue dans l'échantillon $(X_1,\dots, X_n)$. On formalise cette idée de la façon suivante :

:::: {.defbox .def data-latex="important"}
<center>**Statistiques exhaustives**</center>

\noindent Une statistique $T_n$ est dite **exhaustive** si la loi conditionnelle $\mathcal{L}(X|T_n=t)$ est indépendante de $\theta$. Conditionnellement à l'observation $T=t$, la loi de $X$ ne dépend plus de $\theta$ :

$$\mathbb{P}(X=x|T=t,\theta)=\mathbb{P}(X=x|T=t) \text{    (pour une loi discrète)}$$

$$f(x|T=t,\theta)=f(x|T=t) \text{    (pour une loi continue)}$$
::::

\noindent Dit autrement, une fois que l'on sait que $T_n=t$, ajouter la connaissance de $X$ n'apporte plus aucune information supplémentaire sur $\theta$.

\noindent Cette définition n'est pas très commode à manipuler, et en pratique pour démontrer qu'une statistique est (ou n'est pas) exhaustive on utilise plutôt le théorème de factorisation de Neyman-Fisher :

:::: {.defbox .def data-latex="important"}
**Théorème (factorisation de Neyman-Fisher) :** Une statistique $T_n$ est exhaustive si, et seulement s'il existe deux fonctions mesurables positives $g$ et $h$ telles qu'on ait la factorisation suivante :

$$L_n(x;\theta)=g(T_n(x);\theta).h(x)$$
::::

\noindent
**Remarques : i.** La notion de mesurabilité n'est pas au programme du concours. Il s'agit d'une classe très générale de fonctions, qui englobe en particulier les fonctions continues, continues par morceaux etc. Il est même assez compliqué de construire une fonction non mesurable. En pratique :

- pour démontrer qu'une statistique est exhaustive, il suffit de montrer qu'une telle décomposition existe avec $g$ et $h$ continues (car continue implique mesurable) ;

- pour démontrer qu'une telle statistique n'est pas exhaustive, il suffit de démontrer qu'une telle décomposition avec $g$ et $h$ quelconques est impossible (elle sera en particulier impossible avec $g$ et $h$ mesurables).

\noindent 
**ii.** Il n'y a pas unicité du couple $(g,h)$. Par exemple, si $(g,h)$ permet une factorisation, alors pour tout $\lambda$ strictement positif $\left(\lambda g, \frac{h}{\lambda}\right)$ aussi.

#### Famille exponentielle

\noindent Les densités suivantes assurent l'existence d'une statistique exhaustive :

:::: {.thmbox .thm data-latex="important"}
**Théorème de Darmois :** Soit $\theta\in\Theta\subset\mathbb{R}$. Soit $f(x;\theta)$ une densité telle que l'ensemble $\{x\in\mathbb{R}^n, \, f(x;\theta)>0\}$ ne dépend pas de $\theta$. Alors, l'échantillon $(X_1,\dots,X_n)$ admet une statistique exhaustive si et seulement si $f(x;\theta)$ est de la forme

$$f(x;\theta)=\exp\left(a(x)\alpha(\theta)+b(x)+\beta(\theta)\right)$$

\noindent Par ailleurs, si l'application $a$ est de classe $\mathcal{C}^1$, alors $T_n\equiv\sum\limits_{i=1}^n a(X_i)$ est une statistique exhaustive.

\noindent La famille des densités $f(x;\theta)$ vérifiant ces propriétés s'appelle la **famille exponentielle**.
:::

\noindent 
**Remarque.** Ici, on convient de parler de *densité* aussi bien pour une variable discrète que pour une variable continue. Pour une variable discrète, la densité est par définition $f(x;\theta)\equiv\mathbb{P}_{\theta}(X=x)$.

\noindent 
**Exemple (loi de Poisson).** La densité d'une loi de Poisson de paramètre $\lambda$ est $f(x;\lambda)=e^{-\lambda}\frac{\lambda^x}{x!}\mathbb{1}_{x\in\mathbb{N}}$. L'ensemble $\left\{x\in\mathbb{R},\,f(x;\theta)>0\right\}$ est $\mathbb{N}$, qui ne dépend pas de $\lambda$. Par ailleurs :
$$f(x;\lambda)=\exp\left(-\lambda+x\,\ln(\lambda)-\sum\limits_{i=1}^x \ln i\right)$$

\noindent On reconnait bien la forme générale d'une densité de la famille exponentielle, avec $a(x)=x$, $\alpha(\lambda)=\ln(\lambda)$, $b(x)=\sum\limits_{i=1}^x \ln i$, $\beta(\lambda)=-\lambda$. L'application $a$ est par ailleurs de classe $\mathcal{C}^1$. On en déduit avec le théorème de Darmois que la statistique $T_n=\sum\limits_{i=1}^n X_i$ est une statistique exhaustive pour $\theta$.

\noindent La famille exponentielle permet de construire des estimateurs **efficaces**.

:::: {.thmbox .thm data-latex="important"}
\noindent
**Théorème :** On suppose les hypothèses de Cramer-Rao vérifiées. On suppose également que $\theta\mapsto \frac{\partial}{\partial\theta}f(x;\theta)$ est continue en $\theta$. Soit $T_n$ un estimateur **sans biais** de $g(\theta)$.
 
\noindent Alors, $T_n$ est un estimateur efficace si et seulement si la densité $f(x;\theta)$ appartient à la famille exponentielle.
:::


#### Rao-Blackwellisation d'un estimateur

\noindent Le théorème de Rao-Blackwell montre comment améliorer un estimateur.

:::: {.thmbox .thm data-latex="important"}
**Théorème de Rao-Blackwell :** Soient $T_n$ une statistique exhaustive et $S$ un estimateur sans biais de $g(\theta)$. Alors, l'estimateur $\mathbb{E}_{\theta}(S|T_n)$ est sans biais et $\mathbb{V}_{\theta}(\mathbb{E}_{\theta}(S|T_n))\leq\mathbb{V}_{\theta}(S)$.
::::

\noindent L'estimateur $\mathbb{E}_{\theta}(S|T_n)$ est dit **préférable** à l'estimateur $S$.

### Estimation des coefficients d'une régression linéaire

#### Présentation du modèle

\noindent $X$ et $Y$ sont deux variables aléatoires pour lesquelles on dispose d'observations $x_1,\dots, x_n$ et $y_1,\dots y_n$. On considère le modèle

$$Y_i=aX_i+b+u_i$$

\noindent où $(a,b)$ est un couple de réels inconnus et $u_i$ est un terme d'erreur (inconnu lui aussi). Le but est d'estimer des coefficients $(a,b)$ à partir de l'échantillons d'observations $(x_i, y_i)$ et de donner des propriétés des estimateurs obtenus sous certaines hypothèses.

\noindent 
**Hypothèses du modèle.** On fait les hypothèses suivantes :

-   **(H1) :** Les couples $(X_i, Y_i)$ sont i.i.d.
-   **(H2) :** Les termes d'erreur $u_i$ sont indépendants des $X_i$
-   **(H3) :** $\mathbb{E}(u_i|X_i)=0$ (hypothèse d'exogénéité)
-   **(H4) :** $\mathbb{V}(u_i|X_i)=\sigma_u^2$ ne dépend pas de $X_i$ (hypothèse d'homoscédasticité)
-   **(H5) :** $u_i|X_i\sim\mathcal{N}(0, \sigma_u^2)$ (hypothèse de normalité des termes d'erreur)

\noindent On présente deux approches différentes pour estimer $a$ et $b$ : par la méthode des moindres carrés et par maximum de vraisemblance. Bien que différentes, ces méthodes vont fournir les mêmes estimateurs.

\noindent Avant cela, on rappelle quelques résultats classiques de statistique descriptive.

#### Rappels utiles

\noindent Avant de présenter cette méthode, on rappelle des égalités qui àa la fois très utiles et très classiques. Il faut les connaître pour le concours et savoir les redémontrer.

:::: {.methbox .meth data-latex="important"}
<center>**Moyenne, covariance, variance**</center>

\vspace{0.5cm}

\noindent Pour $x=(x_1,\dots, x_n)\in\mathbb{R}^n$ on note

-   $\overline{x}_n=\frac{1}{n}\sum\limits_{i=1}^n x_i$ la moyenne de $x$
-   $\sigma_x^2=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\overline{x}_n)^2$ la variance de $x$
-   si de plus $y=(y_1,\dots, y_n)$, $\sigma_{xy}=\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n)$ est la covariance de $x$ et $y$.

\noindent On a alors les égalités suivantes :

**1.** $\sigma_{xx}=\sigma_x^2$

**2.** $\sum\limits_{i=1}^n (x_i-\overline{x}_n)=0$

**3. Différentes formules de la covariance :**

```{=tex}
\begin{align}
\sigma_{xy} &= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) \\
&= \frac{1}{n}\sum\limits_{i=1}^n(x_i-\overline{x}_n)y_i \\
&= \frac{1}{n}\sum\limits_{i=1}^n x_i(y_i-\overline{y}_n) \\
&= \frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\overline{y}_n
\end{align}
```
**4. Différentes formules de la variance :**

```{=tex}
\begin{align}
\sigma_x^2 &= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)^2 \\
&= \frac{1}{n}\sum\limits_{i=1}^n x_i^2-(\overline{x}_n)^2
\end{align}
```
::::

**Démonstration.**

\noindent 
**1.** Evidente

**2.** \begin{align} 
\sum\limits_{i=1}^n (x_i-\overline{x}_n) &= \sum\limits_{i=1}^n x_i -n\overline{x}_n \\
&= n\overline{x}_n-n\overline{x}_n \\
& =0
\end{align}

**3.** \begin{align}
\sigma_{xy} &= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) \\
&=\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i-\frac{\overline{y}_n}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n) \\
&= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i
\end{align}

\noindent d'après l'égalité 2.

\noindent Par symétrie des rôles joués par $x$ et $y$ on a donc aussi $\sigma_{xy}=\frac{1}{n}\sum\limits_{i=1}^n x_i(y_i-\overline{y}_n)$.

\noindent On montre la dernière égalité :

```{=tex}
\begin{align}
\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) &=
\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i \\
&= \frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\frac{1}{n}\sum\limits_{i=1}^n y_i
\\
&=\frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\overline{y}_n
\end{align}
```
**4.** \noindent On applique la dernière égalité de 4 dans le cas particulier où $x=y$. On obtient alors

$$\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)^2=\frac{1}{n}\sum\limits_{i=1}^n x_i^2-\left(\frac{1}{n}\sum\limits_{i=1}^n x_i\right)^2$$ $\square$

#### Estimation de $a$ et $b$ par la méthode des moindres carrés

\noindent On montre maintenant les formules des estimateurs de $a$ et $b$ par application de la méthode des moindres carrés :

:::: {.methbox .meth data-latex="important"}
<center>**Estimation de $a$ et $b$ par la méthode des moindres carrés**</center>

\vspace{0.5cm}

\noindent La méthode des moindres carrés consiste à minimiser l'erreur quadratique globale

$$E(\alpha,\beta)\equiv\sum\limits_{i=1}^n (Y_i-\alpha X_i-\beta)^2$$ \noindent qui représente l'erreur globale faite en approchant $Y_i$ par $\alpha X_i+\beta$.

\noindent Cette méthode fournit les estimateurs suivants de $a$ et $b$ :

```{=tex}
\begin{align}
\left\{
\begin{array}{ll}
\widehat{a} &= \frac{\overline{\sigma_{XY}}}{\overline{\sigma^2_X}} \\
\widehat{b} &= \overline{Y}_n-\widehat{a}\overline{X}_n
\end{array}
\right.
\end{align}
```
\noindent où on note $\overline{\sigma_{XY}}=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X}_n)(Y_i-\overline{Y}_n)$ et $\overline{\sigma^2_X}=\overline{\sigma_{XX}}=\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2$.
::::

\noindent 
**Démonstration.** La fonction $(\alpha, \beta)\mapsto E(\alpha, \beta)$ est deux fois dérivable par rapport à chacune de ses variables. \noindent

**Conditions de premier ordre (CPO) :** \noindent Les conditions du premier ordre s'écrivent

```{=tex}
\begin{align}
\left\{
\begin{array}{ll}
\frac{\partial}{\partial\alpha} E(\alpha, \beta) &=0 \\
\frac{\partial}{\partial\beta} E(\alpha, \beta) &=0 \\
\end{array}
\right.
\end{align}
```
\noindent i.e.

```{=tex}
\begin{align}
\left\{
\begin{array}{ll}
\sum\limits_{i=1}^n X_i Y_i-\alpha\sum\limits_{i=1}^n X_i^2-\beta\sum\limits_{i=1}^n X_i&=0 \\
\sum\limits_{i=1}^n Y_i-\alpha\sum\limits_{i=1}^n X_i-n\beta &= 0 \\
\end{array}
\right.
\end{align}
```
\noindent Il s'agit d'un système de deux équations à deux inconnues $(\alpha, \beta)$. Sa résolution donne

```{=tex}
\begin{align}
\left\{
\begin{array}{ll}
\alpha &= \frac{\frac{1}{n}\sum\limits_{i=1}^n X_iY_i-\left(\frac{1}{n}\sum\limits_{i=1}^n X_i\right)\left(\frac{1}{n}\sum\limits_{i=1}^n Y_i\right)}{\frac{1}{n}\sum\limits_{i=1}^n X_i^2-\left(\frac{1}{n}\sum\limits_{i=1}^n X_i\right)^2} \\
\beta &= \overline{Y}_n-\alpha\overline{X}_n
\end{array}
\right.
\end{align}
```
soit encore

```{=tex}
\begin{align}
\left\{
\begin{array}{ll}
\alpha &= \frac{\overline{\sigma_{XY}}}{\overline{\sigma^2_X}} \\
\beta &= \overline{Y}_n-\alpha\overline{X}_n
\end{array}
\right.
\end{align}
```
\noindent Par ailleurs, pour tout couple $(x, y)$ de réels, la fonction $(\alpha, \beta)\mapsto (y-\alpha x-\beta)^2$ est convexe. Le point critique trouvé ci-dessus est donc un minimum.

\noindent On en déduit le résultat.

$\square$

\vspace{0.5cm}

#### Estimation de $a$ et $b$ par la méthode du maximum de vraisemblance

\noindent La méthode par maximum de vraisemblance requiert une information supplémentaire : celle de la distribution de la variable de terme d'erreur $u$. Or, une telle information est justement donnée ici par l'hypothèse (H5) de distribution normale du terme d'erreur.

:::: {.methbox .meth data-latex="important"}
<center>**Estimation de $a$ et $b$ par la méthode du maximum de vraisemblance.**</center>

\vspace{0.5cm}

\noindent Sous l'hypothèse $(H5)$ de distribution normale des termes d'erreur, la méthode par maximum de vraisemblance fournit les mêmes estimateurs $\widehat{a}$ et $\widehat{b}$ que la méthode des moindres carrés.
:::

\noindent 
**Démonstration.** La vraisemblance est donnée par

$$L_n((\alpha,\beta);u)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma_u}e^{-\frac{(Y_i-\alpha X_i-\beta)^2}{2\sigma_u^2}}$$

\noindent On passe à la log-vraisemblance, qui est plus simple à dériver

$$l_n((\alpha,\beta);u)=-n\ln(\sqrt{2\pi}\sigma_u^2)-\frac{(Y_i-\alpha X_i-\beta)^2}{2\sigma_u^2}$$

\noindent On résout alors en $(\alpha, \beta)$ le système d'équations

```{=tex}
\begin{align}
\frac{\partial l_n}{\partial\alpha}l((\alpha,\beta);u) &= 0 \\
\frac{\partial l_n}{\partial\beta}l((\alpha,\beta);u) &= 0 \\
\end{align}
```
\noindent soit

```{=tex}
\begin{align}
\frac{X_i(Y_i-\alpha X_i-\beta)}{2\sigma_u^2} &= 0 \\
\frac{Y_i-\alpha X_i-\beta}{2\sigma_u^2} &= 0 \\
\end{align}
```
\noindent On vérifie facilement qu'on obtient le même couple de solution qu'avec la méthode des moindres carrés, et que ce couple constitue bien un maximum de la log-vraisemblance.

$\square$

\noindent 
**Remarque :** Dans des approches plus générales que celle présentée ici, aucune hypothèse n'est faite sur la distribution des termes d'erreur. Dans ce cas, la méthode par maximum de vraisemblance n'est plus applicable. On peut cependant toujours utiliser la méthode des moindres carrés.

#### Absence de biais des estimateurs $\widehat{a}$ et $\widehat{b}$

:::: {.thmbox .thm data-latex="important"}
**Théorème : (absence de biais des estimateurs MCO)** Les estimateurs

$$\widehat{a}=\frac{\overline{\sigma_{XY}}}{\overline{\sigma_X^2}}$$ et $$\widehat{b}=\overline{Y}_n-\widehat{a}\overline{X}_n$$ \noindent sont des estimateurs **sans biais** de $a$ et $b$.
::::

\noindent 
**Démonstration.** On remarque d'abord qu'avec l'hypothèse d'exogénéité (H3) $\mathbb{E}(u_i|X_i)=0$ on a $\mathbb{E}(Y_i|X_i)=aX_i+b$ et donc $\mathbb{E}(Y_i-\overline{Y}_n|X_1,\dots, X_n)=a(X_i-\overline{X}_n)$. D'où

```{=tex}
\begin{align}
\mathbb{E}(\widehat{a}|X_1,\dots, X_n) &= \mathbb{E}\left(\left.\frac{\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)(Y_i-\overline{Y}_n)}{\overline{\sigma_X^2}}\right|X_1,\dots, X_n\right) \\
&=\frac{1}{\overline{\sigma_X^2}}\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X}_n)\mathbb{E}(\left. Y_i-\overline{Y}_n\right|X_1\,\dots,X_n) \\
&= a\frac{1}{\overline{\sigma_X^2}}\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2 \\
& =a\frac{\overline{\sigma_X^2}}{\overline{\sigma_X^2}} \\
& =a
\end{align}
```
\noindent Par ailleurs

\begin{align}
\mathbb{E}(\widehat{b}|X_1,\dots,X_n)&=\mathbb{E}(\overline{Y}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&=\frac{1}{n}\sum\limits_{i=1}^n \mathbb{E}(Y_i|X_1,\dots,X_n)-\overline{X}_n\mathbb{E}(\widehat{a}|X_1,\dots,X_n) \\
&=\frac{1}{n}\sum\limits_{i=1}^n (aX_i+b)-a\overline{X}_n \\
&= a\overline{X}_n+b-a\overline{X}_n \\
&= b
\end{align} $\square$

#### Variance des estimateurs $\widehat{a}$ et $\widehat{b}$

:::: {.thmbox .thm data-latex="important"}
**Théorème (variance des estimateurs MCO)** Les estimateurs $\widehat{a}$ et $\widehat{b}$ ont pour variances

```{=tex}
\begin{align}
\mathbb{V}(\widehat{a}|X_1,\dots, X_n) &= \frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2} \\
\mathbb{V}(\widehat{b}|X_1,\dots,X_n) &=\sigma_u^2\left(\frac{1}{n}+ \frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right)
\end{align}
```
::::

\noindent 
**Démonstration.** On remarque tout d'abord que

$$\mathbb{V}(Y_i|X_1,\dots,X_n)=\sigma_u^2$$

\noindent En effet

```{=tex}
\begin{align}
\mathbb{V}(Y_i|X_1,\dots,X_n) &= \mathbb{V}(aX_i+b+u_i|X_1,\dots, X_n) \\
&= \mathbb{V}(u_i|X_1,\dots,X_n) \\
&= \sigma_u^2
\end{align}
```
\noindent Le passage de la première à la deuxième ligne vient du fait qu'à $X_1,\dots, X_n$ fixées, $aX_i+b$ est considérée comme une constante, et donc ce terme a une contribution à la variance conditionnellement à $X_1,\dots X_n$.

\noindent On a donc

```{=tex}
\begin{align}
\mathbb{V}(\widehat{a}|X_1,\dots X_n) &= \mathbb{V}\left(\left.\frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)Y_i}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right|X_1,\dots, X_n\right) \\
&= \frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\mathbb{V}(Y_i|X_1,\dots,X_n)}{\left(\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\right)^2} \\
& \text{ (somme de VA i.i.d.)} \\
&= \frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\sigma_u^2}{\left(\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\right)^2} \\
&= \frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}
\end{align}
```
\noindent et

```{=tex}
\begin{align}
\mathbb{V}(\widehat{b}|X_1,\dots,X_n) &= \mathbb{V}(\overline{Y}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&= \mathbb{V}(a\overline{X}_n+b+\overline{u}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&= \mathbb{V}((a-\widehat{a}\overline{X}_n)+b+\overline{u}_n|X_1,\dots, X_n) \\
&= \overline{X}_n^2\underbrace{\mathbb{V}(\widehat{a}|X_1,\dots,X_n)}_{=\frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}+\underbrace{\mathbb{V}(\overline{u}_n|X_1,\dots,X_n)}_{=\frac{\sigma_u^2}{n} \text{ car } u_i \text{ i.i.d. de variance } \sigma_u^2} \\
&= \overline{X}_n^2\frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}+\frac{\sigma_u^2}{n} \\
&= \sigma_u^2\left(\frac{1}{n}+\frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right)
\end{align}
```
$\square$

\noindent

#### Résidus

\noindent La variance $\sigma_u^2$ des termes d'erreur $u_i$ n'est pas connue. Cependant, elle peut être estimée. Pour cela, on introduit la notion de **résidu**.

\noindent Le résidu $\widehat{u}_i$ est défini comme l'écart entre la vraie valeur $Y_i$ et sa prédiction $\widehat{Y}_i=\widehat{a}X_i+\widehat{b}$ :

$$\widehat{u}_i\equiv Y_i-\widehat{Y}_i$$ \noindent On a donc $$\widehat{u}_i=Y_i-\widehat{a}X_i-\widehat{b}$$

\noindent Il s'agit d'une estimation (sans biais) de la vraie erreur

$$u_i=Y_i-aX_i-b$$

:::: {.thmbox .thm data-latex="important"}
**Théorème (estimation de la variance) : ** $\sigma_u^2$</center>

\noindent La variance $\sigma_u^2$ est estimée par

$$s^2=\frac{1}{n-2}\sum\limits_{i=1}^n \widehat{u}_i^2$$
::::

#### Distributions des estimateurs $\widehat{a}$ et $\widehat{b}$

\noindent On admet alors le résultat suivant

:::: {.thmbox .thm data-latex="important"}
**Théorème :** Sous l'hypothèse de normalité des termes d'erreur $u_i$, on a

$$\frac{(n-2)s^2}{\sigma_u^2}\sim\chi^2_{(n-2)}$$ \noindent et les statistiques

$$\frac{\widehat{a}-a}{s\sqrt{\frac{1}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}}$$ et

$$\frac{\widehat{b}-b}{s\sqrt{\frac{1}{n}+\frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}}$$ \noindent suivent une loi de Student à $n-2$ degrés de liberté.
::::

#### Convergence des estimateurs $\widehat{a}$ et $\widehat{b}$

\noindent Si on suppose que les $X_i$ admettent des moments d'ordre $1$ et $2$, alors on peut montrer que les estimateurs $\widehat{a}$ et $\widehat{b}$ sont des estimateurs convergents.

\noindent On sait déjà qu'ils sont sans biais, il suffit donc de démontrer que leurs variances tendent vers $0$.

\noindent Or, comme $X_i$ admet des moments d'ordres $1$ et $2$ on a, conditionnellement à $X_1,\dots,X_n$ :

$$\overline{X}_n\approx\mathbb{E}(X)$$ \noindent et $$\sum\limits_{i=1}^n(X_i-\overline{X}_n)^2\approx n\mathbb{V}(X_1)$$ \noindent On en déduit que

$$\mathbb{V}(\widehat{a}|X_1,\dots, X_n)\approx\frac{\sigma_u^2}{n\mathbb{V}(X_1)}\longrightarrow 0$$

et

$$\mathbb{V}(\widehat{b}|X_1,\dots,X_n)\approx\sigma_u^2\left(\frac{1}{n}+\frac{\overline{X}_n^2}{n\mathbb{V}(X_1)}\right)\longrightarrow 0$$

\noindent $\widehat{a}$ et $\widehat{b}$ sont des estimateurs sans biais de $a$ et $b$ de variances asymptotiquement nulles. Ce sont donc des estimateurs convergents de $a$ et $b$.

### Intervalles de confiance

\noindent Jusqu'à présent, l'estimation était uniquement envisagée du point de vue de l'estimation *ponctuelle* : il s'agissait, à partir de l'observation d'un échantillon $(X_1,\dots,X_n)$ de fournir une valeur ponctuelle $\widehat{\theta}_n$ approchant la vraie valeur inconnue d'un paramètre $\theta$. Cependant, la valeur estimée dépend de l'échantillon tiré. En effet, si l'on tire $1,000$ échantillons différents, on va obtenir $1\,000$ estimations $\widehat{\theta}^{(1)}_n,\dots,\widehat{\theta}^{(1\,000)}_n$ *a priori* différentes également. Certaines de ces estimations peuvent être des valeurs atypiques. Se pose donc la question de la *confiance* que l'on peut accorder à l'estimation obtenue à partir d'une seule réalisation particulière $(x_1,\dots,x_n)$ de l'échantillon, puisqu'en pratique c'est tout ce dont on dispose pour inférer sur $\theta$. 

\noindent L'approche présentée jusqu'ici ne répond pas à cette question. Le bon outil pour aborder ce problème est la notion d'*intervalle de confiance*.

:::: {.defbox .def data-latex="important"}
<center>**Intervalles de confiance**</center>

\vspace{0.5cm}

\noindent Soit $\theta$ un paramètre inconnu et $\alpha$ un réel compris entre $0$ et $1$. On appelle **intervalle de confiance de niveau $1-\alpha$ du paramètre $\theta$** tout intervalle $[a;b]$ tel que $$\mathbb{P}\left(\theta\in [a;b]\, \right)=1-\alpha$$
:::

\noindent
**Remarques : i.** Les réels $a$ et $b$ dépendent de $\theta$, du niveau de confiance $1-\alpha$. En pratique, pour les déterminer on utilise l'échantillon $(X_1,\dots, X_n)$, ou plus précisément un résumé $T_n$ de cet échantillon, i.e. une statistique $T_n=T_n(X_1,\dots,X_n)$. On a donc 

\begin{align}
a &= a_n(T_n\,;\,\theta\,;\,\alpha) \\
b &= b_n(T_n\,;\,\theta\,;\,\alpha) \\
\end{align}

\noindent Ce sont donc des **variables aléatoires**, que l'on notera désormais plus simplement $a_n$ et $b_n$. L'intervalle de confiance est donc lui-même un objet aléatoire.

\vspace{0.5cm}

\noindent 
**ii.** Idéalement, on aimerait savoir avec certitude que $\theta\in [a,b]$. Comme les réels $a$ et $b$ dépendent de l'échantillon tiré, on ne peut espérer mieux qu'une probabilité d'appartenance de $\theta$ à $[a,b]$. A défaut qu'elle soit égale à 1, on la veut proche de $1$, autrement dit on veut $\alpha$ proche de $0$. En pratique, on prendra souvent $\alpha=0,05$, parfois $\alpha=0,01$.

**iii.** Le réel $\alpha$ représente un *risque* : celui de donner un intervalle de confiance qui ne contienne pas la vraie valeur de $\theta$.

\noindent
**iv.** Réduire la valeur de $\alpha$ n'est pas gratuit. Le prix à payer est un élargissement de l'intervalle de confiance $[a,b]$, ce qui signifie des intervalles de confiance moins fins et donc moins informatifs sur la vraie valeur de $\theta$. Inversement, si on veut des intervalles de confiance plus fins, il faut assumer un risque plus grand d'avoir un intervalle de confiance laissant échapper le vrai $\theta$.


\noindent On voit maintenant une méthode générale de construction de $[a_n\,;\,b_n]$.

:::: {.methbox .meth data-latex="important"}
<center>**Construction d'un intervalle de confiance**</center>

\noindent On cherche un couple de réels $(a_n,b_n)$ tel que $$\mathbb{P}\left(a_n\leq \theta\leq b_n\right)=1-\alpha$$
\noindent On suppose qu'on dispose d'une statistique $T_n$ à partir de laquelle on calcule ces réels :

\begin{align}
a_n &= a_n(T_n) \\
b_n &= b_n(T_n)
\end{align}

\noindent On cherche alors à transformer l'écriture $\theta\in[a_n(T_n)\,;\,b_n(T_n)]$ en une écriture équivalente du type $T_n\in[\alpha_n(\theta)\,;\,\beta_n(\theta)]$, autrement dit on veut

$$\theta\in[a_n(T_n)\,;\,b_n(T_n)]\Leftrightarrow T_n\in[\alpha_n(\theta)\,;\,\beta_n(\theta)]$$

\noindent Dans ce cas, on doit avoir

$$\mathbb{P}\left(T_n\in[\alpha_n(\theta)\,;\,\beta_n(\theta)]\right)=1-\alpha$$
\noindent Il s'agit donc de trouver un couple de réels $(\alpha_n,\beta_n)=(\alpha_n(\theta),\beta_n(\theta))$ tel que

$$F_{T_n}(\beta_n)-F_{T_n}(\alpha_n)=1-\alpha$$
\noindent ou, de façon équivalente

$$\mathbb{P}(T_n<\alpha_n)+\mathbb{P}(T_n>\beta_n)=\alpha$$
::::

\noindent
**Remarque.** La dernière égalité s'interprète comme un *risque* à répartir entre $\mathbb{P}(T_n<\alpha_n)$ et $\mathbb{P}(T_n>\alpha_n)$.


\noindent Une première approche pour construire des intervalles de confiance consiste à utiliser, lorsque cela est possible, l'inégalité de Bienaymé-Tchebychev :

:::: {.methbox .meth data-latex="important"}
<center>**Construction d'intervalles de confiance par application de l'inégalité de Bienaymé-Tchebychev**</center>

\vspace{0.5cm}

\noindent  Soit $\widehat{\theta}_n$ un estimateur sans biais de $\theta$ et admettant une variance $\sigma^2$ que l'on suppose connue. On peut donc appliquer l'inégalité de Bienaymé-Tchebychev :

$$\mathbb{P}\left(|\widehat{\theta}_n-\theta|\geq\varepsilon\right)\leq\frac{\sigma^2}{\varepsilon^2}$$

\noindent soit

$$\mathbb{P}\left(|\widehat{\theta}_n-\theta|<\varepsilon \right)>1-\frac{\sigma^2}{\varepsilon^2}$$

\noindent On choisit $\varepsilon$ de façon à avoir $\alpha=\frac{\sigma^2}{\varepsilon^2}$, i.e. on pose

$$\varepsilon=\frac{\sigma}{\sqrt{\alpha}}$$
\noindent Par inversion des inégalités on a $|\widehat{\theta}_n-\theta|<\varepsilon\Leftrightarrow\widehat{\theta}_n-\varepsilon<\theta<\widehat{\theta}_n+\varepsilon$. On en déduit un intervalle de confiance de $\theta$ au niveau de confiance $1-\alpha$ :

$$IC^{1-\alpha}_m=\left[\widehat{\theta}_n-\frac{\sigma}{\sqrt{\alpha}}\,;\,\widehat{\theta}_n+\frac{\sigma}{\sqrt{\alpha}}\right]$$

\noindent Cet intervalle de confiance est bien calculable en pratique puisqu'on a supposé $\sigma^2$ connue.

\noindent 
**Exemple (moyenne empirique).** Soient $X$ une VA admettant une espérance $m$ et une variance $\sigma^2$, et $(X_1,\dots, X_n)$ des VA i.i.d. de même loi que $X$. La moyenne empirique $\widehat{\theta}_n\equiv\overline{X}_n$ est un estimateur sans biais de $\theta\equiv m$ et admettant comme variance $\frac{\sigma^2}{n}$, on peut donc appliquer ce qui précède et obtenir un intervalle de confiance de $m$ au niveau de confiance $1-\alpha$ :

$$IC^{1-\alpha}_m=\left[\overline{X}_n-\frac{\sigma}{\sqrt{n\alpha}}\,;\,\overline{X}_n+\frac{\sigma}{\sqrt{n\alpha}}\right]$$
::::


\noindent Pour le concours d'administrateur, il est précisé que la construction d'intervalle de confiances est abordée dans un contexte d'application du théorème central limite (*Construction d’un intervalle de confiance dans le cadre des modèles d’échantillonnage, dans le cas où le théorème central limite s’applique.*). En d'autres termes, il s'agit de se ramener - si l'on n'y est pas déjà - au cas d'une loi normale et d'en déduire un intervalle de confiance (asymptotique).

\noindent On commence par considérer le cas où la statistique $T_n$ est gaussienne.

:::: {.methbox .meth data-latex="important"}
<center>**Construction d'intervalles de confiance dans le cas gaussien**</center>

\noindent Supposons que $X$ suive une loi normale :

$$X\sim\mathcal{N}(m,\sigma^2)$$

\noindent
**Exemple : estimation de $m$ lorsque $\sigma$ est connu.** $m$ est l'espérance de $X$, on peut l'estimer par la moyenne empirique

$$\overline{X}_n\sim\mathcal{N}\left(m\,;\,\frac{\sigma^2}{n}\right)$$

- on commence par centrer et réduire $\overline{X}_n$ pour se ramener à une loi normale standard $\mathcal{N}(0,1)$. On pose donc 

$$Z_n\equiv\frac{\overline{X}_n-m}{\frac{\sigma}{\sqrt{n}}}$$

- on cherche ensuite un intervalle $[-v,v]$ de niveau de confiance $1-\alpha$ pour $Z_n$. Comme $Z_n$ est symétrique par rapport à $0$, il est plus simple de le chercher sous la forme $[-v,v]$. On résout donc, en notant $\Phi$ la fonction de répartition d'un loi normale standard et en remarquant que $\Phi(-x)=1-\Phi(x)$ :

\begin{align}
\Phi(v)-\Phi(-v) &= 1-\alpha  \\
2\Phi(v)-1 &= 1-\alpha \\
\Phi(v) &= 1-\frac{\alpha}{2} \\
v &= \Phi^{-1}\left(1-\frac{\alpha}{2}\right)
\end{align}

- on en déduit un intervalle de confiance de niveau $1-\alpha$ pour $m$ :

\begin{align}
& -v\leq Z_n \leq v  \\
\text{ssi } & -v\leq\frac{\overline{X}_n-m}{\frac{\sigma}{\sqrt{n}}}\leq n \\
\text{ssi } & \overline{X}_n-\frac{\sigma}{\sqrt{n}}v\leq m\leq \overline{X}_n+\frac{\sigma}{\sqrt{n}}v & \\
\end{align}

\noindent Un intervalle de confiance de $m$ au niveau $1-\alpha$ est donc

$$\text{IC}^m_{1-\alpha}\equiv\left[\overline{X}_n-\frac{\sigma}{\sqrt{n}}\Phi^{-1}\left(1-\frac{\alpha}{2}\right) \, , \,  \overline{X}_n+\frac{\sigma}{\sqrt{n}}\Phi^{-1}\left(1-\frac{\alpha}{2}\right)\right]$$
\noindent On peut calculer cet intervalle de confiance à partir de l'échantillon pusiqu'on a supposé $\sigma$ connu.
::::

\noindent On considère maintenant un cas où la stastistique n'est plus gaussienne, mais où il est possible d'appliquer le théorème central-limite, et donc se ramener à une loi approximativement gaussienne. Dans ce cas, peut obtenir des **intervalles de confiance asymptotiques** :


:::: {.methbox .meth data-latex="important"}
<center>**Construction d'intervalles de confiance asymptotiques dans un cas d'application du TCL**</center>

\noindent On suppose que $X$ suit une loi normale centrée :

$$X_n\sim\mathcal{N}(0,\sigma^2)$$

\noindent
**Exemple : estimation de $\sigma$.** On considère la statistique 

$$D_n\equiv\frac{1}{n}\sum\limits_{i=1}^n |X_i|$$

\noindent On vérifie facilement que l'intégrale $\int_{\mathbb{R}} |x|\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}\,dx$ est convergente et vaut $\sqrt{\frac{2}{\pi}}\sigma$. Ainsi, la variable aléatoire $|X|$ admet une espérance et 

$$\mathbb{E}(|X|)=\sqrt{\frac{2}{\pi}}\,\sigma$$
\noindent On en déduit un estimateur sans biais de $\sigma$ :

$$T_n\equiv\sqrt{\frac{\pi}{2}}\,D_n$$
\noindent Avec la loi faible des grands nombres, $T_n$ est un estimateur convergent de $\sigma$. Par ailleurs, $|X|$ admet un moment d'ordre $2$ ($\mathbb{E}(|X|^2)=\mathbb{E}(X^2)=\sigma^2$) et 

\begin{align}
\mathbb{V}(|X|) &= \mathbb{E}(X^2)-(\mathbb{E}(|X|))^2 \\
&= \left(1-\frac{2}{\pi}\right)\sigma^2
\end{align}

\noindent On en déduit que $T_n$ admet une variance et que 

$$\mathbb{V}(T_n)=\left(\frac{\pi}{2}-1\right)\frac{\sigma^2}{n}$$
\noindent Avec le théorème central limite on a l'approximation en loi

$$\frac{T_n-\sigma}{\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\pi}{2}-1}}\approx\mathcal{N}(0,1)$$
\noindent En posant $u_{\alpha}=\Phi^{-1}\left(1-\frac{\alpha}{2}\right)$, avec toujours $\Phi$ la fonction de répartition de la loi normale standard, on a donc

$$\mathbb{P}\left(-u_{\alpha}\leq\sqrt{n}\frac{T_n-\sigma}{\sigma\sqrt{\frac{\pi}{2}-1}}\leq u_{\alpha}\right)\approx 1-\alpha$$
\noindent Or

$$-u_{\alpha}\leq\sqrt{n}\frac{T_n-\sigma}{\sigma\sqrt{\frac{\pi}{2}-1}}\leq u_{\alpha}\Leftrightarrow \frac{T_n}{1+\frac{u_{\alpha}}{\sqrt{n}}\sqrt{\frac{\pi}{2}-1}}\leq\sigma\leq\frac{T_n}{1-\frac{u_{\alpha}}{\sqrt{n}}\sqrt{\frac{\pi}{2}-1}}$$
\noindent On en déduit un intervalle de confiance asymptotique de $\sigma$ au niveau de confiance $1-\alpha$ :

$$\text{IC}^{1-\alpha}_{\sigma}\equiv\left[\frac{T_n}{1+\frac{u_{\alpha}}{\sqrt{n}}\sqrt{\frac{\pi}{2}-1}}\, ;\, \frac{T_n}{1-\frac{u_{\alpha}}{\sqrt{n}}\sqrt{\frac{\pi}{2}-1}}\right]$$
:::


## Tests statistiques

### Définition et principes

\noindent On s'intéresse à un phénomène réel que l'on modélise par une variable aléatoire $X$ dont la loi est modélisée statistiquement par une famille paramétrique de distributions $\{P_{\theta}, \theta\in\Theta\}$. On suppose que l'espace $\Theta$ de tous les paramètres possibles est partitionné en deux sous-ensembles $\Theta_0$ et $\Theta_1$. On note $\theta$ la vraie valeur associée à la loi de $X$. On considère alors les deux hypothèses suivantes :

$$(H_0):\,\theta\in\Theta_0$$
$$(H_1):\,\theta\in\Theta_1$$

\noindent L'hypothèse $(H_0)$ s'appelle l'**hypothèse nulle** alors que l'hypothèse $(H_1)$ s'appelle l'**hypothèse alternative**.

\noindent Construire un test revient à construire une partition de $\mathbb{R}^n$ :

$$\mathbb{R}^n=W\cup\overline{W}$$
\noindent tel que, pour toute réalisation $(x_1,\dots,x_n)$ d'un échantillon aléatoire $(X_1,\dots,X_n)$ de variables aléatoires i.i.d. $X_i$ de loi $P_{\theta}$, on suive la **règle de décision** suivante :

$$\hbox{Si } (x_1,\dots, x_n)\in W \hbox{ alors on rejette l'hypothèse nulle } (H_0)$$
$$\hbox{Si } (x_1,\dots, x_n)\in \overline{W} \hbox{ alors on ne rejette pas l'hypothèse nulle } (H_0)$$
\noindent La partie $W$ s'appelle la **région de rejet** de $(H_0)$, ou encore la **région critique**. La partie $\overline{W}$ s'appelle la **région d'acceptation** de l'hypothèse nulle $(H_0)$.

\noindent Construire un test, c'est donc décider d'une partition particulière $(W, \overline{W})$ de $\mathbb{R}^n$. 

\noindent Les tests constituent un outil d'aide à la décision. On en présente ici une méthodologie générale :

:::: {.methbox .meth data-latex="important"}
<center>**Méthodologie des tests d'hypothèse**</center>
\noindent On suit généralement les étapes suivantes :

- on définit l'hypothèse nulle $(H_0)$ et l'hypothèse alternative $(H_1)$ ;
- on choisit une **statistique de test** $T=T(X_1,\dots, X_n)$ ;
- on détermine la distribution de $T$ sous l'hypothèse nulle $(H_0)$ ;
- on choisit un **niveau de significativité** $\alpha$ du test, et on calcule, à partir de la distibution de $T$ obtenue à l'étape précédente, la **région de rejet** de $(H_0)$ ;
- on calcule, à partir des données observées $(x_1,\dots x_n)$ (qui constituent une réalisation de l'échantillon aléatoire $(X_1,\dots, X_n)$) la valeur $T(x_1,\dots, x_n)$ prise par $T$ ;
- à partir de cette dernière valeur et de la région de rejet, on prend une décision.
::::

\noindent Cette méthodologie correspond en gros à un **raisonnement par l'absurde probabiliste**. En effet, dans un raisonnement par l'absurde classique :

- on veut démontrer une certaine affirmation $A$ ;
- sous l'hypothèse contraire $\overline{A}$, on en déduit quelque chose que l'on sait faux ;
- on conclut que notre hypothèse de départ $\overline{A}$ est fausse, i.e. on rejette $\overline{A}$, ou, de façon équivalente, on conclut que $A$ est vraie.


Lorsqu'on fait un test :

- on définit une hypothèse nulle $(H_0)$, qui est l'hypothèse que l'on aimerait pouvoir rejeter ;
- on calcule la distribution de $T$ et on calcule par ailleurs la valeur $T(x_1,\dots, x_n)$ ;
- si la probabilité **sous l'hypothèse nulle** que $T$ prenne cette valeur est très faible, i.e. en dessous d'un certain seuil (le seuil standard étant 5 %), on rejette l'hypothèse nulle. Si en revanche cette probabilité est au-dessus de ce seuil, on ne rejette pas l'hypothèse nulle.

\noindent 
**Remarques. i** En résumé, on suit donc le principe suivant : je suppose $(H_0)$ vraie, j'arrive à un résultat très improbable, je conclus que $(H_0)$ est fausse.

\noindent 
**ii.** A la différence d'un raisonnement par l'absurde classique, cette conclusion peut toutefois être erronée. Autrement dit, il est possible de rejeter à tort $(H_0)$. Toutefois, ce risque de rejet à tort est maitrisable : il s'agit du seuil de significativité $\alpha=\mathbb{P}(\hbox{on rejette } H_0\,|\, H_0)$. Donc, si on choisit $\alpha$ petit, ce risque -appelé **risque de première espèce**- sera très limité.

\noindent
**iii.** Lorsqu'on tente de faire un raisonnement par l'absurde, mais qu'on ne parvient pas à aboutir à une contradiction, il est incorrect de conclure que l'hypothèse initiale est vraie. Notre raisonnement ne nous a juste pas permis de conclure qu'elle était fausse... ce qui ne signifie pas qu'elle est vraie. Dans un tel cas, on ne peut tout simplement rien conclure. De manière analogue, si la probabilité calculée dans un test est au-dessus du seuil de significativité, il serait incorrect d'*accepter* l'hypothèse nulle. Même si on trouve parfois l'expression *accepter $(H_0)$* dans la littérature, il s'agit d'un abus de langage et on lui préfèrera l'expression *ne pas rejeter $(H_0)$*. Cela ne signifie pas que $(H_1)$ est probablement vraie, mais plutôt que notre test ne nous a pas permis de conclure que $(H_0)$ était probablement fausse, ce qui est assez différent.

\noindent
**iv.** De la même façon que l'on peut rejeter à tort l'hypothèse nulle $(H_0)$, il est possible de ne pas rejeter à tort $(H_0)$. La probabilité d'un tel événement est $\mathbb{P}(\hbox{ne pas rejeter } H_0\,|\, H_1)$. Cette probabilité s'appelle le **risque de deuxième espèce**.

\noindent 
**v.** Tout test présente une dyssymétrie dans sa façon de traiter $(H_0)$ et $(H_1)$. Parmi les deux types d'erreurs que l'on peut commettre dans la conclusion d'un test, il y en a généralement l'une des deux que l'on veut éviter en priorité. Par exemple, un test médical peut amener deux types d'erreurs : les faux positifs et les faux négatifs. On préfère en général avoir des tests pour lesquels la probabilité de faux négatif est faible, car conclure que le patient n'est pas malade (et donc ne pas le traiter) alors qu'il l'est est en général plus ennuyeux que conclure qu'il est malade (et donc le traiter) alors qu'il ne l'est pas. On retient le plus souvent la convention suivante : l'erreur de première espèce est celle que l'on veut éviter, on veut donc maitriser le risque de première espèce $\alpha=\mathbb{P}(\hbox{rejeter } H_0\,|\,H_0)$. On choisit un $\alpha$ petit et on en déduit une région de rejet $W=W_{\alpha}$. La valeur $\beta=\mathbb{P}(\hbox{ne pas rejeter } H_0\,|\, H_1)$ dépend alors du choix de $\alpha$, on ne la contrôle pas. 


### Tests unilatéraux, tests bilatéraux

\noindent On note $\theta$ le paramètre inconnu associé à la loi de $X$.

:::: {.defbox .def data-latex="important"}
<center>**Test bilatéral**</center>

\noindent Un **test bilatéral** est un test dont les hypothèses nulle $(H_0)$ et alternative $(H_1)$ sont de la forme

$$(H_0):\,\theta=\theta_0$$
$$(H_1):\,\theta\neq\theta_0$$
::::

\noindent
**Exemple : tester si une pièce est équilibrée.** On veut s'assurer qu'une pièce est équilibrée en utilisant un test statistique.

On note $p$ sa probabilité de tomber sur pile. La pièce est équilibrée si et seulement si $p=\frac{1}{2}$.

On définit alors le test suivant :

$$(H_0):\,p=\frac{1}{2}$$
$$(H_1):\, p\neq\frac{1}{2}$$
\noindent Il s'agit d'un test bilatéral.


\vspace{0.5cm}

\noindent De même, il existe des tests unilatéraux :

:::: {.defbox .def data-latex="important"}
<center>**Test unilatéral**</center>

\noindent Un **test unilatéral** est un test dont les hypothèses nulle $(H_0)$ et alternative $(H_1)$ sont soit de la forme

$$(H_0):\,\theta=\theta_0$$
$$(H_1):\,\theta>\theta_0$$

\noindent soit de la forme

$$(H_0):\,\theta=\theta_0$$
$$(H_1):\,\theta<\theta_0$$
::::

\noindent Un test unilatéral suppose donc connu le signe de la différence $\theta-\theta_0$, contrairement à un test bilatéral.

\noindent

**Exemple : tester l'efficacité d'un médicament.** Le fabricant d'un médicament annonce une efficacité à $90/,/%$ pour l'un de ses produits. Sur un échantillon de 200 personnes, ce médicament a fonctionné pour 160 personnes. On souhaite déterminer si l'information du fabrication est exacte au seuil de significativité $\alpha=1\,\%$.

\noindent On va ici prendre pour hypothèses

$$(H_0):\,p=0,9$$

$$(H_1):\,p<0,9$$

\noindent où $p$ est la probabilité que le médicament soit efficace.

\noindent Il s'agit d'un test unilatéral gauche.


### Exemples

\noindent On reprend les deux exemples précédents.

::: {.methbox .meth data-latex="important"}
<center>**Exemple 1 : tester si une pièce est équilibrée.**</center>

\vspace{0.5cm}

\noindent
On effectue $n=10\,000$ lancers de cette pièce. Sur ces $10\,000$ lancers, la pièce est tombée $4\,880$ fois sur pile.

**On veut déterminer si cette pièce est équilirée au seuil de significativité $\alpha = 5\,\%$.**

\noindent Pour $1\leq i\leq n$, on note $X_i\in\{0,1\}$ la variable aléatoire qui prend la valeur $1$ si la pièce est tombée sur pile au lancer numéro $i$, qui prend la valeur $0$ si elle est tombée sur face. Les variables aléatoires $X_1,\dots,X_n$ sont i.i.d. de loi de Bernoulli $\mathcal{B}(p)$ où $p$ est la probabilité que la pièce tombe sur pile : $p=\mathbb{P}(X_i=1)$. La variable aléatoire $S_n\equiv\sum\limits_{i=1}^n X_i$ suit une loi binomiale $\mathcal{B}(n, p)$.

\noindent On rappelle l'énoncé du **théorème de Moivre-Laplace**, qui n'est qu'un cas particulier du théorème central limite :


:::: {.thmbox .thm data-latex="important"}
**Théorème de Moivre-Laplace.** Si la variable aléatoire $X_n$ suit une loi binomiale 

$$X_n\sim\mathcal{B}(n, \,p)$$

\noindent alors la variable aléatoire $$Z_n\equiv\frac{X_n-np}{\sqrt{np(1-p)}}$$
\noindent converge en loi vers la loi normale standard $\mathcal{N}(0,1)$.

\noindent En pratique, dès lors que les conditions suivantes sont vérifiées :

- $n\geq 30$
- $np\geq 5$
- $n(1-p)\geq 5$

\noindent on peut écrire l'approximation en loi

$$Z_n\equiv\frac{X_n-np}{\sqrt{np(1-p)}}\approx\mathcal{N}(0,1)$$
::::

\noindent Sous l'hypothèse nulle $(H_0):\,p=\frac{1}{2}$, on a 

$$S_n\sim\mathcal{B}\left(n, \frac{1}{2}\right)$$
\noindent Les trois conditions à vérifier sont bien satisfaites :

- $n=10\,000\geq 30$
- $np=5\,000\geq 5$
- $n(1-p)=5\,000\geq 5$

si bien que

$$Z_n\equiv\frac{S_n-np}{\sqrt{np(1-p)}}\approx\mathcal{N}(0,1)$$
\noindent i.e.

$$Z_n\equiv\frac{S_n-5\,000}{50}\approx\mathcal{N}(0,1)$$
La variable aléatoire $Z_n$ est notre **statistique de test**. On vient de déterminer sa loi, on peut donc en déduire la **zone de rejet** de notre test. Pour une loi normale standard $Z_n$, on a 

$$\mathbb{P}\left(-1,96\leq Z_n\leq 1,96\right)\approx 0,95$$

\noindent La **région d'acceptation** du test au seuil de significativité $\alpha=5\,\%$ est donc $\overline{W}=[-1,96\,;\,1,96]$ et son complémentaire $W=]-\infty\;\,-1,96[\cup]1,96\,;\,+\infty[$ est donc la **région de rejet**.

\noindent On applique donc la **règle de décision** suivante :

$$\hbox{Si } z_n\not\in \overline{W}=[-1,96\,;\,1,96] \hbox{ on rejette l'hypothèse nulle} (H_0)$$
$$\hbox{Si } z_n\in\overline{W}=[-1,96\,;\,1,96] \hbox{ on ne rejette pas l'hypothèse nulle} (H_0)$$
\noindent Or, la valeur observée de $Z_n$ sur l'échantillon tiré est $z_n=\frac{4\,880-5\,000}{50}=-2,4$. Elle appartient à la région de rejet. 

\noindent
**Conclusion. Au seuil de significativité $5\,\%$ on rejette donc l'hypothèse nulle, ce qui revient à dire qu'on conclut que la pièce est désiquilibrée.**

\noindent 
**Remarque.** Si on avait obtenu $4\,920$ fois pile sur nos $5\,000$ tirages, on aurait calculé $z_n=-1,6$ qui est dans la zone d'acceptation. On n'aurait donc pas rejeté l'hypothèse nulle, autrement dit on n'aurait pas rejeté l'hypothèse d'une pièce équilibrée.
:::

::: {.methbox .meth data-latex="important"}
<center>**Exemple 2 : tester l'efficacité d'un médicament.**</center>

\vspace{0.5cm}

\noindent Pour $i$ compris entre $1$ et $200$, on pose $X_i=1$ si le médicament a été efficace pour l'individu numéro $i$, et $X_i=0$ dans le cas contraire.

Comme dans l'exemple 1, sous l'hypothèse nulle $(H_0):\,p=0,9$ les $X_i$ sont i.i.d. et suivent une loi de Benoulli, de paramètre $p$. On en déduit que leur somme $S_n=\sum\limits_{i=1}^{200}X_i$ suit une loi binomiale de paramètre $np=180$. On a 

- $n=200\geq 30$
- $np=180\geq 5$
- $n(1-p)=20\geq 5$

\noindent On a donc 

$$Z_n\equiv\frac{S_n-180}{\sqrt{18}}\approx\mathcal{N}(0,1)$$
\noindent Il s'agit de notre statistique de test. 
Sur l'échantillon observé, elle prend comme valeur

$$z_n=\frac{160-180}{\sqrt{0.5}}\approx -4,71$$
\noindent La région de rejet est $W=]-\infty, t]$ avec $t$ l'unique réel tel que $\mathbb{P}(Z_n\leq t)=0,01$. Grâce à la table de la loi normale standard, on obtient $t\approx -2,33$, donc  $W=]-\infty\,;\,-2,33]$. Ainsi, $z_n$ est dans la région de rejet. On rejette donc l'hypothèse nulle : l'affirmation du fabricant est fausse au seuil de significativité $1\,\%$.
:::




