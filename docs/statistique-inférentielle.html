<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 7 Statistique inférentielle | Cours de probabilités-statistiques pour le concours interne d’administrateur Insee</title>
  <meta name="description" content="Cours de probabilités et statistiques" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 7 Statistique inférentielle | Cours de probabilités-statistiques pour le concours interne d’administrateur Insee" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Cours de probabilités et statistiques" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 7 Statistique inférentielle | Cours de probabilités-statistiques pour le concours interne d’administrateur Insee" />
  
  <meta name="twitter:description" content="Cours de probabilités et statistiques" />
  

<meta name="author" content="Olivier Guin" />


<meta name="date" content="2023-10-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistique-descriptive.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Cours de probabilités et statistique pour le concours interne d'administrateur de l'Insee</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Présentation du cours</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#généralités"><i class="fa fa-check"></i><b>1.1</b> Généralités</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#coquilles-et-erreurs"><i class="fa fa-check"></i><b>1.2</b> Coquilles et erreurs</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html"><i class="fa fa-check"></i><b>2</b> Dénombrement et probabilités</a>
<ul>
<li class="chapter" data-level="2.1" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#dénombrement"><i class="fa fa-check"></i><b>2.1</b> Dénombrement</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#permutations"><i class="fa fa-check"></i><b>2.1.1</b> Permutations</a></li>
<li class="chapter" data-level="2.1.2" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#arrangements"><i class="fa fa-check"></i><b>2.1.2</b> Arrangements</a></li>
<li class="chapter" data-level="2.1.3" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#combinaisons"><i class="fa fa-check"></i><b>2.1.3</b> Combinaisons</a></li>
<li class="chapter" data-level="2.1.4" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#formule-du-binôme-de-newton"><i class="fa fa-check"></i><b>2.1.4</b> Formule du binôme de Newton</a></li>
<li class="chapter" data-level="2.1.5" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#triangle-de-pascal"><i class="fa fa-check"></i><b>2.1.5</b> Triangle de Pascal</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#evénements-et-probabilités"><i class="fa fa-check"></i><b>2.2</b> Evénements et probabilités</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#evénements"><i class="fa fa-check"></i><b>2.2.1</b> Evénements</a></li>
<li class="chapter" data-level="2.2.2" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#probabilités"><i class="fa fa-check"></i><b>2.2.2</b> Probabilités</a></li>
<li class="chapter" data-level="2.2.3" data-path="dénombrement-et-probabilités.html"><a href="dénombrement-et-probabilités.html#probabilités-conditionnelles"><i class="fa fa-check"></i><b>2.2.3</b> Probabilités conditionnelles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html"><i class="fa fa-check"></i><b>3</b> Variables aléatoires discrètes</a>
<ul>
<li class="chapter" data-level="3.1" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#définition-et-premières-propriétés"><i class="fa fa-check"></i><b>3.1</b> Définition et premières propriétés</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#définition"><i class="fa fa-check"></i><b>3.1.1</b> Définition</a></li>
<li class="chapter" data-level="3.1.2" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#loi-dune-variable-aléatoire-discrète"><i class="fa fa-check"></i><b>3.1.2</b> Loi d’une variable aléatoire discrète</a></li>
<li class="chapter" data-level="3.1.3" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#définition-de-pxin-a"><i class="fa fa-check"></i><b>3.1.3</b> Définition de <span class="math inline">\(p(X\in A)\)</span></a></li>
<li class="chapter" data-level="3.1.4" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#fonction-de-répartition-quantiles"><i class="fa fa-check"></i><b>3.1.4</b> Fonction de répartition, quantiles</a></li>
<li class="chapter" data-level="3.1.5" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#exemples-classiques"><i class="fa fa-check"></i><b>3.1.5</b> Exemples classiques :</a></li>
<li class="chapter" data-level="3.1.6" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#moments-dune-variable-aléatoire"><i class="fa fa-check"></i><b>3.1.6</b> Moments d’une variable aléatoire</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#transformation-dune-variable-aléatoire-discrète"><i class="fa fa-check"></i><b>3.2</b> Transformation d’une variable aléatoire discrète</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#définition-1"><i class="fa fa-check"></i><b>3.2.1</b> Définition</a></li>
<li class="chapter" data-level="3.2.2" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#théorème-de-transfert-cas-discret"><i class="fa fa-check"></i><b>3.2.2</b> Théorème de transfert (cas discret)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#vecteurs-aléatoires"><i class="fa fa-check"></i><b>3.3</b> Vecteurs aléatoires</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#définition-2"><i class="fa fa-check"></i><b>3.3.1</b> Définition</a></li>
<li class="chapter" data-level="3.3.2" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#loi-jointe-loi-marginales"><i class="fa fa-check"></i><b>3.3.2</b> Loi jointe, loi marginales</a></li>
<li class="chapter" data-level="3.3.3" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#loi-conditionnelle-mathcallyx"><i class="fa fa-check"></i><b>3.3.3</b> Loi conditionnelle <span class="math inline">\(\mathcal{L}(Y|X)\)</span></a></li>
<li class="chapter" data-level="3.3.4" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#loi-dun-couple-de-va-indépendantes"><i class="fa fa-check"></i><b>3.3.4</b> Loi d’un couple de VA indépendantes</a></li>
<li class="chapter" data-level="3.3.5" data-path="variables-aléatoires-discrètes.html"><a href="variables-aléatoires-discrètes.html#espérance-conditionnelle-variance-conditionnelle"><i class="fa fa-check"></i><b>3.3.5</b> Espérance conditionnelle, variance conditionnelle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html"><i class="fa fa-check"></i><b>4</b> Variables aléatoires à densité</a>
<ul>
<li class="chapter" data-level="4.1" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#définition-et-premières-propriétés-1"><i class="fa fa-check"></i><b>4.1</b> Définition et premières propriétés</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#définition-3"><i class="fa fa-check"></i><b>4.1.1</b> Définition</a></li>
<li class="chapter" data-level="4.1.2" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#loi-dune-variable-aléatoire-à-densité"><i class="fa fa-check"></i><b>4.1.2</b> Loi d’une variable aléatoire à densité</a></li>
<li class="chapter" data-level="4.1.3" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#définition-de-pxin-a-1"><i class="fa fa-check"></i><b>4.1.3</b> Définition de <span class="math inline">\(p(X\in A)\)</span></a></li>
<li class="chapter" data-level="4.1.4" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#fonction-de-répartition-quantiles-1"><i class="fa fa-check"></i><b>4.1.4</b> Fonction de répartition, quantiles</a></li>
<li class="chapter" data-level="4.1.5" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#exemples-classiques-1"><i class="fa fa-check"></i><b>4.1.5</b> Exemples classiques :</a></li>
<li class="chapter" data-level="4.1.6" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#moments-dune-variable-aléatoire-1"><i class="fa fa-check"></i><b>4.1.6</b> Moments d’une variable aléatoire</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#transformation-dune-variable-aléatoire-à-densité"><i class="fa fa-check"></i><b>4.2</b> Transformation d’une variable aléatoire à densité</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#définition-4"><i class="fa fa-check"></i><b>4.2.1</b> Définition</a></li>
<li class="chapter" data-level="4.2.2" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#théorème-de-transfert-cas-à-densité"><i class="fa fa-check"></i><b>4.2.2</b> Théorème de transfert (cas à densité)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#vecteurs-aléatoires-1"><i class="fa fa-check"></i><b>4.3</b> Vecteurs aléatoires</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#définition-5"><i class="fa fa-check"></i><b>4.3.1</b> Définition</a></li>
<li class="chapter" data-level="4.3.2" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#loi-jointe-loi-marginales-1"><i class="fa fa-check"></i><b>4.3.2</b> Loi jointe, loi marginales</a></li>
<li class="chapter" data-level="4.3.3" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#loi-conditionnelle-mathcallyx-1"><i class="fa fa-check"></i><b>4.3.3</b> Loi conditionnelle <span class="math inline">\(\mathcal{L}(Y|X)\)</span></a></li>
<li class="chapter" data-level="4.3.4" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#loi-dun-couple-de-va-indépendantes-1"><i class="fa fa-check"></i><b>4.3.4</b> Loi d’un couple de VA indépendantes</a></li>
<li class="chapter" data-level="4.3.5" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#espérance-matrice-de-variance-covariance"><i class="fa fa-check"></i><b>4.3.5</b> Espérance, matrice de variance-covariance</a></li>
<li class="chapter" data-level="4.3.6" data-path="variables-aléatoires-à-densité.html"><a href="variables-aléatoires-à-densité.html#espérance-conditionnelle-variance-conditionnelle-1"><i class="fa fa-check"></i><b>4.3.6</b> Espérance conditionnelle, variance conditionnelle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="convergence.html"><a href="convergence.html"><i class="fa fa-check"></i><b>5</b> Convergence</a>
<ul>
<li class="chapter" data-level="5.1" data-path="convergence.html"><a href="convergence.html#différents-modes-de-convergence"><i class="fa fa-check"></i><b>5.1</b> Différents modes de convergence</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="convergence.html"><a href="convergence.html#convergence-en-probabilité"><i class="fa fa-check"></i><b>5.1.1</b> Convergence en probabilité</a></li>
<li class="chapter" data-level="5.1.2" data-path="convergence.html"><a href="convergence.html#convergence-dans-les-espaces-lp"><i class="fa fa-check"></i><b>5.1.2</b> Convergence dans les espaces <span class="math inline">\(L^p\)</span></a></li>
<li class="chapter" data-level="5.1.3" data-path="convergence.html"><a href="convergence.html#convergence-en-loi"><i class="fa fa-check"></i><b>5.1.3</b> Convergence en loi</a></li>
<li class="chapter" data-level="5.1.4" data-path="convergence.html"><a href="convergence.html#convergence-presque-sûre-hors-prgramme"><i class="fa fa-check"></i><b>5.1.4</b> Convergence presque-sûre (hors-prgramme ?)</a></li>
<li class="chapter" data-level="5.1.5" data-path="convergence.html"><a href="convergence.html#liens-entre-les-différents-modes-de-convergence"><i class="fa fa-check"></i><b>5.1.5</b> Liens entre les différents modes de convergence</a></li>
<li class="chapter" data-level="5.1.6" data-path="convergence.html"><a href="convergence.html#approximations"><i class="fa fa-check"></i><b>5.1.6</b> Approximations</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="convergence.html"><a href="convergence.html#loi-faible-des-grands-nombres-loi-forte-des-grands-nombres"><i class="fa fa-check"></i><b>5.2</b> Loi Faible des Grands Nombres, Loi Forte des Grands Nombres</a></li>
<li class="chapter" data-level="5.3" data-path="convergence.html"><a href="convergence.html#théorème-central-limite-tcl"><i class="fa fa-check"></i><b>5.3</b> Théorème Central Limite (TCL)</a></li>
<li class="chapter" data-level="5.4" data-path="convergence.html"><a href="convergence.html#variantes-du-tcl-hors-programme"><i class="fa fa-check"></i><b>5.4</b> Variantes du TCL (hors-programme)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html"><i class="fa fa-check"></i><b>6</b> Statistique descriptive</a>
<ul>
<li class="chapter" data-level="6.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#vocabulaire"><i class="fa fa-check"></i><b>6.1</b> Vocabulaire</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#population-individus-échantillon"><i class="fa fa-check"></i><b>6.1.1</b> Population, individus, échantillon</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#analyse-statistique-univariée"><i class="fa fa-check"></i><b>6.2</b> Analyse statistique univariée</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#notion-de-série-statistique-univariée"><i class="fa fa-check"></i><b>6.2.1</b> Notion de série statistique univariée</a></li>
<li class="chapter" data-level="6.2.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#indicateurs-dune-série-statistique-univariée"><i class="fa fa-check"></i><b>6.2.2</b> Indicateurs d’une série statistique univariée</a></li>
<li class="chapter" data-level="6.2.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#représentations-graphiques"><i class="fa fa-check"></i><b>6.2.3</b> Représentations graphiques</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#analyse-statistique-bivariée"><i class="fa fa-check"></i><b>6.3</b> Analyse statistique bivariée</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#notion-de-série-statistique-bivariée"><i class="fa fa-check"></i><b>6.3.1</b> Notion de série statistique bivariée</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#indicateurs-propres-à-lanalyse-statistique-multivariée"><i class="fa fa-check"></i><b>6.3.2</b> Indicateurs propres à l’analyse statistique multivariée</a></li>
<li class="chapter" data-level="6.3.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#nuage-de-points"><i class="fa fa-check"></i><b>6.3.3</b> Nuage de points</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#ajustement-des-moindres-carrés"><i class="fa fa-check"></i><b>6.3.4</b> Ajustement des moindres carrés</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html"><i class="fa fa-check"></i><b>7</b> Statistique inférentielle</a>
<ul>
<li class="chapter" data-level="7.1" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#estimation"><i class="fa fa-check"></i><b>7.1</b> Estimation</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#premières-définitions"><i class="fa fa-check"></i><b>7.1.1</b> Premières définitions</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#convergence-dun-estimateur"><i class="fa fa-check"></i><b>7.1.2</b> Convergence d’un estimateur</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#exemples-classiques-2"><i class="fa fa-check"></i><b>7.1.3</b> Exemples classiques</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#méthodes-de-construction-des-estimateurs"><i class="fa fa-check"></i><b>7.1.4</b> Méthodes de construction des estimateurs</a></li>
<li class="chapter" data-level="7.1.5" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#compléments-hors-programme"><i class="fa fa-check"></i><b>7.1.5</b> Compléments (hors-programme)</a></li>
<li class="chapter" data-level="7.1.6" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#estimation-des-coefficients-dune-régression-linéaire"><i class="fa fa-check"></i><b>7.1.6</b> Estimation des coefficients d’une régression linéaire</a></li>
<li class="chapter" data-level="7.1.7" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#intervalles-de-confiance"><i class="fa fa-check"></i><b>7.1.7</b> Intervalles de confiance</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#tests-statistiques"><i class="fa fa-check"></i><b>7.2</b> Tests statistiques</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#définition-et-principes"><i class="fa fa-check"></i><b>7.2.1</b> Définition et principes</a></li>
<li class="chapter" data-level="7.2.2" data-path="statistique-inférentielle.html"><a href="statistique-inférentielle.html#exemples-de-test"><i class="fa fa-check"></i><b>7.2.2</b> Exemples de test</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Cours de probabilités-statistiques pour le concours interne d’administrateur Insee</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistique-inférentielle" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapitre 7</span> Statistique inférentielle<a href="statistique-inférentielle.html#statistique-inférentielle" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="estimation" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Estimation<a href="statistique-inférentielle.html#estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>On s’intéresse à une loi probabiliste <span class="math inline">\(\mathcal{L}_{\theta}\)</span>, qui est entièrement décrite par la donnée d’un paramètre inconnu <span class="math inline">\(\theta\)</span>. Pour mieux appréhender cette loi, il serait intéressant de connaître la valeur de <span class="math inline">\(\theta\)</span>. Plutôt que de chercher à déterminer la valeur exacte de <span class="math inline">\(\theta\)</span>, on peut essayer de l’approcher. Dans le cadre de la statistique inférentielle, on suppose qu’on dispose d’un <strong>échantillon i.i.d.</strong> de <span class="math inline">\(\mathcal{L}_{\theta}\)</span>, autrement dit d’un certain nombre de réalisations <span class="math inline">\((Y_1,\dots, Y_n)\)</span> indépendantes et identiquement distribuées de la loi <span class="math inline">\(\mathcal{L}_{\theta}\)</span>.</p>
<p>La donnée d’un tel échantillon constitue un ensemble d’<strong>informations</strong> qui vont nous être utiles pour <strong>estimer</strong> le paramètre <span class="math inline">\(\theta\)</span>. on fait donc bien ici de l’<em>inférence</em> - ou encore de l’<em>induction</em> - dans le sens où on part d’observations particulières (les réalisations <span class="math inline">\(Y_1,\dots, Y_n)\)</span> pour énoncer une règle générale (le fait que ces réalisations sont issues de la loi <span class="math inline">\(\mathcal{L}_{\theta}\)</span>).</p>
<div id="premières-définitions" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Premières définitions<a href="statistique-inférentielle.html#premières-définitions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="defbox def">
<center>
<strong>Estimateurs</strong>
</center>
<p>Soit <span class="math inline">\(Y\)</span> une variable aléatoire de loi <span class="math inline">\(\mathcal{L}(Y)\)</span>, paramétrée par un réel <span class="math inline">\(\theta\)</span> inconnu. Soit <span class="math inline">\((Y_1,\dots, Y_n)\)</span> un échantillon i.i.d. de loi <span class="math inline">\(\mathcal{L}(Y)\)</span>. On appelle <em>estimateur</em> de <span class="math inline">\(\theta\)</span> toute fonction de <span class="math inline">\(Y_1,\dots, Y_n\)</span>, i.e. <span class="math display">\[\widehat{\theta}_n=S(Y_1,\dots, Y_n)\]</span></p>
</div>
<p>On veut estimer la moyenne d’une loi normale <span class="math inline">\(\mathcal{N}(\mu\,;\,1)\)</span>, à partir d’un échantillon d’observations i.i.d. <span class="math inline">\((Y_1,\dots, Y_n)\)</span> tirées sous cette loi. Une façon naturelle d’estimer <span class="math inline">\(\mu=\mathbb{E}(Y_1)\)</span> est de poser <span class="math inline">\(\widehat{\mu}_n=\frac{Y_1+\dots Y_n}{n}\)</span>. Ici, on estime donc une moyenne théorique par sa contrepartie <em>empirique</em>.</p>
<div class="defbox def">
<center>
<strong>Biais, erreur quadratique</strong>
</center>
<p>Soit <span class="math inline">\(\widehat{\theta}_n\)</span> un estimateur de <span class="math inline">\(\theta\)</span> admettant un moment d’ordre <span class="math inline">\(1\)</span>.</p>
<ul>
<li><p>On appelle <strong>biais</strong> de <span class="math inline">\(\widehat{\theta}_n\)</span> la quantité <span class="math inline">\(b_{\theta}(\widehat{\theta}_n)=\mathbb{E}(\widehat{\theta}_n)-\theta\)</span>.</p></li>
<li><p>Un estimateur est dit <strong>sans biais</strong> lorsque son biais est nul, i.e. <span class="math inline">\(\mathbb{E}(\widehat{\theta}_n)=\theta\)</span>.</p></li>
<li><p>Il est dit <strong>asymptotiquement sans biais</strong> lorsque son biais tend vers <span class="math inline">\(0\)</span>, i.e. <span class="math inline">\(b_{\theta}(\widehat{\theta}_n)\underset{n\to +\infty}{\longrightarrow}0\)</span>.</p></li>
<li><p>Pour un estimateur des moments d’ordre <span class="math inline">\(1\)</span> et <span class="math inline">\(2\)</span>, on appelle <strong>erreur quadratique moyenne</strong> la quantité (positive) <span class="math inline">\(\text{EQM}_{\theta}(\widehat{\theta}_n)=\mathbb{E}\left(\left(\widehat{\theta}_n-\theta\right)^2\right)\)</span></p></li>
</ul>
</div>
<p>L’erreur quadratique moyenne s’écrit à l’aide de l’espérance et de la variance :</p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> Soit <span class="math inline">\(\widehat{\theta}_n\)</span> un estimateur de <span class="math inline">\(\theta\)</span> admettant des moments d’ordres <span class="math inline">\(1\)</span> et <span class="math inline">\(2\)</span>. Son erreur quadratique moyenne peut se décomposer en biais au carré/variance :</p>
<p><span class="math display">\[\text{EQM}_{\theta}(\widehat{\theta}_n)=b_{\theta}^2(\widehat{\theta}_n)+\mathbb{V}(\widehat{\theta}_n)\]</span></p>
</div>
<p>Autrement dit, réduire l’erreur (quadratique moyenne) d’un estimateur revient à essayer de réduire son biais et/ou sa variance. En pratique, uune réduction du biais implique souvent une augmentation de la variance (et vice-versa) et il faut trouver un compromis entre les deux, i.e. un estimateur pour lequel la combinaison (biais, variance) implique une faible erreur quadratique moyenne. On parle alors de <strong>compromis biais-variance</strong>.</p>
</div>
<div id="convergence-dun-estimateur" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Convergence d’un estimateur<a href="statistique-inférentielle.html#convergence-dun-estimateur" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="defbox def">
<center>
<strong>Estimateurs convergents</strong>
</center>
<p>Un estimateur <span class="math inline">\(\widehat{\theta}_n\)</span> de <span class="math inline">\(\theta\)</span> est dit <strong>convergent</strong> lorsqu’il converge en probabilité vers <span class="math inline">\(\theta\)</span> i.e. lorsque</p>
<p><span class="math display">\[\forall\varepsilon &gt;0, \mathbb{P}\left(|\widehat{\theta}_n-\theta|&gt;\varepsilon\right)\longrightarrow 0\]</span></p>
</div>
<p>La convergence d’un estimateur sans biais peut se montrer à l’aide du critère pratique suivant :</p>
<div class="thmbox thm">
<p>
<strong>Théorème (critère pratique de convergence) :</strong> Un estimateur <span class="math inline">\(\widehat{\theta}_n\)</span> sans biais de <span class="math inline">\(\theta\)</span> est <strong>convergent</strong> dès que sa variance tend vers <span class="math inline">\(0\)</span>, i.e. <span class="math display">\[\left(\mathbb{E}_{\theta}(\widehat{\theta}_n)=0 \text{ et } \mathbb{V}_{\theta}(\widehat{\theta}_n)\longrightarrow 0\right)\Rightarrow \left(\widehat{\theta}_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\theta\right)\]</span></p>
</div>
<p>
<strong>Démonstration.</strong> Compte-tenu du fait que <span class="math inline">\(\widehat{\theta}_n\)</span> est un estimateur sans biais pour <span class="math inline">\(\theta\)</span>, l’inégalité de Bieanymé-Tchebychev s’écrit <span class="math inline">\(\mathbb{P}(|\widehat{\theta}_n-\theta|&gt;\varepsilon)\leq\frac{\mathbb{V}_{\theta}(\widehat{\theta}_n)}{\varepsilon^2}\)</span>, ce qui permet de conclure. <span class="math inline">\(\square\)</span></p>
<p>On peut même affaiblir un peu l’hypothèse d’absence de biais par une hypothèse de biais asymptotiquement nul :</p>
<div class="thmbox thm">
<p>
<strong>Théorème (critère pratique de convergence (suite)) :</strong> Un estimateur <span class="math inline">\(\widehat{\theta}_n\)</span> asymtotiquement sans biais de <span class="math inline">\(\theta\)</span> est <strong>convergent</strong> dès que sa variance tend vers <span class="math inline">\(0\)</span>, i.e. <span class="math display">\[\left(\mathbb{E}_{\theta}(\widehat{\theta}_n)\underset{n\to +\infty}{\longrightarrow}\theta \text{ et } \mathbb{V}_{\theta}(\widehat{\theta}_n)\longrightarrow 0\right)\Rightarrow \left(\widehat{\theta}_n \underset{n \to +\infty}{\overset{\mathbb{P}}{\longrightarrow}}\theta\right)\]</span></p>
</div>
</div>
<div id="exemples-classiques-2" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Exemples classiques<a href="statistique-inférentielle.html#exemples-classiques-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Quelques exemples très classiques d’estimateurs :</p>
<p>
<strong>Exemple 1 : moyenne empirique.</strong> Soit <span class="math inline">\(X_1,\dots X_n\)</span> une suite de <span class="math inline">\(VAR\)</span> i.i.d. de même loi que <span class="math inline">\(X\)</span>, admettant une espérance <span class="math inline">\(\mu\)</span>. La <em>moyenne empirique</em> est l’estimateur <span class="math display">\[\overline{X_n}=\frac{X_1+\dots + X_n}{n}\]</span></p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> Quelle que soit la loi suivie par <span class="math inline">\(X\)</span>, la moyenne empirique <span class="math inline">\(\overline{X_n}\)</span> est un <strong>estimateur sans biais</strong> de l’espérance <span class="math inline">\(\mu=\mathbb{E}(X)\)</span>. Si, de plus, <span class="math inline">\(X\)</span> admet une variance <span class="math inline">\(\sigma^2\)</span>, alors <span class="math inline">\(\overline{X_n}\)</span> admet également une variance et celle-ci est donnée par <span class="math inline">\(\mathbb{V}(\overline{X_n})=\frac{\sigma^2}{n}\)</span>.</p>
</div>
<p>
<strong>Démonstration.</strong> Par linéarité de l’espérance : <span class="math inline">\(\mathbb{E}(\overline{X_n})=\frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i)=\frac{1}{n}\sum\limits_{i=1}^n\mu=\mu\)</span>. Si <span class="math inline">\(X\)</span> admet une variance, alors <span class="math inline">\(\overline{X_n}\)</span> aussi et <span class="math inline">\(\mathbb{V}(\overline{X_n})=\frac{1}{n^2}\sum\limits_{i=1}^n\mathbb{V}(X_i)=\frac{\sigma^2}{n}\)</span>, par indépendance de <span class="math inline">\(X_1,\dots X_n\)</span>. <span class="math inline">\(\square\)</span></p>
<div class="thmbox thm">
<p><strong>Corollaire :</strong> La moyenne empirique est un estimateur convergent de l’espérance (lorsqu’elle existe).</p>
</div>
<p>
<strong>Démonstration.</strong> L’estimateur <span class="math inline">\(\overline{X_n}\)</span> est sans biais et <span class="math inline">\(\mathbb{V}(\overline{X_n})=\frac{\sigma^2}{n}\longrightarrow 0\)</span>. C’est donc un estimateur convergent de l’espérance. <span class="math inline">\(\square\)</span>.</p>
<p>
<strong>Exemple 2 : estimation d’une proportion.</strong> Au sein d’une population, une proportion <span class="math inline">\(p\)</span> d’individus présente une caractéristique. On suppose que la présence de cette caractéristique est distribuée de façon identique et indépendante d’un individu à l’autre suivant la loi de Bernoulli de paramètre <span class="math inline">\(p\)</span>. On peut donc estimer la proportion <span class="math inline">\(p\)</span> au niveau population par la proportion <span class="math inline">\(\widehat{p_n}\)</span> au niveau échantillon : cet estimateur est la moyenne empirique, il est sans biais et de variance (inconnue) <span class="math inline">\(\frac{p(1-p)}{n}\)</span>.</p>
<p>
<strong>Exemple 3 : variance empirique.</strong> Si <span class="math inline">\(X\)</span> admet une variance <span class="math inline">\(\sigma^2\)</span> et <span class="math inline">\(X_1,\dots, X_n\)</span> sont i.i.d. de même loi que <span class="math inline">\(X\)</span>, alors un estimateur de <span class="math inline">\(\sigma^2\)</span> est donné par la variance empirique <span class="math inline">\(S_n^{&#39;2}=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X_n})^2\)</span>.</p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> La variance empirique <span class="math inline">\(S_n^{&#39;2}\)</span> est un estimateur biaisé de la variance <span class="math inline">\(\sigma^2\)</span>. Plus précisément, on a</p>
<p><span class="math display">\[\mathbb{E}(S_n^{&#39;2})=\frac{n-1}{n}\sigma^2\]</span></p>
</div>
<p>
<strong>Démonstration.</strong></p>
<span class="math display">\[\begin{align}
\mathbb{E}(S_n^{&#39;2}) &amp;= \frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i^2)-\frac{2\overline{X_n}}{n}\sum\limits_{i=1}^n\mathbb{E}(X_i)+\frac{1}{n}\sum\limits_{i=1}^n\mathbb{E}(\overline{X_n}^2) \\
&amp;= \mathbb{E}(X^2)-\mathbb{E}(\overline{X_n}^2)
\end{align}\]</span>
<p>Par ailleurs :</p>
<span class="math display">\[\begin{align}
\mathbb{E}(\overline{X_n}^2)&amp;=\frac{1}{n^2}\sum\limits_{i=1}^n\mathbb{E}(X_i^2)+\frac{1}{n^2}\sum\limits_{i\neq j}\mathbb{E}(X_iX_j) \\
&amp;= \frac{1}{n}^2\sum\limits_{i=1}^n\mathbb{E}(X_i^2)+\frac{1}{n^2}\sum\limits_{i\neq j}\mathbb{E}(X_i)\mathbb{E}(X_j) \\
&amp;= \frac{1}{n}\mathbb{E}(X^2)+\frac{n-1}{n}\left(\mathbb{E}(X)\right)^2
\end{align}\]</span>
<p>Avec la formule de Huygens <span class="math inline">\(\mathbb{V}(X)=\mathbb{E}(X^2)-\mathbb{E}(X)^2\)</span>, on en déduit que</p>
<p><span class="math display">\[\mathbb{E}(S_n^{&#39;2})=\frac{n-1}{n}\sigma^2\]</span> <span class="math inline">\(\square\)</span></p>
<p>
<strong>Remarque.</strong> Le biais de l’estimateur <span class="math inline">\(S_n^{&#39;2}\)</span> devient cependant très faible pour <span class="math inline">\(n\)</span> suffisamment grand. Il s’agit d’un estimateur asymptotiquement sans biais de la variance <span class="math inline">\(\sigma^2\)</span> : <span class="math inline">\(\mathbb{E}(S_n^{&#39;2})\longrightarrow \sigma^2\)</span>.</p>
<p>
<strong>Exemple 4 : variance empirique corrigée.</strong> En modifiant l’estimateur de la variance empirique par un petit facteur correctif, on obtient un estimateur sans biais de la variance. Il suffit de poser</p>
<p><span class="math display">\[S_n^2=\frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\overline{X_n})^2\]</span></p>
<p>Cet estimateur s’appelle la <strong>variance empirique corrigée</strong>.</p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> La variance empirique corrigée <span class="math inline">\(S_n^2=\frac{1}{n-1}\sum\limits_{i=1}^n (X_i-\overline{X_n})^2\)</span> est un estimateur sans biais de la variance <span class="math inline">\(\sigma^2=\mathbb{V}(X)\)</span> :</p>
<p><span class="math display">\[\mathbb{E}(S_n^2)=\sigma^2\]</span></p>
</div>
</div>
<div id="méthodes-de-construction-des-estimateurs" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Méthodes de construction des estimateurs<a href="statistique-inférentielle.html#méthodes-de-construction-des-estimateurs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>On présente ici deux méthodes classiques de construction des estimateurs ; la <strong>méthode des moments</strong> et la <strong>méthode du maximum de vraisemblance</strong>.</p>
<div id="la-méthode-des-moments" class="section level4 hasAnchor" number="7.1.4.1">
<h4><span class="header-section-number">7.1.4.1</span> La méthode des moments<a href="statistique-inférentielle.html#la-méthode-des-moments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="methbox meth">
<center>
<strong>La méthode des moments</strong>
</center>
<p>Soit <span class="math inline">\(X\)</span> une variable aléatoire réelle de loi <span class="math inline">\(\mathcal{L}_{\theta}\)</span>, où <span class="math inline">\(\theta\)</span> est un paramètre inconnu. On considère une fonction <span class="math inline">\(f\)</span> de <span class="math inline">\(I\subset\mathbb{R}\)</span> dans <span class="math inline">\(\mathbb{R}\)</span> telle que <span class="math inline">\(f(X)\)</span> admette une espérance. Comme la loi de <span class="math inline">\(X\)</span> dépend de <span class="math inline">\(\theta\)</span>, il en est de même de <span class="math inline">\(\mathbb{E}(f(X))\)</span>. La méthode des moments suppose qu’on sait expliciter une telle dépendance, i.e. qu’on connaisse une fonction <span class="math inline">\(g\)</span> telle que</p>
<p><span class="math display">\[\mathbb{E}(f(X))=g(\theta)\]</span></p>
<p>La contrepartie empirique du membre de gauche de cette égalité est <span class="math inline">\(\frac{1}{n}\sum\limits_{i=1}^n f(X_i)\)</span>, et la méthode des moments consiste alors à résoudre l’équation en <span class="math inline">\(\widehat{\theta}\)</span> :</p>
<p><span class="math display">\[g(\widehat{\theta})=\frac{1}{n}\sum\limits_{i=1}^n f(X_i)\]</span></p>
</div>
<p>
<strong>Exemple 5 : estimation du paramètre d’une loi exponentielle.</strong> Soit <span class="math inline">\(X\sim\mathcal{E}(\lambda)\)</span>, où <span class="math inline">\(\lambda&gt;0\)</span> est un paramètre inconnu que l’on veut estimer. La variable aléatoire <span class="math inline">\(X\)</span> admet une espérance, et celle-ci est donnée par <span class="math inline">\(\mathcal{E}(X)=\frac{1}{\lambda}\)</span>. La méthode des moments consiste alors à résoudre l’équation</p>
<p><span class="math display">\[\frac{1}{\widehat{\lambda_n}}=\frac{1}{n}\sum\limits_{i=1}^n X_i\]</span></p>
<p>Cette équation est très simple, elle admet pour solution</p>
<p><span class="math display">\[\widehat{\lambda_n}=\frac{1}{\overline{X_n}}\]</span></p>
<p>C’est l’estimateur que l’on obtient par la méthode des moments.</p>
<p>
<strong>Remarque.</strong> En reprenant les notations explicitées ci-dessus, on peut identifier les fonctions <span class="math inline">\(f\)</span> et <span class="math inline">\(g\)</span> :</p>
<p><span class="math display">\[f(x)=x\]</span> <span class="math display">\[g(x)=\frac{1}{x}\]</span> et ici évidemment <span class="math inline">\(\theta=\lambda\)</span>. En général, la méthode des moments s’utilise de façon complètement intuitive sans qu’on ait même à expliciter forcément les fonctions <span class="math inline">\(f\)</span> et <span class="math inline">\(g\)</span>.</p>
</div>
<div id="la-méthode-du-maximum-de-vraisemblance" class="section level4 hasAnchor" number="7.1.4.2">
<h4><span class="header-section-number">7.1.4.2</span> La méthode du maximum de vraisemblance<a href="statistique-inférentielle.html#la-méthode-du-maximum-de-vraisemblance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Une autre méthode de construction d’estimateurs est celle du <strong>maximum de vraisemblance</strong>. L’idée générale de cette méthode est la suivante. On suppose qu’on dispose de réalisations <span class="math inline">\(x_1,\dots x_n\)</span> d’une même variable aléatoire, dont la loi appartient à une famille paramétrique <span class="math inline">\(\left\{\mathcal{L}_{\theta}\,;\,\theta\in\Theta\right\}\)</span> et on cherche à estimer <span class="math inline">\(\theta\)</span>. Si par exemple on dispose d’une série de cinq obersations <span class="math inline">\((0.12, -0.65, 1.35, 1.04, -1.19, 0.08)\)</span> et qu’on veut inférer sur <span class="math inline">\(\theta\)</span> à partir de ces observations, on est enclin à penser que la valeur <span class="math inline">\(\theta=0\)</span> est plus plausible que la valeur <span class="math inline">\(\theta=-10\)</span>. La vraisemblance est une formalisation de l’idée intuitive de plausibilité d’un paramètre à partir d’une observation ou d’un ensemble d’observations.</p>
<p>A nouveau, <span class="math inline">\(X\)</span> désigne une variable aléatoire de loi dépendant d’un paramètre inconnu <span class="math inline">\(\theta\)</span>, et <span class="math inline">\(x\)</span> une réalisation de <span class="math inline">\(X\)</span>.</p>
<p>La <em>vraisemblance</em> <span class="math inline">\(L(x,.)\)</span> est une fonction de <span class="math inline">\(\theta\)</span> définie par</p>
<p><span class="math display">\[L(x;\theta)=\left\{
\begin{array}{lll}
\mathbb{P}_{\theta}(X=x) &amp;\text{; si } X \text{ est discrète} \\
f(x;\theta) &amp;\text{; si } X \text{ est une continue de densité } f(.;\theta) \\
\end{array}
\right.\]</span></p>
<p>
<strong>Remarque :</strong> D’autres notations existent dans la littérature, comme <span class="math inline">\(L(x|\theta), \mathbb{P}(X=x|\theta), f(x|\theta)\)</span>. Ces notations viennent de la statistique bayésienne (hors programme du concours) qui envisage <span class="math inline">\(\theta\)</span> comme une variable aléatoire de distribution inconnue. Dans ce cas, la vraisemblance s’interprète comme une probabilité ou une densité de probabilité.</p>
<p>La définition précédente s’étend au cas d’un échantillon <span class="math inline">\((X_1,\dots X_n)\)</span> de VA de même loi que <span class="math inline">\(X\)</span>. Un cas particulier important est celui où ces VA sont i.i.d. Dans ce cas, la vraisemblance est définie par</p>
<p><span class="math display">\[L(x;\theta)=L(x_1,\dots,x_n ; \theta) \left\{
\begin{array}{lll}
\prod\limits_{i=1}^n\mathbb{P}_{\theta}(X_i=x_i) &amp;\text{; si } X \text{ est discrète} \\
\prod\limits_{i=1}^n f(x_i,\theta) &amp;\text{; si } X \text{ est une continue de densité } f(.;\theta) \\
\end{array}
\right.\]</span></p>
<p>La méthode du maximum de vraisemblance consiste juste à dire que si toute l’information dont on dispose sur la variable aléatoire <span class="math inline">\(X\)</span> est l’observation de l’échantillon <span class="math inline">\((x_1, \dots, x_n)\)</span>, alors la meilleure estimation que l’on puisse faire de <span class="math inline">\(\theta\)</span> à partir de cette information est celle qui maximise la fonction de vraisemblance. Autrement dit, on cherche la valeur de <span class="math inline">\(\theta\)</span> qui rend l’observation <span class="math inline">\((x_1,\dots, x_n)\)</span> la plus plausible. Formellement :</p>
<div class="methbox meth">
<center>
<strong>Méthode du maximum de vraisemblance</strong>
</center>
<p>Etant donné une collection de <span class="math inline">\(n\)</span> réalisations <span class="math inline">\(x=(x_1,\dots, x_n)\)</span> des VA <span class="math inline">\((X_1,\dots X_n)\)</span> de même loi <span class="math inline">\(\mathcal{L}_{\theta}\)</span> de paramètre inconnu <span class="math inline">\(\theta\)</span>, on appelle <em>estimation du maximum de vraisemblance</em> toute estimation <span class="math inline">\(\widehat{\theta}_n=\widehat{\theta}_n(x_1,\dots,x_n)\)</span> vérifiant</p>
<p><span class="math display">\[\widehat{\theta}_n\in \arg\max\limits_{\theta\in\Theta}L(x;\theta)\]</span> **Cas particulier :** si la fonction <span class="math inline">\(\theta\mapsto L(x;\theta)\)</span> est deux fois dérivable sur <span class="math inline">\(\Theta\)</span>, alors on peut chercher à résoudre (en $) le système</p>
<p><span class="math display">\[\left\{
\begin{array}{lll}
\frac{\partial}{\partial\theta}L(x;\theta)=0 \\
\frac{\partial^2}{\partial\theta^2}L(x;\theta)&lt;0 \\
\end{array}
\right.\]</span>
Les solutions de ce système fournissent des estimations par maximum de vraisemblance.</p>
</div>
<p>
<strong>Log-vraisemblance.</strong> Il est souvent plus commode de considérer la log-vraisemblance <span class="math inline">\(l(x;\theta)=\ln L(x;\theta)=\sum\limits_{i=1}^n \ln L(x_i;\theta)\)</span>. La fonction <span class="math inline">\(\ln\)</span> étant croissante sur <span class="math inline">\(\mathbb{R}_{+}^*\)</span>, maximiser la vraisemblance équivaut à maximiser la log-vraisemblance.</p>
<div class="methbox meth">
<center>
<strong>Méthode du maximum de vraisemblance (version log-vraisemblance)</strong>
</center>
<p>En supposant que <span class="math inline">\(L(x;\theta)&gt;0\)</span> pour tout <span class="math inline">\(\theta\in\Theta\)</span>, on note <span class="math inline">\(l(x;\theta)=\ln L(x;\theta)\)</span> la log-vraisemblance. Sous les mêmes hypothèses que ci-dessus, on cherche</p>
<p><span class="math display">\[\widehat{\theta}_n\in\arg\max\limits_{\theta\in\Theta}\left(\ln L(x;\theta)\right)\]</span> **Cas particulier :** si la fonction <span class="math inline">\(\theta\mapsto L(x;\theta)\)</span> est deux fois dérivable sur <span class="math inline">\(\Theta\)</span>, alors la fonction <span class="math inline">\(\theta\mapsto l(x;\theta)\)</span> l’est aussi et on peut chercher à résoudre (en <span class="math inline">\(\theta\)</span>) le système</p>
<p><span class="math display">\[\left\{
\begin{array}{lll}
\frac{\partial}{\partial\theta}l(x;\theta)=0 \\
\frac{\partial^2}{\partial\theta^2}l(x;\theta)&lt;0 \\
\end{array}
\right.\]</span> Les solutions de ce système fournissent des estimations par maximum de vraisemblance.</p>
</div>
<p>
<strong>Exemple 6 : loi normale.</strong> <span class="math inline">\(\mathcal{N}(\mu, 1)\)</span>. On veut estimer le paramètre inconnu <span class="math inline">\(\mu\)</span> par maximum de vraisemblance. La vraisemblance est donnée par</p>
<span class="math display">\[\begin{align}
L(x;\mu)&amp;=\prod\limits_{i=1}^n\left(\frac{e^{-\frac{(x_i-\mu)^2}{2}}}{\sqrt{2\pi}}\right)\\
&amp;=\frac{1}{(2\pi)^{\frac{n}{2}}}e^{-\sum\limits_{i=1}^n (x_i-\mu)^2}
\end{align}\]</span>
<p>La log-vraisemblance est plus facile à manipuler :</p>
<span class="math display">\[\begin{align}
l(x;\mu)&amp;=\ln L(x;\mu) \\
&amp;= -\frac{n}{2}\ln(2\pi)-\sum\limits_{i=1}^n(x_i-\mu)^2
\end{align}\]</span>
<p>La fonction <span class="math inline">\(\mu\mapsto l(x;\mu)\)</span> est deux fois dérivable sur <span class="math inline">\(\mathbb{R}\)</span> et <span class="math inline">\(\frac{\partial}{\partial\mu}l(x;\mu)=2\sum_{i=1}^n(\mu-x_i)\)</span>. Une seule valeur de <span class="math inline">\(\mu\)</span> l’annule :</p>
<p><span class="math display">\[\widehat{\mu}_n=\frac{1}{n}\sum\limits_{i=1}^n x_i=\overline{x}_n\]</span> Par ailleurs <span class="math inline">\(\frac{\partial^2}{\partial\mu^2}l(x;\mu)=2n&gt;0\)</span>, et donc <span class="math inline">\(\widehat{\mu}_n\in\arg\max\limits_{\mu\in\mathbb{R}}l(x;\mu)\)</span>. Finalement, un estimateur par maximum de vraisemblance est donné par</p>
<p><span class="math display">\[\widehat{\mu}_n=\overline{X}_n=\frac{1}{n}\sum\limits_{i=1}^n X_i\]</span></p>
<p>
<strong>Remarque.</strong> Comme souvent en statistique, on commet un léger abus de notation en désignant par la même lettre l’estimateur <span class="math inline">\(\widehat{\mu}_n=\frac{X_1+\dots+X_n}{n}=\widehat{\mu}_n(X_1,\dots,X_n)\)</span> (qui est une statistique, i.e. une fonction de <span class="math inline">\((X_1,\dots,X_n\)</span>) et l’estimation <span class="math inline">\(\widehat{\mu}_n=\frac{x_1+\dots+x_n}{n}=\widehat{\mu}_n(x_1,\dots, X_n)\)</span> qui en est une réalisation. Conditionnellement à <span class="math inline">\((X_1,\dots,X_n)\)</span> (i.e. si l’on suppose que l’on observe <span class="math inline">\((X_1,\dots, X_n)\)</span>) ces deux objets sont bien les mêmes.</p>
<p>
<strong>Exemple 7 : loi expoentielle.</strong> Soit <span class="math inline">\((X_1,\dots,X_n)\)</span> un échantillon i.i.d. tiré selon une loi exponentielle <span class="math inline">\(\mathcal{E}(\lambda)\)</span> de paramètre <span class="math inline">\(\lambda&gt;0\)</span> inconnu. La vraisemblance est donnée par <span class="math display">\[\begin{align}
L(x;\lambda)&amp;=\prod\limits_{i=1}^n (\lambda e^{-\lambda x_i}\mathbb{1}_{x_i\geq 0}) \\
\end{align}\]</span></p>
<p>Si l’un des <span class="math inline">\(x_i\)</span> est négatif elle vaut <span class="math inline">\(0\)</span>. Sinon on calcule la log-vraisemblance</p>
<p><span class="math display">\[l(x;\lambda)=n\ln(\lambda)-\lambda\sum_{i=1}^n x_i\]</span></p>
<p>La fonction <span class="math inline">\(\lambda\in\mathbb{R}_{+}^*\mapsto l(x;\lambda)\)</span> est deux fois dérivable et <span class="math inline">\(\frac{\partial}{\partial\lambda}l(x;\lambda)=\frac{n}{\lambda}-\sum\limits_{i=1}^n x_i\)</span>, qui s’annule en <span class="math inline">\(\lambda=\frac{n}{\sum\limits_{i=1}^n x_i}=\frac{1}{\overline{x}_n}\)</span>. De plus, <span class="math inline">\(\frac{\partial^2}{\partial\lambda^2}l(x;\lambda)=-\frac{n}{\lambda^2}&lt;0\)</span> et donc à <span class="math inline">\(x\)</span> fixé, <span class="math inline">\(l(x;\lambda)\)</span> atteint son maximum en <span class="math inline">\(\frac{1}{\overline{x_n}}\)</span>. L’estimateur du maximum de vraisemblance est donc <span class="math inline">\(\widehat{\lambda}_n=\frac{1}{\overline{X}_n}\)</span>. On remarque qu’on retrouve ici le même estimateur que celui obtenu par la méthode des moments.</p>
</div>
</div>
<div id="compléments-hors-programme" class="section level3 hasAnchor" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Compléments (hors-programme)<a href="statistique-inférentielle.html#compléments-hors-programme" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>On présente dans cette partie les notions suivantes :</p>
<ul>
<li>information de Fisher</li>
<li>borne de Cramer-Rao</li>
<li>statistique exhaustive</li>
<li>famille exponentielle</li>
<li>amélioration d’un estimateur</li>
</ul>
<p>Ces notions ne sont pas au programme du concours, mais elles sont clairement dans sa périphérie immédiate. On les retrouve d’ailleurs dans le sujet d’interne 2022, mais leur connaissance n’est pas requise pour traiter le sujet.</p>
<div id="information-de-fisher" class="section level4 hasAnchor" number="7.1.5.1">
<h4><span class="header-section-number">7.1.5.1</span> Information de Fisher<a href="statistique-inférentielle.html#information-de-fisher" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Soient <span class="math inline">\(X\)</span> une variable aléatoire à valeurs dans <span class="math inline">\(\mathcal{X}\)</span> de loi <span class="math inline">\(f(x;\theta)&gt;0\)</span>, <span class="math inline">\(\theta\in\mathbb{R}\)</span>. On fait les hypothèses suivantes :</p>
<ul>
<li>existence de <span class="math inline">\(\frac{\partial f}{\partial\theta}(x;\theta)\)</span> et de <span class="math inline">\(\frac{\partial^2}{\partial\theta^2}f(x;\theta)\)</span></li>
<li>on peut échanger tous les opérateurs de dérivation (à l’ordre <span class="math inline">\(1\)</span> et <span class="math inline">\(2\)</span>) et d’intégration</li>
</ul>
<p>On appelle alors <strong>score</strong> la quantité (aléatoire) <span class="math inline">\(\frac{\partial}{\partial\theta}\,\ln f(X;\theta)\)</span>, i.e. la dérivée de la log-vraisemblance par rapport à <span class="math inline">\(\theta\)</span>.</p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> On a</p>
<p><span class="math display">\[\mathbb{E}_{\theta}\left(\frac{\partial }{\partial\theta}\,\ln f(X;\theta)\right)=0\]</span></p>
<p>i.e. le score est d’espérance nulle.</p>
</div>
<p>
<strong>Démonstration.</strong>
<span class="math display">\[\begin{align}
\mathbb{E}_{\theta}\left(\frac{\partial}{\partial\theta}\,\ln f(X ; \theta)\right) &amp;= \int_{\mathbb{R}} \frac{\partial}{\partial\theta}\ln f(x;\theta)\,f(x;\theta)\,dx \\
&amp;=\int_{\mathbb{R}}\frac{\frac{\partial f}{\partial\theta}(x;\theta)}{f(x;\theta)}\,f(x;\theta)\,dx \\
&amp;= \int_{\mathbb{R}} \frac{\partial f}{\partial\theta}(x;\theta)\,dx \\
&amp;=\frac{\partial}{\partial\theta}\int_{\mathbb{R}} f(x;\theta) \, dx \text{ (on permute intégrale et dérivée)} \\
&amp;= 0 \text{ (car } \int_{\mathbb{R}} f(x;\theta)\,dx=1\text{)} \\
\end{align}\]</span>
<span class="math inline">\(\square\)</span></p>
<p>L’information de Fisher est définie à partir du score de la façon suivante :</p>
<div class="defbox def">
<center>
<strong>Information de Fisher</strong>
</center>
<p>L’<strong>information de Fisher</strong> est la quantité définie par</p>
<p><span class="math display">\[I(\theta)\equiv\mathbb{E}_{\theta}\left(\left(\frac{\partial }{\partial\theta}\,\ln f(X;\theta)\right)^2\right)=\mathbb{V}_{\theta}\left(\frac{\partial}{\partial\theta}\ln f(X;\theta)\right)\]</span></p>
<p>L’information de Fisher est aussi égale à</p>
<p><span class="math display">\[I(\theta)=-\mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2} \ln f(X;\theta)\right)\]</span></p>
</div>
<p>
<strong>Interprétation de l’information de Fisher.</strong> On utilise généralement l’information de Fisher lorsqu’on veut inférer sur un paramètre inconnu <span class="math inline">\(\theta\)</span> par maximum de vraisemblance. Par construction, l’estimation <span class="math inline">\(\widehat{\theta}\)</span> que l’on obtient par cette méthode est celle qui maximise la log-vraisemblance <span class="math inline">\(\ln f(X;\theta)\)</span>, pour une observation de <span class="math inline">\(X\)</span> donnée. L’expression <span class="math inline">\(I(\theta)=-\mathbb{E}_{\theta}\left(\frac{\partial^2}{\partial\theta^2}\ln f(X;\theta)\right)\)</span> montre que l’information de Fisher correspond (au signe près) à la courbure de la log-vraisemblance. Plus celle-ci est importante, plus la courbe présente un “pic” autour du maximum, et donc plus la valeur estimée de ce maximum est précise. Au contraire, si la courbure est faible, la courbe est aplatie autour du maximum, et donc l’estimation de <span class="math inline">\(\theta\)</span> sera moins précise. Dit autrement, l’information de Fisher quantifie le niveau d’information que nous apporte l’observation relativement au paramètre <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="borne-de-cramer-rao" class="section level4 hasAnchor" number="7.1.5.2">
<h4><span class="header-section-number">7.1.5.2</span> Borne de Cramer-Rao<a href="statistique-inférentielle.html#borne-de-cramer-rao" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sous certaines hypothèses, on peut montrer que la variance d’un estimateur sans biais ne peut être inférieure à une certaine borne, appelée borne de Cramer-Rao. Cette borne est liée à l’information de Fisher.</p>
</div>
</div>
<div id="estimation-des-coefficients-dune-régression-linéaire" class="section level3 hasAnchor" number="7.1.6">
<h3><span class="header-section-number">7.1.6</span> Estimation des coefficients d’une régression linéaire<a href="statistique-inférentielle.html#estimation-des-coefficients-dune-régression-linéaire" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="présentation-du-modèle" class="section level4 hasAnchor" number="7.1.6.1">
<h4><span class="header-section-number">7.1.6.1</span> Présentation du modèle<a href="statistique-inférentielle.html#présentation-du-modèle" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont deux variables aléatoires pour lesquelles on dispose d’observations <span class="math inline">\(x_1,\dots, x_n\)</span> et <span class="math inline">\(y_1,\dots y_n\)</span>. On considère le modèle</p>
<p><span class="math display">\[Y_i=aX_i+b+u_i\]</span></p>
<p>où <span class="math inline">\((a,b)\)</span> est un couple de réels inconnus et <span class="math inline">\(u_i\)</span> est un terme d’erreur (inconnu lui aussi). Le but est d’estimer des coefficients <span class="math inline">\((a,b)\)</span> à partir de l’échantillons d’observations <span class="math inline">\((x_i, y_i)\)</span> et de donner des propriétés des estimateurs obtenus sous certaines hypothèses.</p>
<p>
<strong>Hypothèses du modèle.</strong> On fait les hypothèses suivantes :</p>
<ul>
<li><strong>(H1) :</strong> Les couples <span class="math inline">\((X_i, Y_i)\)</span> sont i.i.d.</li>
<li><strong>(H2) :</strong> Les termes d’erreur <span class="math inline">\(u_i\)</span> sont indépendants des <span class="math inline">\(X_i\)</span></li>
<li><strong>(H3) :</strong> <span class="math inline">\(\mathbb{E}(u_i|X_i)=0\)</span> (hypothèse d’exogénéité)</li>
<li><strong>(H4) :</strong> <span class="math inline">\(\mathbb{V}(u_i|X_i)=\sigma_u^2\)</span> ne dépend pas de <span class="math inline">\(X_i\)</span> (hypothèse d’homoscédasticité)</li>
<li><strong>(H5) :</strong> <span class="math inline">\(u_i|X_i\sim\mathcal{N}(0, \sigma_u^2)\)</span> (hypothèse de normalité des termes d’erreur)</li>
</ul>
<p>On présente deux approches différentes pour estimer <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> : par la méthode des moindres carrés et par maximum de vraisemblance. Bien que différentes, ces méthodes vont fournir les mêmes estimateurs.</p>
<p>Avant cela, on rappelle quelques résultats classiques de statistique descriptive.</p>
</div>
<div id="rappels-utiles" class="section level4 hasAnchor" number="7.1.6.2">
<h4><span class="header-section-number">7.1.6.2</span> Rappels utiles<a href="statistique-inférentielle.html#rappels-utiles" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Avant de présenter cette méthode, on rappelle des égalités qui àa la fois très utiles et très classiques. Il faut les connaître pour le concours et savoir les redémontrer.</p>
<div class="methbox meth">
<center>
<strong>Moyenne, covariance, variance</strong>
</center>
<p>Pour <span class="math inline">\(x=(x_1,\dots, x_n)\in\mathbb{R}^n\)</span> on note</p>
<ul>
<li><span class="math inline">\(\overline{x}_n=\frac{1}{n}\sum\limits_{i=1}^n x_i\)</span> la moyenne de <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(\sigma_x^2=\frac{1}{n}\sum\limits_{i=1}^n(x_i-\overline{x}_n)^2\)</span> la variance de <span class="math inline">\(x\)</span></li>
<li>si de plus <span class="math inline">\(y=(y_1,\dots, y_n)\)</span>, <span class="math inline">\(\sigma_{xy}=\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n)\)</span> est la covariance de <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</li>
</ul>
<p>On a alors les égalités suivantes :</p>
<p><strong>1.</strong> <span class="math inline">\(\sigma_{xx}=\sigma_x^2\)</span></p>
<p><strong>2.</strong> <span class="math inline">\(\sum\limits_{i=1}^n (x_i-\overline{x}_n)=0\)</span></p>
<p><strong>3. Différentes formules de la covariance :</strong></p>
<span class="math display">\[\begin{align}
\sigma_{xy} &amp;= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n(x_i-\overline{x}_n)y_i \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n x_i(y_i-\overline{y}_n) \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\overline{y}_n
\end{align}\]</span>
<p><strong>4. Différentes formules de la variance :</strong></p>
<span class="math display">\[\begin{align}
\sigma_x^2 &amp;= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)^2 \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n x_i^2-(\overline{x}_n)^2
\end{align}\]</span>
</div>
<p><strong>Démonstration.</strong></p>
<p>
<strong>1.</strong> Evidente</p>
<p><strong>2.</strong> <span class="math display">\[\begin{align}
\sum\limits_{i=1}^n (x_i-\overline{x}_n) &amp;= \sum\limits_{i=1}^n x_i -n\overline{x}_n \\
&amp;= n\overline{x}_n-n\overline{x}_n \\
&amp; =0
\end{align}\]</span></p>
<p><strong>3.</strong> <span class="math display">\[\begin{align}
\sigma_{xy} &amp;= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) \\
&amp;=\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i-\frac{\overline{y}_n}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n) \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i
\end{align}\]</span></p>
<p>d’après l’égalité 2.</p>
<p>Par symétrie des rôles joués par <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> on a donc aussi <span class="math inline">\(\sigma_{xy}=\frac{1}{n}\sum\limits_{i=1}^n x_i(y_i-\overline{y}_n)\)</span>.</p>
<p>On montre la dernière égalité :</p>
<span class="math display">\[\begin{align}
\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)(y_i-\overline{y}_n) &amp;=
\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)y_i \\
&amp;= \frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\frac{1}{n}\sum\limits_{i=1}^n y_i
\\
&amp;=\frac{1}{n}\sum\limits_{i=1}^n x_iy_i-\overline{x}_n\overline{y}_n
\end{align}\]</span>
<p><strong>4.</strong> On applique la dernière égalité de 4 dans le cas particulier où <span class="math inline">\(x=y\)</span>. On obtient alors</p>
<p><span class="math display">\[\frac{1}{n}\sum\limits_{i=1}^n (x_i-\overline{x}_n)^2=\frac{1}{n}\sum\limits_{i=1}^n x_i^2-\left(\frac{1}{n}\sum\limits_{i=1}^n x_i\right)^2\]</span> <span class="math inline">\(\square\)</span></p>
</div>
<div id="estimation-de-a-et-b-par-la-méthode-des-moindres-carrés" class="section level4 hasAnchor" number="7.1.6.3">
<h4><span class="header-section-number">7.1.6.3</span> Estimation de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> par la méthode des moindres carrés<a href="statistique-inférentielle.html#estimation-de-a-et-b-par-la-méthode-des-moindres-carrés" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>On montre maintenant les formules des estimateurs de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> par application de la méthode des moindres carrés :</p>
<div class="methbox meth">
<center>
<strong>Estimation de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> par la méthode des moindres carrés</strong>
</center>
<p>La méthode des moindres carrés consiste à minimiser l’erreur quadratique globale</p>
<p><span class="math display">\[E(\alpha,\beta)\equiv\sum\limits_{i=1}^n (Y_i-\alpha X_i-\beta)^2\]</span> qui représente l’erreur globale faite en approchant <span class="math inline">\(Y_i\)</span> par <span class="math inline">\(\alpha X_i+\beta\)</span>.</p>
<p>Cette méthode fournit les estimateurs suivants de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> :</p>
<span class="math display">\[\begin{align}
\left\{
\begin{array}{ll}
\widehat{a} &amp;= \frac{\overline{\sigma_{XY}}}{\overline{\sigma^2_X}} \\
\widehat{b} &amp;= \overline{Y}_n-\widehat{a}\overline{X}_n
\end{array}
\right.
\end{align}\]</span>
<p>où on note <span class="math inline">\(\overline{\sigma_{XY}}=\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X}_n)(Y_i-\overline{Y}_n)\)</span> et <span class="math inline">\(\overline{\sigma^2_X}=\overline{\sigma_{XX}}=\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\)</span>.</p>
</div>
<p>
<strong>Démonstration.</strong> La fonction <span class="math inline">\((\alpha, \beta)\mapsto E(\alpha, \beta)\)</span> est deux fois dérivable par rapport à chacune de ses variables. </p>
<p><strong>Conditions de premier ordre (CPO) :</strong> Les conditions du premier ordre s’écrivent</p>
<span class="math display">\[\begin{align}
\left\{
\begin{array}{ll}
\frac{\partial}{\partial\alpha} E(\alpha, \beta) &amp;=0 \\
\frac{\partial}{\partial\beta} E(\alpha, \beta) &amp;=0 \\
\end{array}
\right.
\end{align}\]</span>
<p>i.e.</p>
<span class="math display">\[\begin{align}
\left\{
\begin{array}{ll}
\sum\limits_{i=1}^n X_i Y_i-\alpha\sum\limits_{i=1}^n X_i^2-\beta\sum\limits_{i=1}^n X_i&amp;=0 \\
\sum\limits_{i=1}^n Y_i-\alpha\sum\limits_{i=1}^n X_i-n\beta &amp;= 0 \\
\end{array}
\right.
\end{align}\]</span>
<p>Il s’agit d’un système de deux équations à deux inconnues <span class="math inline">\((\alpha, \beta)\)</span>. Sa résolution donne</p>
<span class="math display">\[\begin{align}
\left\{
\begin{array}{ll}
\alpha &amp;= \frac{\frac{1}{n}\sum\limits_{i=1}^n X_iY_i-\left(\frac{1}{n}\sum\limits_{i=1}^n X_i\right)\left(\frac{1}{n}\sum\limits_{i=1}^n Y_i\right)}{\frac{1}{n}\sum\limits_{i=1}^n X_i^2-\left(\frac{1}{n}\sum\limits_{i=1}^n X_i\right)^2} \\
\beta &amp;= \overline{Y}_n-\alpha\overline{X}_n
\end{array}
\right.
\end{align}\]</span>
<p>soit encore</p>
<span class="math display">\[\begin{align}
\left\{
\begin{array}{ll}
\alpha &amp;= \frac{\overline{\sigma_{XY}}}{\overline{\sigma^2_X}} \\
\beta &amp;= \overline{Y}_n-\alpha\overline{X}_n
\end{array}
\right.
\end{align}\]</span>
<p>Par ailleurs, pour tout couple <span class="math inline">\((x, y)\)</span> de réels, la fonction <span class="math inline">\((\alpha, \beta)\mapsto (y-\alpha x-\beta)^2\)</span> est convexe. Le point critique trouvé ci-dessus est donc un minimum.</p>
<p>On en déduit le résultat.</p>
<p><span class="math inline">\(\square\)</span></p>
</div>
<div id="estimation-de-a-et-b-par-la-méthode-du-maximum-de-vraisemblance" class="section level4 hasAnchor" number="7.1.6.4">
<h4><span class="header-section-number">7.1.6.4</span> Estimation de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> par la méthode du maximum de vraisemblance<a href="statistique-inférentielle.html#estimation-de-a-et-b-par-la-méthode-du-maximum-de-vraisemblance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La méthode par maximum de vraisemblance requiert une information supplémentaire : celle de la distribution de la variable de terme d’erreur <span class="math inline">\(u\)</span>. Or, une telle information est justement donnée ici par l’hypothèse (H5) de distribution normale du terme d’erreur.</p>
<div class="methbox meth">
<center>
<strong>Estimation de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> par la méthode du maximum de vraisemblance.</strong>
</center>
<p>Sous l’hypothèse <span class="math inline">\((H5)\)</span> de distribution normale des termes d’erreur, la méthode par maximum de vraisemblance fournit les mêmes estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span> que la méthode des moindres carrés.</p>
</div>
<p>
<strong>Démonstration.</strong> La vraisemblance est donnée par</p>
<p><span class="math display">\[L((\alpha,\beta);u)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma_u}e^{-\frac{(Y_i-\alpha X_i-\beta)^2}{2\sigma_u^2}}\]</span></p>
<p>On passe à la log-vraisemblance, qui est plus simple à dériver</p>
<p><span class="math display">\[l((\alpha,\beta);u)=-n\ln(\sqrt{2\pi}\sigma_u^2)-\frac{(Y_i-\alpha X_i-\beta)^2}{2\sigma_u^2}\]</span></p>
<p>On résout alors en <span class="math inline">\((\alpha, \beta)\)</span> le système d’équations</p>
<span class="math display">\[\begin{align}
\frac{\partial l}{\partial\alpha}l((\alpha,\beta);u) &amp;= 0 \\
\frac{\partial l}{\partial\beta}l((\alpha,\beta);u) &amp;= 0 \\
\end{align}\]</span>
<p>soit</p>
<span class="math display">\[\begin{align}
\frac{X_i(Y_i-\alpha X_i-\beta)}{2\sigma_u^2} &amp;= 0 \\
\frac{Y_i-\alpha X_i-\beta}{2\sigma_u^2} &amp;= 0 \\
\end{align}\]</span>
<p>On vérifie facilement qu’on obtient le même couple de solution qu’avec la méthode des moindres carrés, et que ce couple constitue bien un maximum de la log-vraisemblance.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>
<strong>Remarque :</strong> Dans des approches plus générales que celle présentée ici, aucune hypothèse n’est faite sur la distribution des termes d’erreur. Dans ce cas, la méthode par maximum de vraisemblance n’est plus applicable. On peut cependant toujours utiliser la méthode des moindres carrés.</p>
</div>
<div id="absence-de-biais-des-estimateurs-widehata-et-widehatb" class="section level4 hasAnchor" number="7.1.6.5">
<h4><span class="header-section-number">7.1.6.5</span> Absence de biais des estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span><a href="statistique-inférentielle.html#absence-de-biais-des-estimateurs-widehata-et-widehatb" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="thmbox thm">
<p><strong>Théorème : (absence de biais des estimateurs MCO)</strong> Les estimateurs</p>
<p><span class="math display">\[\widehat{a}=\frac{\overline{\sigma_{XY}}}{\overline{\sigma_X^2}}\]</span> et <span class="math display">\[\widehat{b}=\overline{Y}_n-\widehat{a}\overline{X}_n\]</span> sont des estimateurs <strong>sans biais</strong> de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span>.</p>
</div>
<p>
<strong>Démonstration.</strong> On remarque d’abord qu’avec l’hypothèse d’exogénéité (H3) <span class="math inline">\(\mathbb{E}(u_i|X_i)=0\)</span> on a <span class="math inline">\(\mathbb{E}(Y_i|X_i)=aX_i+b\)</span> et donc <span class="math inline">\(\mathbb{E}(Y_i-\overline{Y}_n|X_1,\dots, X_n)=a(X_i-\overline{X}_n)\)</span>. D’où</p>
<span class="math display">\[\begin{align}
\mathbb{E}(\widehat{a}|X_1,\dots, X_n) &amp;= \mathbb{E}\left(\left.\frac{\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)(Y_i-\overline{Y}_n)}{\overline{\sigma_X^2}}\right|X_1,\dots, X_n\right) \\
&amp;=\frac{1}{\overline{\sigma_X^2}}\frac{1}{n}\sum\limits_{i=1}^n(X_i-\overline{X}_n)\mathbb{E}(\left. Y_i-\overline{Y}_n\right|X_1\,\dots,X_n) \\
&amp;= a\frac{1}{\overline{\sigma_X^2}}\frac{1}{n}\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2 \\
&amp; =a\frac{\overline{\sigma_X^2}}{\overline{\sigma_X^2}} \\
&amp; =a
\end{align}\]</span>
<p>Par ailleurs</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\widehat{b}|X_1,\dots,X_n)&amp;=\mathbb{E}(\overline{Y}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&amp;=\frac{1}{n}\sum\limits_{i=1}^n \mathbb{E}(Y_i|X_1,\dots,X_n)-\overline{X}_n\mathbb{E}(\widehat{a}|X_1,\dots,X_n) \\
&amp;=\frac{1}{n}\sum\limits_{i=1}^n (aX_i+b)-a\overline{X}_n \\
&amp;= a\overline{X}_n+b-a\overline{X}_n \\
&amp;= b
\end{align}\]</span> <span class="math inline">\(\square\)</span></p>
</div>
<div id="variance-des-estimateurs-widehata-et-widehatb" class="section level4 hasAnchor" number="7.1.6.6">
<h4><span class="header-section-number">7.1.6.6</span> Variance des estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span><a href="statistique-inférentielle.html#variance-des-estimateurs-widehata-et-widehatb" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="thmbox thm">
<p><strong>Théorème (variance des estimateurs MCO)</strong> Les estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span> ont pour variances</p>
<span class="math display">\[\begin{align}
\mathbb{V}(\widehat{a}|X_1,\dots, X_n) &amp;= \frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2} \\
\mathbb{V}(\widehat{b}|X_1,\dots,X_n) &amp;=\sigma_u^2\left(\frac{1}{n}+ \frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right)
\end{align}\]</span>
</div>
<p>
<strong>Démonstration.</strong> On remarque tout d’abord que</p>
<p><span class="math display">\[\mathbb{V}(Y_i|X_1,\dots,X_n)=\sigma_u^2\]</span></p>
<p>En effet</p>
<span class="math display">\[\begin{align}
\mathbb{V}(Y_i|X_1,\dots,X_n) &amp;= \mathbb{V}(aX_i+b+u_i|X_1,\dots, X_n) \\
&amp;= \mathbb{V}(u_i|X_1,\dots,X_n) \\
&amp;= \sigma_u^2
\end{align}\]</span>
<p>Le passage de la première à la deuxième ligne vient du fait qu’à <span class="math inline">\(X_1,\dots, X_n\)</span> fixées, <span class="math inline">\(aX_i+b\)</span> est considérée comme une constante, et donc ce terme a une contribution à la variance conditionnellement à <span class="math inline">\(X_1,\dots X_n\)</span>.</p>
<p>On a donc</p>
<span class="math display">\[\begin{align}
\mathbb{V}(\widehat{a}|X_1,\dots X_n) &amp;= \mathbb{V}\left(\left.\frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)Y_i}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right|X_1,\dots, X_n\right) \\
&amp;= \frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\mathbb{V}(Y_i|X_1,\dots,X_n)}{\left(\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\right)^2} \\
&amp; \text{ (somme de VA i.i.d.)} \\
&amp;= \frac{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\sigma_u^2}{\left(\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2\right)^2} \\
&amp;= \frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}
\end{align}\]</span>
<p>et</p>
<span class="math display">\[\begin{align}
\mathbb{V}(\widehat{b}|X_1,\dots,X_n) &amp;= \mathbb{V}(\overline{Y}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&amp;= \mathbb{V}(a\overline{X}_n+b+\overline{u}_n-\widehat{a}\overline{X}_n|X_1,\dots,X_n) \\
&amp;= \mathbb{V}((a-\widehat{a}\overline{X}_n)+b+\overline{u}_n|X_1,\dots, X_n) \\
&amp;= \overline{X}_n^2\underbrace{\mathbb{V}(\widehat{a}|X_1,\dots,X_n)}_{=\frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}+\underbrace{\mathbb{V}(\overline{u}_n|X_1,\dots,X_n)}_{=\frac{\sigma_u^2}{n} \text{ car } u_i \text{ i.i.d. de variance } \sigma_u^2} \\
&amp;= \overline{X}_n^2\frac{\sigma_u^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}+\frac{\sigma_u^2}{n} \\
&amp;= \sigma_u^2\left(\frac{1}{n}+\frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}\right)
\end{align}\]</span>
<p><span class="math inline">\(\square\)</span></p>
<p></p>
</div>
<div id="résidus" class="section level4 hasAnchor" number="7.1.6.7">
<h4><span class="header-section-number">7.1.6.7</span> Résidus<a href="statistique-inférentielle.html#résidus" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La variance <span class="math inline">\(\sigma_u^2\)</span> des termes d’erreur <span class="math inline">\(u_i\)</span> n’est pas connue. Cependant, elle peut être estimée. Pour cela, on introduit la notion de <strong>résidu</strong>.</p>
<p>Le résidu <span class="math inline">\(\widehat{u}_i\)</span> est défini comme l’écart entre la vraie valeur <span class="math inline">\(Y_i\)</span> et sa prédiction <span class="math inline">\(\widehat{Y}_i=\widehat{a}X_i+\widehat{b}\)</span> :</p>
<p><span class="math display">\[\widehat{u}_i\equiv Y_i-\widehat{Y}_i\]</span> On a donc <span class="math display">\[\widehat{u}_i=Y_i-\widehat{a}X_i-\widehat{b}\]</span></p>
<p>Il s’agit d’une estimation (sans biais) de la vraie erreur</p>
<p><span class="math display">\[u_i=Y_i-aX_i-b\]</span></p>
<div class="thmbox thm">
<strong>Théorème (estimation de la variance) : </strong> <span class="math inline">\(\sigma_u^2\)</span>
</center>
<p>La variance <span class="math inline">\(\sigma_u^2\)</span> est estimée par</p>
<p><span class="math display">\[s^2=\frac{1}{n-2}\sum\limits_{i=1}^n \widehat{u}_i^2\]</span></p>
</div>
</div>
<div id="distributions-des-estimateurs-widehata-et-widehatb" class="section level4 hasAnchor" number="7.1.6.8">
<h4><span class="header-section-number">7.1.6.8</span> Distributions des estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span><a href="statistique-inférentielle.html#distributions-des-estimateurs-widehata-et-widehatb" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>On admet alors le résultat suivant</p>
<div class="thmbox thm">
<p><strong>Théorème :</strong> Sous l’hypothèse de normalité des termes d’erreur <span class="math inline">\(u_i\)</span>, on a</p>
<p><span class="math display">\[\frac{(n-2)s^2}{\sigma_u^2}\sim\chi^2_{(n-2)}\]</span> et les statistiques</p>
<p><span class="math display">\[\frac{\widehat{a}-a}{s\sqrt{\frac{1}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}}\]</span> et</p>
<p><span class="math display">\[\frac{\widehat{b}-b}{s\sqrt{\frac{1}{n}+\frac{\overline{X}_n^2}{\sum\limits_{i=1}^n (X_i-\overline{X}_n)^2}}}\]</span> suivent une loi de Student à <span class="math inline">\(n-2\)</span> degrés de liberté.</p>
</div>
</div>
<div id="convergence-des-estimateurs-widehata-et-widehatb" class="section level4 hasAnchor" number="7.1.6.9">
<h4><span class="header-section-number">7.1.6.9</span> Convergence des estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span><a href="statistique-inférentielle.html#convergence-des-estimateurs-widehata-et-widehatb" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Si on suppose que les <span class="math inline">\(X_i\)</span> admettent des moments d’ordre <span class="math inline">\(1\)</span> et <span class="math inline">\(2\)</span>, alors on peut montrer que les estimateurs <span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span> sont des estimateurs convergents.</p>
<p>On sait déjà qu’ils sont sans biais, il suffit donc de démontrer que leurs variances tendent vers <span class="math inline">\(0\)</span>.</p>
<p>Or, comme <span class="math inline">\(X_i\)</span> admet des moments d’ordres <span class="math inline">\(1\)</span> et <span class="math inline">\(2\)</span> on a, conditionnellement à <span class="math inline">\(X_1,\dots,X_n\)</span> :</p>
<p><span class="math display">\[\overline{X}_n\approx\mathbb{E}(X)\]</span> et <span class="math display">\[\sum\limits_{i=1}^n(X_i-\overline{X}_n)^2\approx n\mathbb{V}(X_1)\]</span> On en déduit que</p>
<p><span class="math display">\[\mathbb{V}(\widehat{a}|X_1,\dots, X_n)\approx\frac{\sigma_u^2}{n\mathbb{V}(X_1)}\longrightarrow 0\]</span></p>
<p>et</p>
<p><span class="math display">\[\mathbb{V}(\widehat{b}|X_1,\dots,X_n)\approx\sigma_u^2\left(\frac{1}{n}+\frac{\overline{X}_n^2}{n\mathbb{V}(X_1)}\right)\longrightarrow 0\]</span></p>
<p><span class="math inline">\(\widehat{a}\)</span> et <span class="math inline">\(\widehat{b}\)</span> sont des estimateurs sans biais de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span> de variances asymptotiquement nulles. Ce sont donc des estimateurs convergents de <span class="math inline">\(a\)</span> et <span class="math inline">\(b\)</span>.</p>
</div>
</div>
<div id="intervalles-de-confiance" class="section level3 hasAnchor" number="7.1.7">
<h3><span class="header-section-number">7.1.7</span> Intervalles de confiance<a href="statistique-inférentielle.html#intervalles-de-confiance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>jusqu’à présent : estimation ponctuelle</p></li>
<li><p>on veut être plus informatif et calculer la précision de cette estimation ponctuelle –&gt; intervalle de confiance</p></li>
<li><p>méthode de construction classique d’un intervalle de confiance : utiliser le TCL</p>
<ul>
<li>permet de se ramener à une hypothèse de quasi-normalité</li>
<li>donner plusieurs exemples</li>
</ul></li>
</ul>
</div>
</div>
<div id="tests-statistiques" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Tests statistiques<a href="statistique-inférentielle.html#tests-statistiques" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="définition-et-principes" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Définition et principes<a href="statistique-inférentielle.html#définition-et-principes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>approche très intuitive (d’après le programme du concours)</li>
</ul>
</div>
<div id="exemples-de-test" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Exemples de test<a href="statistique-inférentielle.html#exemples-de-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>le but est avant tout de montrer la démarche générale d’un test statistique à travers quelques exemples simples</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistique-descriptive.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/6_statistique_inférentielle.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Probabilites_Statistique.pdf", "Probabilites_Statistique.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
