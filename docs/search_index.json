[["dénombrement-et-probabilités.html", "Chapitre 2 Dénombrement et probabilités 2.1 Rappels sur les opérations ensemblistes 2.2 Dénombrement 2.3 Langage des probabilités", " Chapitre 2 Dénombrement et probabilités 2.1 Rappels sur les opérations ensemblistes Un ensemble est une collection d’objets, dont la nature peut être extrêment variée : des nombres, des droites, des matrices, etc. Pour \\(E\\) un ensemble, la notation \\(x\\in E\\) signifie que \\(x\\) est un élément de \\(E\\), et se lit x appartient à E. Inclusion, réunion, intersection, complémentaire Soient \\(E\\) et \\(F\\) deux ensembles quelconques. Ensemble vide. L’ensemble vide est l’ensemble n’admettant aucun élément. Inclusion. On dit que \\(F\\) est inclus dans \\(E\\), et on note \\(F\\subset E\\) si dès que \\(x\\) appartient à \\(F\\), il appartient aussi à \\(E\\) : \\[(F\\subset E) \\Leftrightarrow \\left((x\\in F)\\Rightarrow (x\\in E)\\right)\\] Réunion. \\(E\\cup F\\) est l’ensemble des éléments qui appartiennent à au moins l’un des deux ensembles \\(E\\) et \\(F\\) : \\[E\\cup F=\\{x, \\, x\\in E \\text{ ou } x\\in F\\}\\] Intersection. \\(E\\cap F\\) est l’ensemble des éléments qui appartiennent à la fois à \\(E\\) et à \\(F\\) : \\[E\\cap F=\\{x, \\, x\\in E \\text{ et } x\\in F\\}\\] Complémentaire. Si \\(F\\subset E\\), le complémentaire de \\(F\\) dans \\(E\\), noté \\(\\overline{F}^E\\) est défini par \\[\\overline{F}^E=\\{x\\in E, \\, x\\not\\in F\\}\\] Lorsque le contexte n’est pas ambigu, on allège la notation en l’écrivant plus simplement \\(\\overline{F}\\). D’autres notations existent : \\(F^c\\), \\(E\\backslash F\\), etc. Toutes les propriétés qu’on présente ici (sans démonstration) sont d’un usage très fréquent dans les calculs des probabilités. Commutativité de \\(\\cup\\) et \\(\\cap\\). Soient \\(A\\) et \\(B\\) deux ensembles. Alors : \\[A\\cup B=B\\cup A\\] \\[A\\cap B=B\\cap A\\] Associativité de \\(\\cup\\) et \\(\\cap\\). Soient \\(A, B, C\\) trois ensembles. Alors on a : \\[A\\cup(B\\cup C)=(A\\cup B)\\cup C\\] \\[A\\cup(B\\cap C)=(A\\cap B)\\cap C\\] Autrement dit, on peut calculer une succession de réunions ou d’intersections d’ensemble dans l’ordre qu’on veut, ce qui nous autorise à enlever les parenthèses : \\[A\\cup B\\cup C\\] \\[A\\cap B\\cap C\\] Plus généralement, étant donné une famille quelconque d’ensembles \\((E_i)_{i\\in I}\\), on peut définir de façon complètement analogue et sans amibuité leur réunion \\(\\bigcup\\limits_{i\\in I}E_i\\) et leur intersection \\(\\bigcap\\limits_{i\\in I}E_i\\) L’ensemble vide est un minorant de tout ensemble. Pour tout ensemble \\(E\\), on a : \\[\\emptyset\\subset E\\] L’ensemble vide est neutre pour la réunion. Pour tout ensemble \\(E\\), on a \\[E\\cup\\emptyset=\\emptyset\\cup E=E\\] L’ensemble vide est absorbant pour l’intersection. Pour tout ensemble \\(E\\), on a \\[E\\cap\\emptyset=\\emptyset\\cap E=\\emptyset\\] Le passage au complémentaire est involutif. Pour tout ensemble \\(E\\) et pour tout sous-ensemble \\(F\\) de \\(E\\), on a : \\[\\overline{\\overline{F}}=F\\] Les propriétés suivantes précisent comment la réunion, l’intersection et le passage complémentaire interagissent : Lois de Morgan. Pour tout ensemble \\(E\\), pour tout couple \\((A, B)\\) de sous-ensembles de \\(E\\), on a : \\[\\overline{A\\cup B}=\\overline{A}\\cap\\overline{B}\\] \\[\\overline{A\\cap B}=\\overline{A}\\cup\\overline{B}\\] Cette propriété se généralise à toute famille \\((F_i)_{i\\in, I}\\) d’un ensemble \\(E\\) quelconque : \\[\\overline{\\bigcup\\limits_{i\\in I} F_i}=\\bigcap\\limits_{i\\in I}\\overline{F_i}\\] \\[\\overline{\\bigcap\\limits_{i\\in I} F_i}=\\bigcup\\limits_{i\\in I}\\overline{F_i}\\] On peut résumer ces propriétés que l’opération de passage au complémentaire inverse la réunion et l’intersection. Distributivité. Soient \\(A, B, C\\) trois ensembles. Alors on a : \\[A\\cap(B\\cup C)=(A\\cap B)\\cup(A\\cap C)\\] \\[A\\cup(B\\cap C)=(A\\cup B)\\cap(A\\cup C)\\] 2.2 Dénombrement On commence par quelques éléments d’analyse combinatoire. Ces résultats sont utilisés dans le cas où, lors d’une expérience aléatoire, tous les événements élémentaires (on parle aussi d’issues) sont de même probabilité. On parle dans ce cas d’équiprobabilité ou d’équirépartition des résultats. Des exemples classiques de tirages équirépartis : lancer d’un dé à 6 faces non truqué. Dans ce cas, chaque face a une probabilité d’apparition de \\(\\frac{1}{6}\\). dans une urne composée de 10 boules rouges, 10 boules blanches et 10 boules noires, tirage au hasard d’une boule. Les trois couleurs que l’on peut obtenir sont équiprobables, de probabilité commune \\(\\frac{1}{3}\\). L’utilité du dénombrement dans le cas équiprobable vient de la formule que l’on apprend au lycée : \\[\\mathbb{P}(A)=\\frac{\\text{Nombre de cas favorables à } A}{\\text{Nombre total de cas}}\\] Cette formule, qui n’est valable que dans le cas équiréparti, suppose de savoir compter le nombre de cas où l’événement \\(A\\) se réalise ainsi que de savoir compter le nombre total d’issues de l’expérience aléatoire que l’on étudie, autrement dit il s’agit bien de savoir dénombrer. Cette première partie présente les types de dénombrements les plus classiques. Si les concepts sont très simples (on reste vraiment sur du niveau lycée), on peut assez facilement mal s’y prendre (ce qui voudra dire essentiellement : oublier de compter des cas, ou au contraire compter plusieurs fois le même cas) et passer complètement à côté du résultat. Bref, malgré les apparences, les questions de dénombrement (assez peu courantes au concours ces dernières années cela dit) sont potentiellement piégeuses… C’est donc typiquement le genre de questions qu’il ne faut pas sous-estimer et qu’il faut traiter en prenant son temps, à plus forte raison si elle est posée en début de sujet, et qu’elle est donc potentiellement structurante pour la suite. 2.2.1 Produit cartésien et principe multiplicatif Produit cartésien Soient \\(k\\) un entier naturel non nul et \\(E_1,\\dots E_k\\) des ensembles finis de cardinaux respectifs \\(n_1,\\dots, n_k\\). Le produit cartésien de \\(E_1,\\dots, E_k\\) est l’ensemble noté \\(E_1\\times\\dots\\times E_k\\) de toutes les listes ordonnées \\((x_1,\\dots,x_k)\\) telles que, pour tout \\(i\\in\\{1,\\dots, k\\}\\) l’élément numéro \\(i\\) noté \\(x_i\\) appartient à l’ensemble \\(E_i\\). On dit aussi que \\((x_1,\\dots, x_k)\\) est un k-uplet. Cas particuliers : i. Les listes ordonnées à 2 éléments sont appelées des couples, celles à 3 élements sont appelées des triplets et les listes à 4 éléments sont les quadruplets. ii. Lorsque tous les ensembles \\(E_1,\\dots E_k\\) sont égaux, i.e. \\(E_1=\\dots=E_k=E\\), le produit cartésien \\(E\\times\\dots\\times E\\) est noté plus simplement \\(E^k\\). Remarques. Le mot important dans cette définition est ordonnée. L’ordre a en effet une importance ici, autrement dit \\((1, 2)\\in\\mathbb{R}^2\\) et \\((2, 1)\\in\\mathbb{R}^2\\) sont bien considérés comme deux couples distincts de réels. Le premier résultat de ce cours, qui est absolument fondamental lorsqu’on pratique le dénombrement, et donc lorsqu’on est en situation d’équiprobabilité, est le principe multiplicatif. Ce principe, à la fois très simple et très intuitif, répond à la question Combien existe-t-il de k-uplets ? Principe multiplicatif : Avec les notations de la définition précédente, le nombre d’élements du produit cartésien \\(E_1\\times \\dots\\times E_k\\) est égal à \\(n_1\\times\\dots\\times E_k\\). Formule que l’on peut aussi écrire (en notant \\(\\text{Card}\\,(E)\\) le cardinal d’un ensemble \\(E\\), i.e. le nombre d’éléments de \\(E\\)) : \\[\\text{Card }(E_1\\times\\dots\\times E_k)=\\text{Card }(E_1)\\times\\dots\\times\\text{Card }(E_k)\\] Si on veut faire chic, on peut dire aussi que \\[\\textit{Le cardinal d&#39;un produit est le produit des cardinaux}\\] Exemple 1 : expériences aléatoires successives. On réalise successivement les deux expériences aléatoires suivantes : lancer d’une pièce : 2 résultats possibles P ou F ; puis lancer d’un dé cubique : 6 résultats possibles notés de 1 à 6. L’ensemble des issues de cette double expérience aléatoire peut être modélisé par le produit cartésien \\(\\{P, F\\}\\times\\{1,2,3,4,5,6\\}\\). C’est un ensemble à 12 éléments : \\[(P,1), (P,2), (P,3), (P, 4), (P,5), (P, 6), (F,1), (F,2), (F,3), (F, 4), (F,5), (F, 6)\\] Cet ensemble peut facilement être représenté par un arbre de dénombrement : Exemple 2 : compter des poignées de mains. Soit \\(n\\geq 2\\) un entier. Compter le nombre de poignées de mains possibles dans un groupe de \\(n\\) personnes. Solution 2.a. On note \\(1,\\dots n\\) les personnes de ce groupe, et le couple \\((i, j)\\) modélise le fait que \\(i\\) sert la main de \\(j\\). On compte alors successivement : le nombre de couples : il y en a \\(n^2\\) d’après le principe multiplicatif ; le nombre de couples \\((i, i)\\) qui représenteraient le fait que \\(i\\) se sert la man à lui-même, situtation qu’on ne veut pas dénombrer et dont il faut donc soustraire le nombre d’occurences au nombre précédent. Il y en a \\(n\\), donc il y a \\(n^2-n=n(n-1)\\) façons qu’une personne \\(i\\) serre la main d’une autre personne \\(j\\) du goupe ; le nombre de couples correspondant à une poignée de mains : il s’agit de \\(2\\), puisque une poignée de mains entre deux personnes \\(i\\) et \\(j\\) correspond à exactement deux couples : \\((i,j)\\) et \\((j,i)\\). Il faut donc diviser par deux le nombre trouvé précédemment, ce qui fait un total de \\(\\frac{n(n-1)}{2}\\) poignées de mains. Solution 2.b.(plus directe) Une poignée de mains implique deux personnes différentes. On a \\(n\\) choix possibles pour la première personne, et \\(n-1\\) choix possibles pour la deuxième personne, soit \\(n(n-1)\\) choix de couples possibles. En l’état, on compte deux fois trop de poignées de mains (même raisonnement que dans l’exemple 2.a.), donc il y a en réalité \\(\\frac{n(n-1)}{2}\\) poignées de mans possibles.$ 2.2.2 Principe additif Un deuxième grand principe de dénombrement, tout aussi intuitif et tout aussi fondamental, est le principe additif : Principe additif : Si \\(E_1,\\dots E_k\\) sont \\(k\\) ensembles deux à deux disjoints (i.e. \\(i\\neq j\\Rightarrow E_i\\cap E_j=\\emptyset\\)) alors (en reprenant les mêmes notations que dans la définition précédente) leur union \\(E_1\\cup\\dots\\cup E_k\\) a pour cardinal \\(n_1+\\dots+n_k\\), ce que l’on peut aussi écrire : \\[\\text{Card }\\left(E_1\\cup\\dots\\cup E_k\\right)=\\text{Card }(E_1)+\\dots+\\text{Card }(E_k)\\] On pourra retenir que : \\[\\textit{Le cardinal d&#39;une union disjointe est la somme des cardinaux}\\] Remarque. Lorsqu’un ensemble \\(E\\) peut s’écrire sous la forme \\[E=E_1\\cup E_2\\cup\\dots\\cup E_k\\] avec \\(E_1, E_2, \\dots, E_k\\) deux à deux disjoints, on dit que \\(E_1, E_2,\\dots E_k\\) forment une partition de l’ensemble \\(E\\). Exemple 3 : compter des carrés. Combien la figure suivante compte-t-elle de carrés ? Solution. Tout carré de cette figure a pour côté 1, 2, 3 ou 4 (en supposant avoir fixé une unité de longueur, correspondant au côté d’un “petit” carré). L’ensemble \\(E\\) des carrés de cette figure peut donc s’écrire \\[E=E_1\\cup E_2\\cup E_3\\cup E_4\\] où, pour \\(k\\in\\{1,2,3,4\\}\\), \\(E_k\\) désigne l’ensemble des carrés de côté \\(k\\) de cette figure. Les \\(E_k\\) sont deux à deux disjoints (un carré de la figure ne peut pas avoir un côté de deux longueurs différentes) et donc pour compter le nombre de carrés possibles, il suffit de compter les carrés de côté 1, de côté 2, de côté 3, de côté 4 et d’ajouter tous ces nombres. On trouve : \\[\\begin{align} \\text{Card }(E_1) &amp;= 16 \\\\ \\text{Card }(E_2) &amp;= 9 \\\\ \\text{Card }(E_3) &amp;= 4 \\\\ \\text{Card }(E_4) &amp;= 1 \\\\ \\end{align}\\] d’où \\(\\text{Card }(E)=16+9+4+1=30\\). Exemple 4 (poignées de mains, à nouveau). On peut répondre à cette question en utilisant le principe additif. On note \\(E_1\\) l’ensemble des poignées de mains de la personne \\(1\\). Puis on note \\(E_2\\) l’ensemble des poignées de mains de la personne \\(2\\), hormis celle avec la personne \\(1\\) qui a déjà été comptée, \\(E_3\\) l’ensemble des poignées de mains de la personne \\(3\\) hormis celles avec les personnes \\(1\\) et \\(2\\) qui ont déjà été comptées, et ainsi de suite jusqu’à \\(E_{n}\\). Alors, \\(E_1,E_2,\\dots, E_n\\) forment une partition de l’ensemble \\(E\\) de toutes les poignées de mains du groupe, on va donc compter les cardinaux de \\(E_1,E_2,\\dots, E_n\\) et utiliser le principe additif : la personne \\(1\\) serre la main des \\(2\\) à \\(n\\), donc \\(E_1=n-1\\) ; les poignées de mains non encore comptées de la personne \\(2\\) sont celles avec les personnes \\(3\\) à \\(n\\) donc \\(E_2=n-2\\) ; les poignées de mains non encore comptées de la personne \\(3\\) sont celles avec les personnes \\(4\\) à \\(n\\) donc \\(E_2=n-3\\) ; … une seule poignée de mains n’a pas été comptée pour la personne \\(n-1\\), celle avec la personnes \\(n\\) donc \\(E_{n-1}=1\\) ; enfin, toutes les poignées de mains de la personne \\(n\\) ont été comptées, donc \\(E_n=0\\). En vertu du principe additif, le nombre de poignées de mains est donc égal à \\[0+1+2+\\dots+(n-1)=\\frac{n(n-1)}{2}\\] 2.2.3 Formule de Poincaré Il existe une formule lorsqu’on relâche l’hypothèse de non-djsjonction deux à deux, connue sous le nom de formule de Poincaré. C’est une formule que l’on utilise souvent dans le cas \\(k=2\\), de temps en temps dans le cas \\(k=3\\), plus rarement (mais ça peut arriver dans un sujet du concours) dans le cas \\(k\\geq 4\\) voire dans le cas général. Formule de Poincaré. Soit \\(E\\) un ensemble fini, qui peut s’écrire sous la forme \\[E=E_1\\cup E_2\\cup\\dots\\cup E_k\\] avec \\(E_1,E_2,\\dots E_k\\) des sous-ensembles de \\(E\\) non nécessairement disjoints deux à deux. Alors : \\[\\text{Card }(E)=\\sum\\limits_{i=1}^k (-1)^i \\sum\\limits_{1\\leq j_1 &lt; j_2 &lt;...&lt; j_i\\leq n} \\text{Card }\\left(E_{j_1}\\cap E_{j_2}\\cap\\dots\\cap E_{j_i}\\right)\\] Cas particulier \\(k=2\\) : \\[\\text{Card }(A\\cup B)=\\text{Card }(A)+\\text{Card }(B)-\\text{Card }(A\\cap B)\\] Cas particulier \\(k=3\\) : \\[\\begin{align} \\text{Card }(A\\cup B\\cup C)&amp;=\\text{Card }(A)+\\text{Card }(B)+\\text{Card }(C) \\\\ &amp;-\\text{Card }(A\\cap B)-\\text{Card }(A\\cap C)-\\text{Card }(B\\cap C) \\\\ &amp;+\\text{Card }(A\\cap B\\cap C) \\end{align}\\] Démonstration dans le cas \\(k=2\\). L’intutition de la démonstration est évidente : pour compter tout ce qu’il y a dans la réunion de \\(A\\) et \\(B\\), on compte tout ce qu’il y a dans \\(A\\), tout ce qu’il y a dans \\(B\\) et on ajoute le tout. Mais en faisant cela, on compte deux fois - c’est-à-dire une fois de trop - tout ce qui est à la fois dans \\(A\\) et dans \\(B\\), donc on doit ensuite soustraire l’excédent. Plus formellement, on écrit \\(A\\cup B\\) comme une union disjointe, puis on applique le principe additif. On a, en posant \\(A\\backslash B=A\\cap\\overline{B}\\) et \\(B\\backslash A=B\\cap\\overline{A}\\) : \\[A\\cup B = (A\\backslash B)\\cup(A\\cap B)\\cup (B\\backslash A)\\] Les trois ensembles du membre de droite de cette égalité dont deux à deux disjoints, donc on peut appliquer le principe additif : \\[\\text{Card }(A\\cup B)=\\text{Card }(A\\backslash B)+\\text{Card }(A\\cap B)+\\text{Card }(B\\backslash A)\\] Par ailleurs \\[A=(A\\backslash B)\\cup (A\\cap B)\\] \\[B=(B\\backslash A)\\cup (B\\cap A)\\] avec à nouveau des ensembles deux à disjoints dans les membres de droite. Donc, en appliquant deux fois le principe additif \\[\\text{Card }(A\\backslash B)=\\text{Card }(A)-\\text{Card }(A\\cap B)\\] \\[\\text{Card }(B\\backslash A)=\\text{Card }(B)-\\text{Card }(A\\cap B)\\] et donc finalement \\[\\begin{align} \\text{Card }(A\\cup B)&amp;=\\text{Card }(A\\backslash B)+\\text{Card }(A\\cap B)+\\text{Card }(B\\backslash A) \\\\ &amp;=\\text{Card }(A)-\\text{Card }(A\\cap B)+\\text{Card }(A\\cap B)+\\text{Card }(B)-\\text{Card }(A\\cap B) \\\\ &amp;=\\text{Card }(A)+\\text{Card }(B)-\\text{Card }(A\\cap B) \\end{align}\\] \\(\\square\\) 2.2.4 Dénombrement par bijection Une technique classique de dénombrement d’un ensemble fini \\(E\\) est l’utilisation d’un bijection entre \\(E\\) et un ensemble fini \\(F\\) dont on connaît le cardinal : Conservation du cardinal par bijection. Soient \\(E\\) un ensemble, \\(F\\) un ensemble fini. S’il existe une bijection \\(\\varphi : E\\longrightarrow F\\), alors \\(E\\) est un ensemble fini et il est de même cardinal que \\(F\\). Application classique : nombre de parties d’un ensemble fini. Soit \\(E\\) un ensemble de cardinal \\(n\\). Alors l’ensemble \\(\\mathcal{P}(E)\\) des parties de \\(E\\) (i.e. l’ensemble de tous les sous-ensembles de \\(E\\), y compris l’ensemble vide et \\(E\\) lui-même) est égal à \\(2^n\\) : \\[\\text{Card }(E)=n\\Rightarrow\\text{Card }(\\mathcal{P}(E))=2^n\\] En effet, notons \\(\\{0,1\\}^E\\) l’ensemble de toutes les fonctions possibles de \\(E\\) dans \\(\\{0,1\\}\\). Soit alors \\(\\varphi\\) l’application \\(\\varphi:\\{0,1\\}^E\\longrightarrow\\mathcal{P}(E)\\) définie, pour toute fonction \\(f\\in\\{0,1\\}^E\\) par \\[\\varphi(f)=\\{x\\in E, \\,f(x)=1\\}\\] L’application \\(\\varphi\\) est bien définie (si \\(f=g\\) on a clairement \\(\\varphi(f)=\\varphi(g)\\)). Elle est injective : si \\(\\varphi(f)=\\varphi(g)\\), alors \\(f(x)=1\\) si et seulement si \\(g(x)=1\\), et donc \\(f\\) et \\(g\\) ne pouvant prendre que \\(0\\) et \\(1\\) comme valeurs on en déduit que \\(f=g\\). Enfin, \\(\\varphi\\) est surjective. En effet, si \\(P\\in\\mathcal{P}(E)\\), alors en posant, pour tout \\(x\\) dans \\(E\\), \\(f(x)=1\\) si \\(x\\in P\\) et \\(f(x)=0\\) si \\(x\\not\\in P\\), on a \\(f\\in\\{0,1\\}^E\\) et \\(\\varphi(f)=P\\). Enfin, en vertu du principe multiplicatif on a \\(\\text{Card }(\\{0,1\\}^E)=2^n\\). On en déduit que \\(\\text{Card }(\\mathcal{P}(E))=2^n\\). \\(\\square\\) Intutivement, la démonstration précédente montre que toute partie \\(P\\) d’un ensemble fini \\(E\\) peut être codé de façon unique en une fonction sur \\(E\\) à valeurs dans \\(\\{0,1\\}\\), prenant la valeur \\(1\\) pour les éléments de \\(P\\) et \\(0\\) pour les élements qui ne sont pas dans \\(P\\). 2.2.5 Permutations Permutation d’un ensemble fini Soient \\(n\\) un entier naturel non nul et \\(E=\\{x_1,\\dots, x_n\\}\\)} un ensemble fini à \\(n\\) éléments. On appelle permutation de \\(E\\) tout réarragement ordonné et sans répétition des éléments de \\(E\\). De façon équivalente, une permutation est une bijection de \\(E\\) dans lui-même. Notation. L’ensemble des permutations d’un ensemble fini \\(E\\) est noté \\(\\mathfrak{S}(E)\\). Dans le cas particulier où \\(E=\\{1,2,\\dots, n\\}\\), on le note plus simplement \\(\\mathfrak{S}_n\\). Exemple. \\(E=\\{a,b,c\\}\\), les permutations de \\(E\\) sont \\[(a,b,c), (a,c,b), (b,a,c), (b,c,a), (c,a,b), (c,b,a)\\] Ces permutations peuvent aussi s’écrire comme des bijections \\(\\sigma_1,\\dots,\\sigma_6\\) de \\(E\\) dans lui-même : \\[\\sigma_1(a)=a,\\,\\sigma_1(b)=b,\\,\\sigma_1(c)=c\\] \\[\\sigma_2(a)=a,\\,\\sigma_2(b)=c,\\,\\sigma_2(c)=b\\] \\[\\sigma_3(a)=b,\\,\\sigma_3(b)=a,\\,\\sigma_3(c)=c\\] \\[\\sigma_4(a)=b,\\,\\sigma_4(b)=c,\\,\\sigma_4(c)=a\\] \\[\\sigma_5(a)=c,\\,\\sigma_5(b)=a,\\,\\sigma_5(c)=b\\] \\[\\sigma_6(a)=a,\\,\\sigma_6(b)=b,\\,\\sigma_6(c)=a\\] Le nombre de permutations d’un ensemble fini est facile à dénombrer : Théorème (nombre de permutations) : Soit \\(n\\) un entier naturel non nul. Le nombre de permutations d’un ensemble fini à \\(n\\) éléments est égal à \\(n!=1\\times 2\\times 3\\times\\dots\\times n\\). Démonstration. On note \\(x_1,\\dots,x_n\\) les éléments d’un ensemble de cardinal \\(n\\). Choisir une permutation \\(\\sigma\\) de \\(E\\), c’est choisir successivement : l’image \\(\\sigma(x_1)\\) parmi les \\(n\\) éléments \\(x_1,\\dots,x_n\\) : \\(n\\) choix possibles ; l’image \\(\\sigma(x_2)\\) parmi les \\(n-1\\) éléments restants : \\(n-1\\) choix possibles ; l’image \\(\\sigma(x_3)\\) parmi les \\(n-2\\) éléments restants : \\(n-2\\) choix possibles ; … l’image \\(\\sigma(x_n)\\) parmi le seul élément de \\(x_1,\\dots, x_n\\) qui n’a pas encore été choisi : \\(1\\) seul choix possible. D’après le principe multiplicatif, le nombre de permutations de \\(E\\) est donc égal à \\[n\\times(n-1)\\times(n-2)\\times\\dots\\times 1=n!\\] \\(\\square\\) Remarque. En filigrane, le théorème précédént dit aussi que le nombre de permutations d’un ensemble fini \\(E\\) ne dépend de \\(E\\) qu’à travers son cardinal. Autrement dit, peu importe l’ensemble \\(E\\) que l’on choisit, dès lors qu’il a \\(n\\) éléments le nombre de permutations de cet ensemble est \\(n!\\). Ce résultat est une simple conséquence du principe de dénombrement par bijection évoqué plus haut. Exemple : nombre d’anagrammes. Quel est le nombre d’anagrammes du mot MATHS ? Du mot ANAGRAMME ? Solution. i. Une anagramme du mot MATHS correspond à une permutation de l’ensemble \\(\\{M,A,T,H,S\\}\\). Il y en a donc \\(4!=24\\). ii. Pour le mot ANAGRAMME c’est un peu plus compliqué car certaines lettres apparaissent plusieurs fois. On commence par numéroter ces lettres-là, en les traitant comme des lettres différentes, autrement dit on commence par compter le nombre de permutations de l’ensemble \\(\\{A_1,N,A_2,G,R,A_3,M_1,M_2,E\\}\\) : il y en a \\(8!\\). Les lettres n’étant en réalité pas numérotées dans notre problème, il n’y a pas lieu de distinguer, par exemple, l’anagramme \\(A_1NA_2GRA_3M_1M_2E\\) de l’anagramme \\(A_2NA_1GRA_3M_1M_2E\\). Ainsi, la lettre \\(A\\) étant de multiplicité \\(3\\) dans le mot ANAGRAMME, chacune des \\(3!=6\\) permutations de cette lettre fournit exactement le même mot, de sorte que la numérotation de la lettre \\(A\\) conduit à compter \\(6\\) fois plus de permutations qu’il n’y en a en réalité. De même, la lettre \\(M\\) est de multiplicité \\(2\\), et donc en la numérotant on compte \\(2!=2\\) fois plus de permutations qu’il y en a réellement. Finalement, on en déduit que le nombre d’anagrammes du mot ANAGRAMME est égal à \\(\\frac{8!}{3!2!}=420\\). 2.2.6 Arrangements Arrangements Soient \\(n\\) et \\(0\\leq k\\leq n\\) deux entiers naturels, et \\(E\\) un ensemble à \\(n\\) éléments. On appelle arrangement de \\(k\\) éléments pris parmi les \\(n\\) éléments de \\(E\\), tout sous-ensemble ordonné à \\(k\\) éléments de \\(E\\). De la même façon qu’on peut définir les permutations par la notion de bijection, on peut définir les arrangements par la notion d’injection : Définition équivalente des arrangements Soient \\(n\\) et \\(0\\leq k\\leq n\\) deux entiers naturels, et \\(E\\) un ensemble à \\(n\\) éléments. Un arrangement de \\(k\\) éléments pris parmi les \\(n\\) éléments de \\(E\\) peut aussi être vu comme une injection de \\(\\{1,2,\\dots, k\\}\\) dans \\(E\\). Exemple. Si \\(E=\\{1,2,3\\}\\), les arrangements à \\(2\\) éléments de \\(E\\) sont les couples \\((1,2), (1,3), (2,1), (2,3), (3, 1), (3,2)\\). On utilise bien la notion de couple pour modéliser les arrangements car l’ordre a une importance : les arrangements \\((1,2)\\) et \\((2,1)\\) sont bien considérés comme différents. Il y a donc six arrangements de \\(2\\) éléments de \\(E\\). Plus généralement, on a une formule qui permet de calculer le nombre d’arrangements de \\(k\\) éléments pris parmi \\(n\\) éléments : Théorème (nombre d’arrangements). On note \\(A_n^k\\) le nombre d’arrangements à \\(k\\) éléments pris dans un ensemble à \\(n\\) éléments. Alors, on a la formule : \\[A_n^k=\\frac{n!}{(n-k)!}\\] Remarque. Ici aussi, le nombre d’arrangements à \\(k\\) éléments pris dans un ensemble à \\(n\\) éléments ne dépend que de \\(n\\) (et de \\(k\\)). Démonstration. C’est exactement la même démarche que pour le dénombrement des permutations : \\(n\\) façons de choisir le premier élément ; \\(n-1\\) façons de choisir le deuxième élément ; … \\(n-k+1\\) façons de choisir l’élément numéro \\(k\\). D’après le principe multiplicatif on a donc : \\[\\begin{align} A_n^k &amp;= n(n-1)\\dots(n-k+1) \\\\ &amp;= \\frac{n!}{(n-k)!} \\end{align}\\] \\(\\square\\) Remarque. Dans le cas où \\(k=n\\), on a \\(A_n^n=n!\\). Ce résultat était prévisible, puisqu’un arrangement de \\(n\\) éléments parmi \\(n\\) éléments est une injection de \\(\\{1,2,\\dots, n\\}\\) dans lui-même, autrement dit une bijection de \\(\\{1,2,\\dots,n\\}\\) dans lui-même. Il s’agit donc d’une permutation de \\(\\{1,2,\\dots,n\\}\\). 2.2.7 Combinaisons Les combinaisons sont l’équivalent non ordonné des arrangements : Combinaisons Soient \\(n\\) et \\(0\\leq k\\leq n\\) deux entiers naturels, et \\(E\\) un ensemble à \\(n\\) éléments. On appelle combinaison de \\(k\\) éléments pris parmi les \\(n\\) éléments de \\(E\\), tout sous-ensemble non ordonné à \\(k\\) éléments de \\(E\\). Exemples. Si \\(E=\\{1,2,3\\}\\), on a vu que les arrangements à \\(2\\) éléments de \\(E\\) sont les couples \\[(1,2), (1,3), (2,1), (2,3), (3, 1), (3,2)\\] Du point de vue des combinaisons, les couples \\((1,2)\\) et \\((2,1)\\) (resp. \\((1,3)\\) et \\((3,1)\\), \\((2,3)\\) et \\((3,2)\\)) sont considérés comme équivalents. Il y a donc trois combinaisons à \\(2\\) éléments de \\(E\\) : \\[\\{1,2\\}, \\{1,3\\} \\text{ et } \\{2,3\\}\\] Remarque. Bien faire attention à la différence de notation : la notation avec parenthèses \\((a_1, a_2,\\dots, a_n)\\) désigne un \\(n-\\)uplet, c’est-à-dire un objet ordonné. Alors que la notation ensembliste \\(\\{a_1,a_2,\\dots a_n\\}\\) désigne un objet non ordonné. On a donc, pour toute permutation \\(\\sigma\\in\\mathfrak{S}_n\\) différente de l’identité : \\[(a_{\\sigma(1)},a_{\\sigma(Z)},\\dots, a_{\\sigma_(n)})\\neq (a_1,a_2,\\dots, a_n)\\] mais \\[\\{a_{\\sigma(1)},a_{\\sigma(Z)},\\dots, a_{\\sigma_(n)}\\}=\\{a_1,a_2,\\dots, a_n\\}\\] Comme pour les permutations et les arrangements, on a une formule simple pour compter les combinaisons : Théorème (nombre de combinaisons). Soient \\(n\\) et \\(0\\leq k\\leq n\\) des entiers naturels. Le nombre de combinaisons de \\(k\\) éléments pris parmi les \\(n\\) éléments d’un ensemnble \\(E\\) quelconque est noté \\(C_n^k\\) ou \\(\\binom{n}{k}\\). Ce nombre, appelé coefficient binomial, est égal à : \\[\\binom{n}{k}=\\frac{n!}{k!\\,(n-k)!}\\] On peut aussi définir de façon cohérente le coefficient binomial pour \\(k\\) et \\(n\\) des entiers naturels avec \\(k&gt;n\\) en posant \\[\\binom{n}{k}=0\\] Démonstration. On commence par compter les arrangements de \\(k\\) éléments parmi \\(n\\) : il y en a \\(A_n^k=\\frac{n!}{(n-k)!}\\). Par ailleurs, toute combinaison \\((a_1,\\dots, a_k)\\) de \\(k\\) éléments parmi \\(n\\) génère \\(k!\\) arrangements distincts \\((a_{\\sigma(1)},\\dots, a_{\\sigma(k)})\\) distincts \\((\\sigma\\in\\mathfrak{S}_k)\\). Il y a donc \\(k!\\) fois plus d’arrangements que de combinaisons. D’où : \\[\\begin{align} \\binom{n}{k} &amp;= \\frac{A_n^k}{k!} \\\\ &amp;= \\frac{n!}{k!(n-k)!} \\end{align}\\] \\(\\square\\) Plusieurs formules impliquent les combinaisons : Formules usuelles sur les combinaisons. Soient \\(n\\) et \\(0\\leq k\\leq n\\) deux entiers naturels. Alors : i. (Cas particuliers) \\[\\binom{n}{0}=1\\] \\[\\binom{n}{n}=1\\] \\[\\binom{n}{1}=n\\] \\[\\binom{n}{2}=\\frac{n(n-1)}{2}\\] ii. (Complémentaire) \\[\\binom{n}{k}=\\binom{n}{n-k}\\] iii. (Triangle de Pascal) \\[\\binom{n}{k}+\\binom{n}{k+1}=\\binom{n+1}{k+1}\\] iv. (Formule du binôme de Newton) Sous la convention \\(0^0=1\\), pour \\(a\\) et \\(b\\) des réels quelconques et \\(n\\) un entier naturel : \\[(a+b)^n=\\sum\\limits_{k=0}^n \\binom{n}{k}a^k b^{n-k}\\] (si on rejette la convention \\(0^0=1\\), alors la formule est toujours vraie sauf dans le cas où \\(n=0\\) et \\(a=-b\\)). En particulier, pour \\(a=b=1\\) on obtient : \\[\\sum\\limits_{k=0}^n \\binom{n}{k}=2^n\\] Pour \\(a=-1\\) et \\(b=1\\) on obtient : \\[\\sum\\limits_{k=0}^n (-1)^k\\binom{n}{k}=0\\] Pour \\(a=x\\) et \\(b=1\\) on obtient : \\[\\sum\\limits_{k=0}^n \\binom{n}{k}x^k=(x+1)^n\\] Pour \\(a=-1\\) et \\(b=x\\) on obtient : \\[\\sum\\limits_{k=0}^n \\binom{n}{k}(-1)^kx^{n-k}=(x-1)^n\\] Démonstration. i. \\(\\binom{n}{0}=\\frac{n!}{0!(n-0)!}=\\frac{n!}{n!}=1\\) L’égalité \\(\\binom{n}{n}=1\\) est une conséquence de la formule précédente et de la formule ii. qui va être montrée après. \\(\\binom{n}{1}=\\frac{n!}{1!(n-1)!}=\\frac{n(n-1)!}{(n-1)!}=n\\) \\(\\binom{n}{2}=\\frac{n!}{2! (n-2)!}=\\frac{n(n-1)(n-2)!}{2(n-2)!}=\\frac{n(n-1)}{2}\\) Autre méthode. Soit \\(E=\\{1,2, \\dots, n\\}\\). Le seul sous-ensemble de \\(E\\) à zéro élément est \\(\\emptyset\\), donc \\(\\binom{n}{0}=1\\). Le seul sous-ensemble de \\(E\\) à \\(n\\) éléments est \\(E\\) lui-même, donc \\(\\binom{n}{n}=1\\). Les seuls sous-ensembles de \\(E\\) à un élément sont les \\(n\\) singletons \\(\\{1\\}, \\{2\\}, \\dots, \\{n\\}\\), donc \\(\\binom{n}{1}=n\\). Enfin, les sous-ensembles à deux éléments de \\(E\\) sont : \\[\\begin{align} &amp;\\{1,2\\}, \\{1,3\\}, \\dots, \\{1,n\\} \\\\ &amp;\\{2,3\\}, \\dots ,\\{2,n\\} \\\\ &amp;\\dots \\\\ &amp;\\{n-1, n\\} \\end{align}\\] Il y en a donc \\(n+(n-1)+\\dots+1=\\frac{n(n-1)}{2}\\) ii. \\[\\begin{align} \\binom{n}{k}&amp;=\\frac{n!}{k!(n-k)!} \\\\ &amp;=\\frac{n!}{(n-k)! k!} \\\\ &amp;=\\frac{n!}{(n-k)!(n-(n-k))!} \\\\ &amp;=\\binom{n}{n-k} \\end{align}\\] Autre méthode. Choisir un sous-ensemble \\(F\\subset E\\) à \\(k\\) éléments revient à choisir son complémentaire \\(\\overline{F}\\) dans \\(E\\), qui contient \\(n-k\\) éléments. Donc \\(\\binom{n}{k}=\\binom{n}{n-k}\\). iii. \\[\\begin{align} \\binom{n}{k}+\\binom{n}{k+1}&amp;=\\frac{n!}{k!(n-k)!}+\\frac{n!}{(k+1)!(n-k-1)!} \\\\ &amp;=\\frac{n!(k+1)}{(k+1)!(n-k)!}+\\frac{n!(n-k)}{(k+1)!(n-k)!} \\\\ &amp;=\\frac{n!(k+1+n-k)}{(k+1)!(n-k)!} \\\\ &amp;=\\frac{n!(n+1)}{(k+1)!(n-k)!} \\\\ &amp;=\\frac{(n+1)!}{(k+1)!((n+1)-(k+1))!} \\\\ &amp;=\\binom{n+1}{k+1} \\end{align}\\] Autre méthode. On sépare les sous-ensembles à \\(k+1\\) éléments de \\(E=\\{1,2,\\dots, n, n+1\\}\\) en deux parties disjointes : les sous-ensembles \\(F\\) qui contiennent \\(n+1\\). Ils sont de la forme \\(\\{x_1,\\dots, x_k\\}\\cup\\{n+1\\}\\), et il y en a donc autant que de façons de choisir un sous-ensemble à \\(k\\) éléments \\(\\{x_1,\\dots, x_k\\}\\) de l’ensemble \\(E&#39;=\\{1,2,\\dots, n\\}\\), i.e. il y en a exactement \\(\\binom{n}{k}\\). les sous-ensembles \\(F\\) qui ne contiennent pas \\(n+1\\). Ils s’écrivent donc sous la forme \\(F=\\{x_1,x_2,\\dots, x_{k+1}\\}\\), avec les \\(x_{k+1}\\) pris dans \\(E&#39;=\\{1,2,\\dots, n\\}\\). Il y en a donc exactement \\(\\binom{n}{k+1}\\). Commes ces deux parties sont disjointes, on peut appliquer le principe additif, pour affirmer que le nombre de sous-ensembles à \\(k+1\\) éléments d’un ensemble à \\(n+1\\) éléments est égal à \\(\\binom{n}{k}+\\binom{n}{k+1}\\). Par ailleurs, le nombre de sous-ensembles à \\(k+1\\) éléments d’un ensemble à \\(n+1\\) éléments est égal à \\(\\binom{n+1}{k+1}\\) (par défintion des coefficients binomiaux). D’où l’égalité \\(\\binom{n}{k}+\\binom{n}{k+1}=\\binom{n+1}{k+1}\\). iv. On montre la formule par récurrence sur \\(n\\). Pour \\(n=0\\), cette formule s’écrit \\((a+b)^0=\\binom{0}{0}a^0b^0\\), soit \\(1=1\\) (sous la convention \\(0^0=1\\), on a quel que soit \\(x\\) réel, \\(x^0=1\\)). Supposons la formule établie pour un entier naturel \\(n\\) donné. Alors : \\[\\begin{align} (a+b)^{n+1} &amp;= (a+b)(a+b)^n \\\\ &amp;= (a+b).\\sum\\limits_{k=0}^n \\binom{n}{k} a^k b^{n-k} \\\\ &amp;\\text{(d&#39;après l&#39;hypothèse de récurrence)} \\\\ &amp;= \\sum\\limits_{k=0}^n \\binom{n}{k} a^{k+1} b^{n-k}+\\sum\\limits_{k=0}^n \\binom{n}{k} a^{k} b^{n-k+1} \\\\ &amp;=\\sum\\limits_{k=1}^{n+1} \\binom{n}{k-1} a^{k} b^{n-k+1}+\\sum\\limits_{k=0}^n \\binom{n}{k} a^{k} b^{n-k+1} \\\\ &amp;=\\binom{n}{n} a^{n+1}+\\sum\\limits_{k=1}^n \\left(\\binom{n}{k-1}+\\binom{n}{k}\\right) a^kb^{n-k+1}+\\binom{n}{0}b^{n+1} \\\\ &amp;=a^{n+1}+\\sum\\limits_{k=1}^n \\binom{n+1}{k} a^k b^{n+1-k} +b^{n+1} \\\\ &amp;\\text{(d&#39;après la formule du triangle de Pascal)} \\\\ &amp;= \\sum\\limits_{k=0}^{n+1}\\binom{n+1}{k} a^k b^{n+1-k} \\\\ \\end{align}\\] qui est la formule attendue au rang \\(n+1\\). Autre méthode. En développant le produit \\[(a+b)^n=(a+b).(a+b)\\dots (a+b)\\] on obtient une somme de termes de la forme \\(a^kb^{n-k}\\). Pour obtenir un tel terme, on doit choisir \\(k\\) fois le terme \\(a\\) parmi les \\(n\\) facteurs \\((a+b)\\) (et donc, de façon complémentaire, \\(n-k\\) fois le terme \\(b\\) parmi ces mêmes facteurs). On en déduit, par définition des coefficients binomiaux, que le terme \\(a^kb^{n-k}\\) apparaît exactement \\(\\binom{n}{k}\\) fois dans la somme. Autrement dit on a bien \\((a+b)^n=\\sum\\limits_{k=0}^n \\binom{n}{k}a^k b^{n-k}\\). \\(\\square\\) Remarques. i. Comme souvent en analyse combinatoire, la démonstration d’une égalité peut se faire soit par le calcul, soit en utilisant une approche de dénombrement pur (qui en général est plus élégante mais peut-être un peu moins évidente à trouver). ii. La deuxième démonstration de la formule du triangle de Pascal repose sur une approche classique en dénombrement : compter la même chose de deux façons différentes. 2.3 Langage des probabilités On introduit maintenant les notions de base des probabilités. Le but est de construire progressivement un triplet de la forme \\[(\\Omega, \\mathcal{A},\\mathbb{P})\\] où : \\(\\Omega\\) s’appellera l’univers ; \\(\\mathcal{A}\\) s’appellera une tribu : dans le cadre du programme c’est en fait un mot que nous n’utiliserons quasiment jamais, et sauf cas particulier nous n’expliciterons pas cet élément ; \\(\\mathbb{P}\\) s’appellera une probabilité. On va donc construire successivement : l’univers \\(\\Omega\\) ; l’espace probablisable \\((\\Omega, \\mathcal{A})\\), qui est un enrichissement de \\(\\Omega\\) ; l’espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\), qui est un enrichissement de \\((\\Omega, \\mathcal{A})\\). 2.3.1 L’univers \\(\\Omega\\) d’une expérience aléatoire Tout commence avec la notion d’expérience aléatoire, i.e. une expérience dont l’issue est incertaine. La première chose à faire est de dresser la liste de tous les résultats potentiellement réalisables à lorsqu’on effectue une telle expérience. Un tel résultat s’appelle une issue ou un événement élémentaire. L’ensemble des issues est appelé univers de l’expérience, il est généralement noté \\(\\Omega\\). Exemples. i. Lancer d’un dé cubique. On note ses faces par des entiers de \\(1\\) à \\(6\\). Dans ce cas, l’univers \\(\\Omega\\) est fini et \\[\\Omega=\\{1,2,3,4,5,6\\}\\] ii. Echantillonnage d’une population. Afin d’estimer les revenus moyens au sein d’une population \\(\\mathcal{U}=\\{1,2,\\dots, N\\}\\), on tire un échantillon aléatoire de taille \\(n\\) au sein de cette population. Pour l’expérience aléatoire consistant à tirer un tel échantillon, l’univers \\(\\Omega\\) est l’ensemble de toutes les parties de \\(\\mathcal{U}\\) à \\(n\\) éléments : \\[\\Omega=\\{S\\in\\mathcal{P}(\\mathcal{U}), \\, \\text{Card }(S)=n\\}\\] L’univers \\(\\Omega\\) est fini et \\[\\text{Card }(\\Omega)=\\binom{N}{n}\\] iii. Répétition de lancers avec condition d’arrêt. On lance une pièce dont les faces sont notées \\(P\\) et \\(F\\). On relance la pièce tant que PILE n’est pas obtenue. Les issues de cette expérience sont tous les lancers possibles. Comme on ne sait pas à l’avance combien de lancers vont être nécessaires pour stoper l’expérience, il s’agit bien d’une expérience aléatoire. Son univers est infini dénombrable : \\[\\Omega=\\{P, FP, FFP, FFFP, \\dots, FFFFFFFFFP, \\dots \\}\\] iv. Nombre de lancers. On reprend l’expérience précédente, mais cette fois on s’intéresse au nombre de lancers effectués. Il s’agit toujours d’une expérience aléatoire, mais cette fois : \\[\\Omega=\\{1,2,3,4,\\dots, 1\\,000,\\dots\\}=\\mathbb{N}^*\\] L’univers est donc là aussi infini dénombrable. v. Durée de vie d’une lampe. On observe la durée de vie d’une lampe, exprimée en jours. On peut à nouveau assimiler cette expérience à une expérience aléatoire. Cette fois, l’univers associé n’est ni fini ni dénombrable : \\[\\Omega=[0\\,;\\,+\\infty[=\\mathbb{R}_+\\] 2.3.2 L’espace probabilisable \\((\\Omega, \\mathcal{A})\\) Souvent, on définira des événements plus complexes que les événements élémentaires, par exemple des événements composites comme l’événement A : Le nombre obtenu est un nombre impair ou encore l’événement B: Le nombre obtenu est un nombre premier. Il faut donc enrichir l’univers \\(\\Omega\\) d’un ensemble qui décrit précisément quels sont les événements observables associés à une expérience aléatoire. Evénements, tribu, espace probabilisable Soit \\(\\Omega\\) l’univers d’une expérience aléatoire. On appelle tribu sur \\(\\Omega\\) tout sous-ensemble de l’ensemble des parties de \\(\\Omega\\) : \\[\\mathcal{A}\\subset\\mathcal{P}(\\Omega)\\] vérifiant les propriétés suivantes : \\(\\Omega\\in\\mathcal{A}\\) si \\((A_n)_{n\\in\\mathbb{N}}\\) est une famille finie ou dénombrable d’éléments de \\(\\mathcal{A}\\) (i.e. chacun des \\(A_n\\) est un élement de \\(\\mathcal{A}\\)), alors leur réunion est encore dans \\(\\mathcal{A}\\) : \\[\\bigcup\\limits_{n} A_n\\in\\mathcal{A}\\] si \\(A\\in\\mathcal{A}\\) alors \\(\\overline{A}\\in\\mathcal{A}\\). Les éléments de \\(\\mathcal{A}\\) sont appelés des événements ou des observables. En particulier : \\(\\Omega\\) s’appelle l’événement certain ; Pour tout événement \\(A\\), l’événement \\(\\overline{A}\\) s’appelle l’événement contraire de \\(A\\). On dit que le couple \\((\\Omega, \\mathcal{A})\\) est un espace probabilsable. La famille \\(\\mathcal{A}\\) formalise tous les observables relatifs à une expérience aléatoire, autrement dit tout ce qu’on est susceptible d’observer relativement à cette expérience (et dont on aimerait ensuite mesurer le niveau de crédibilité). Exemples. On reprend les exemples i. à v. de la section précédente. Dans les exemples i. et ii., \\(\\Omega\\) est fini. Dans les exemples iii. et iv., \\(\\Omega\\) est infini dénombrable. Dans ce cas on peut prendre \\(\\mathcal{A}=\\mathcal{P}(\\Omega)\\), comme c’est l’usage pour les univers finis ou dénombrables. L’exemple v. est plus complexe car \\(\\Omega\\) est infini et non dénombrable. On pourrait prendre \\(\\mathcal{A}=\\mathbb{P}(\\Omega)\\), mais cela ferait une tribu extrêmement grande, sur laquelle il serait certainement plus difficile par la suite de définir une probabilté. Dans ce genre de cas, on essaie en général de réduire la taille de la tribu à quelque chose de plus raisonnable. De manière un peu floue, on définit une classe d’événemenents que l’on aimerait pouvoir mesurer (i.e. dont on aimerait pouvoir calculer la probabilité) et on définit notre tribu comme la plus petite tribu contenant cette classe d’ensembles (on parle alors de tribu engendrée). Dans le cas présent, comme souvent lorsque \\(\\Omega\\) est un sous-ensemble de \\(\\mathbb{R}\\) non dénombrable, on prend pour \\(\\mathcal{A}\\) la tribu engendrée par les ouverts de \\(\\mathbb{R_+}\\). Une telle tribu s’appelle une tribu borélienne (plus généralement, pour un espace topologique \\((X, \\mathcal{T})\\) donné, la tribu borélienne est définie comme la tribu engendrée par les ouverts de cette topologie, i.e. les éléments de \\(\\mathcal{T}\\)). Il est difficile, pour ne pas dire impossible, de se représenter exactement à quoi cette tribu ressemble. En pratique, on s’y intérresse de toute façon assez peu. Tout cela étant largement hors-programme du concours d’administrateur, vous n’aurez pas du tout à vous en soucier dans votre préparation ! D’autres exemples. i. Lancer d’une pièce. L’univers associé au lancer d’une pièce est \\(\\Omega=\\{P,F\\}\\). On pose \\(\\mathcal{A}=\\left\\{\\emptyset, \\{P\\},\\{F\\}, \\{P,F\\}\\right\\}=\\mathcal{P}(\\Omega)\\). Alors, \\(\\mathcal{A}\\) est une tribu et donc le couple \\((\\Omega, \\mathcal{A})\\) est un espace probabilisable. ii. Si \\(\\Omega=\\{1,2,3,4\\}\\) alors \\(\\mathcal{A}=\\left\\{\\emptyset, \\{1\\}, \\{2,3,4\\}, \\Omega\\right\\}\\) est une tribu. \\(\\mathcal{A}&#39;=\\left\\{\\emptyset, \\{1,2\\}, \\{3, 4\\}, \\Omega\\right\\}\\) est une autre tribu. Remarques. i. Pour tout ensemble \\(\\Omega\\) modélisant l’univers d’une expérience aléatoire, on peut toujours poser \\(\\mathcal{A}=\\mathcal{P}(\\Omega)\\) et défnir ainsi une tribu, et donc un espace probabilisable \\((\\Omega, \\mathcal{P}(\\Omega))\\). C’est d’ailleurs assez souvent ce qui est fait lorsque l’ensemble \\(\\Omega\\) est fin ou dénombrable. Une telle tribu s’appelle la tribu discrète. Il s’agit de la plus grande tribu (au sens de l’inclusion) sur \\(\\Omega\\). ii. On peut aussi définir la tribu grossière : \\[\\mathcal{A}=\\{\\emptyset, \\Omega\\}\\] Il s’agit de la plus petite tribu (au sens de l’inclusion) sur \\(\\Omega\\). iii. Même si la notion de tribu n’est pas explicitement au programme (voire même est explicitement hors-programme), il faut tout de même retenir que la notion d’événement est stable par réunion dénombrable et par passage au complémentaire. Autrement dit : on est toujours capable d’observer la survenue ou non de \\(\\emptyset\\) et \\(\\Omega\\) ; si on est capable d’observer la survenue d’un événement \\(A\\), alors on est capable d’observer la survenue de l’événement contraire \\(\\overline{A}\\) ; si on est capable, pour tout entier naturel \\(n\\), d’observer la survenue d’un événement \\(A_n\\), alors on est capable d’observer la survenue de la réunion dénombrable \\(\\bigcup\\limits_{n\\in\\mathbb{N}} A_n\\) (et donc, à plus forte raison on est capable d’observer la survenue d’une réunion finie \\(\\bigcup\\limits_{n=0}^N A_n\\), puisqu’une réunion finie de \\(0\\) à \\(N\\) est un cas particulier d’une réunion finie sur \\(\\mathbb{N}\\) en posant \\(A_n=\\emptyset\\) pour \\(n\\geq N+1\\)). On peut en fait aller un peu plus loin : Stabilité de la notion d’événement. La notion d’événement est stable par toute opération de réunion finie ou dénombrable, d’intersection finie ou dénombrable et par passage au complémentaire. Autrement dit, si \\((A_n)_{n\\in \\mathcal{I}}\\) est une suite d’événements finie (cas où \\(\\mathcal{I}\\) est fini) ou infinie dénombrable (cas où \\(\\mathcal{I}\\) est infini dénombrable), alors \\(\\bigcup\\limits_{n\\in\\mathcal{I}} A_n\\), \\(\\bigcap\\limits_{n\\in\\mathcal{I}} A_n\\) et les \\(\\overline{A_n}\\) sont des événements. Démonstration. La notion d’événement est, par définition, stable par réunion dénombrable et par passage au complémentaire. Il reste donc à démontrer qu’elle est aussi stable par intersection dénombrable. Or \\[\\bigcap\\limits_{n\\in\\mathcal{I}} A_n=\\overline{\\bigcup\\limits_{n\\in\\mathcal{I}}\\overline{A_n}}\\] En effet, d’après l’une des lois de Morgan on a \\[\\overline{\\bigcup\\limits_{n\\in\\mathcal{I}}\\overline{A_n}}=\\bigcap\\limits_{n\\in\\mathcal{I}} \\overline{\\overline{A_n}}\\] et comme \\(\\overline{\\overline{A_n}}=A_n\\) on en déduit l’égalité annoncée. Par ailleurs : pour tout \\(n\\) dans \\(\\mathcal{I}\\), \\(\\overline{A_n}\\) est un événement (stabilité par passage au complémentaire) donc \\(\\bigcup\\limits_{n\\in\\mathcal{I}}\\overline{A_n}\\) est un événement (stabilité par réunion dénombrable) d’où l’on déduit que \\(\\overline{\\bigcup\\limits_{n\\in\\mathcal{I}}\\overline{A_n}}\\) est aussi un événement (stabilité par passage au complémentaire) Avec l’égalité démontrée plus haut, on obtient finalement que \\(\\bigcap\\limits_{n\\in\\mathcal{I}} A_n\\) est un événement. \\(\\square\\) Conséquence. Cette proposition a pour conséquence immédiate que si les \\((A_n)_{n\\in \\mathcal{I}}\\) forment une suite finie ou dénombrable d’événements, alors toute combinaison - aussi complexe sot-elle - de ces événements à partir des opérateurs \\(\\bigcup, \\bigcap\\) et de passage au complémentaire est encore un événement. Par exemple : \\(\\overline{\\bigcup_{n\\in\\mathcal{I}}\\bigcap_{k\\geq n}\\overline{A_k}}\\) est encore un événement ; \\(\\bigcap\\limits_{n\\in\\mathcal{I}}\\overline{\\bigcup_{k\\leq 2n} \\overline{A_k}}\\) est encore un événement ; \\(\\bigcup\\limits_{n\\in\\mathcal{I}}\\bigcup\\limits_{5\\leq p\\leq n}\\bigcap\\limits_{k\\geq p}\\bigcup\\limits_{l\\leq k}\\overline{A_l}\\) est encore un événement. On voit maintenant des exemples de combinaisons d’événements qui reviennent fréquemment dans les sujets du concours d’administrateur : Exemples classiques d’événements complexes Soit \\(A_1,A_2,\\dots,A_n,\\dots\\) une suite d’événements. \\(\\bigcup\\limits_{n=1}^{\\infty}A_n\\) est l’événement L’un au moins des \\(A_n\\) est réalisé \\(\\bigcup\\limits_{n=1}^{\\infty}\\overline{A_n}\\) est l’événement L’un au moins des \\(A_n\\) n’est pas réalisé \\(\\bigcap\\limits_{n=1}^{\\infty}A_n\\) est l’événement Tous les \\(A_n\\) sont réalisés \\(\\bigcap\\limits_{n=1}^{\\infty}\\overline{A_n}\\) est l’événement Aucun \\(A_n\\) n’est réalisé \\(\\bigcup\\limits_{n=0}^{\\infty}\\bigcap\\limits_{k\\geq n}A_k\\) est l’événement Tous les \\(A_n\\) sont réalisés à partir d’un certain rang. Cet événement est aussi la limite inférieure des \\(A_n\\) : \\[\\lim\\inf\\limits_{n\\to\\infty}A_n=\\bigcup\\limits_{n=0}^{\\infty}\\bigcap\\limits_{k\\geq n}A_k\\] \\(\\bigcap\\limits_{n=0}^{\\infty}\\bigcup\\limits_{k\\geq n}A_k\\) est l’événement Les \\(A_n\\) sont réalisés un nombre infini de fois. Cet événement est aussi la limite supérieure des \\(A_n\\) : \\[\\lim\\sup\\limits_{n\\to\\infty}A_n=\\bigcap\\limits_{n=0}^{\\infty}\\bigcup\\limits_{k\\geq n}A_k\\] Exercice. Ecrire l’événement Les \\(A_n\\) ne sont réalisés qu’un nombre fini de fois à partir des \\(\\overline{A_k}\\). Solution. Soit \\(B\\) l’événement Les \\(A_n\\) ne sont réalisés qu’un nombre fini de fois. Avec ce qui précède, on a, par application des lois de Morgan : \\[\\begin{align} B &amp;= \\overline{\\bigcap\\limits_{n\\to\\infty}\\bigcup\\limits_{k\\geq n}A_k} \\\\ &amp;= \\bigcup\\limits_{n\\to\\infty}\\bigcap\\limits_{k\\geq n}\\overline{A_k} \\end{align}\\] 2.3.3 L’espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) Maintenant qu’on a défini la notion d’événement, on a envie de pouvoir mesurer le niveau de crédibilité de survenue d’un événement donné. Pour cela, on part d’une expérience aléatoire et on définit une application sur l’ensemble de tous les événements associés à cette expérience, et à valeurs dans \\([0,1]\\). Cela revient à enrichir l’espace probabilisable \\((\\Omega, \\mathcal{A})\\) pour le transformer en un espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) : Probabilité, espace probabilisé Soit \\((\\Omega,\\mathcal{A})\\) un espace probabilisable. On appelle probabilité (ou mesure de probabilité) sur cet espace toute application \\[\\mathbb{P}:\\mathcal{A}\\longrightarrow [0,1]\\] vérifiant les axiomes suivants : L’univers est certain : \\(\\mathbb{P}(\\Omega)=1\\) \\(\\sigma-\\)additivité : si \\((A_n)_n\\) est une suite d’événements deux à deux incompatibles (i.e. si \\(A_n\\cap A_p=\\emptyset\\) dès que \\(n\\neq p\\)) alors la série \\(\\sum\\limits_{n}\\mathbb{P}(A_n)\\) est convergente et \\[\\mathbb{P}\\left(\\bigcup\\limits_{n\\to\\infty}A_n\\right)=\\sum\\limits_{n=0}^{\\infty}\\mathbb{P}(A_n)\\] On dit alors que le triplet \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) est un espace probabilisé. Remarque. Le \\(\\sigma\\) de la \\(\\sigma-\\)additivité fait référence au caractère infini de la propriété additivité. On peut aussi parler d’additivité (sans le \\(\\sigma\\)) d’une application \\(P\\) mais dans ce cas on parle plutôt d’une application \\(P\\) telle que \\[P\\left(\\bigcup\\limits_{n=0}^N A_n\\right)=\\sum\\limits_{n=0}^N P(A_n)\\] La \\(\\sigma-\\)additivité est strictement plus forte que l’additivité : \\[(P \\text{ est } \\sigma-\\text{ additive}) \\Rightarrow (P \\text{ est additive})\\] \\[\\text{mais}\\] \\[(P \\text{ est additive}) \\not\\Rightarrow (P \\text{ est } \\sigma-\\text{ additive})\\] Comme on souhaite considérer des événements composites s’écrivant comme réunion disjointe d’un nombre infini d’événements, il est plus naturel d’exiger d’une mesure de probabilité la propriété de \\(\\sigma-\\)additivité. On déduit de façon immédiate un certain nombre de propriétés des mesures de probabilité : Propriétés élémentaires des mesures de probabilité Soit \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé. Alors : L’événement impossible est de probabilité nulle. Autrement dit : \\[\\mathbb{P}(\\emptyset)=0\\] Probabilité de l’événement complémentaire. Pour tout événement \\(A\\), la probabilité de l’événement complémentaire \\(\\overline{A}\\) est donnée par \\[\\mathbb{P}(\\overline{A})=1-\\mathbb{P}(A)\\] Croissance de la probablité. Soient \\(A\\) et \\(B\\) deux événements tels que \\(A\\subset B\\). Alors : \\[\\mathbb{P}(A)\\leq\\mathbb{P}(B)\\] Crible de Poincaré. Soient \\(A\\) et \\(B\\) deux événements tels que \\(A\\subset B\\). Alors : \\[\\mathbb{P}(A\\cup B)=\\mathbb{P}(A)+\\mathbb{P}(B)-\\mathbb{P}(A\\cap B)\\] Plus généralement, étant donné une suite finie \\(A_1,A_2,\\dots A_n\\) d’événements, on a la formule \\[\\mathbb{P}\\left(\\bigcup\\limits_{k=1}^{n} A_k\\right)=\\sum\\limits_{k=1}^{n}(-1)^{k+1}\\sum\\limits_{1\\leq i_1&lt;i_2&lt;\\dots &lt;i_k}\\mathbb{P}\\left(\\bigcap\\limits_{j=1}^k A_{i_j}\\right)\\] Démonstration. Pour tout événement \\(A\\), \\(A\\) et \\(\\overline{A}\\) sont incompatibles, et par ailleurs \\(A\\cup\\overline{A}=\\Omega\\), donc : \\[\\begin{align} 1 &amp;= \\mathbb{P}(\\Omega) \\\\ &amp;= \\mathbb{P}(A\\cup\\overline{A}) \\\\ &amp;= \\mathbb{P}(A)+\\mathbb{P}(\\overline{A}) \\end{align}\\] et donc on a bien \\[\\mathbb{P}(\\overline{A})=1-\\mathbb{P}(A)\\] Par ailleurs, comme \\(\\emptyset=\\overline{\\Omega}\\) et \\(\\mathbb{P}(\\Omega)=1\\), on en déduit que \\[\\mathbb{P}(\\emptyset)=0\\] la formule du crible de Poincaré se démontre exactement comme la formule déjà vue pour le cardinal. Le point important de la démonstration - celui qui fait que tout fonctionne bien - dans la formule avec le cardinal est le fait que l’application cardinal : \\[\\text{Card }:A\\in\\mathcal{A}\\longrightarrow\\text{Card }(A)\\] est additive. Or, l’additivité (et même la \\(\\sigma-\\)additivité) est au coeur même de la construction des probabilités : l’application \\[\\mathbb{P}:A\\in\\mathcal{A}\\longrightarrow\\mathbb{P}(A)\\] est elle aussi additive (et même \\(\\sigma-\\)additive). Donc la démonstration de la formule de Poincaré pour les probabilités est une recopie exacte de celle pour les cardinaux, en remplaçant simplement l’application \\(\\text{Card}\\) par l’application \\(\\mathbb{P}\\). enfin, soient \\(A\\) et \\(B\\) des événements tels que \\(A\\subset B\\). Alors : \\[B= A\\cup (B\\cap\\overline{A})\\] \\[A\\cap(B\\cap\\overline{A})=\\emptyset\\] Avec ce qui précède on a donc \\[\\begin{align} \\mathbb{P}(B) &amp;= \\mathbb{P}(A)+\\mathbb{P}(B\\cap\\overline{A}) \\\\ &amp; \\geq\\mathbb{P}(A) \\end{align}\\] car \\(\\mathbb{P}(B\\cap\\overline{A})\\geq 0\\) (une probabilté est toujours positive). D’où le résultat. \\(\\square\\) Un cas particulier important est celui de l’équiprobabilité : Equiprobabilité Soit \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé tel que \\(\\Omega\\) soit fini à \\(n\\) éléments. On dit qu’il y a équiprobabilité lorsque toutes les probabilités des événements élémentaires \\(\\{\\omega\\}, \\omega\\in\\Omega\\) sont égales. Dans ce cas on a : \\[\\forall\\omega\\in\\Omega, \\,\\mathbb{P}(\\{\\omega\\})=\\frac{1}{n}\\] et plus généralement : \\[\\text{Pour tout événement } A, \\,\\mathbb{P}(A)=\\frac{\\text{Card }(A)}{\\text{Card }(\\Omega)}=\\frac{\\text{Card }(A)}{n}\\] Démonstration. On note \\(\\Omega=\\{\\omega_1,\\dots,\\omega_n\\}\\) et \\(p\\) la probabilité commune des événements élémentaires \\(\\{\\omega_i\\}\\). Les \\(\\{\\omega_i\\}\\) étant deux à deux incompatibles, on a alors : \\[\\begin{align} 1 &amp;= \\mathbb{P}(\\Omega) \\\\ &amp;= \\mathbb{P}\\left(\\bigcup\\limits_{i=1}^n \\{\\omega_i\\}\\right) \\\\ &amp;= \\sum\\limits_{i=1}^n \\mathbb{P}(\\{\\omega_i\\}) \\\\ &amp;= \\sum\\limits_{i=1}^n p \\\\ &amp;= np \\\\ \\end{align}\\] et donc \\[\\forall i\\in\\{1,\\dots,n\\}, \\,\\mathbb{P}(\\{\\omega_i\\})=\\frac{1}{n}\\] Soit maintenant \\(A=\\{\\omega_{i_1},\\dots,\\omega_{i_k}\\}\\) un événement (son cardinal est donc égal à \\(k\\)). Par un raisonnement analogue, on a : \\[\\begin{align} \\mathbb{P}(A) &amp;= \\mathbb{P}\\left(\\bigcup\\limits_{j=1}^k \\{\\omega_{i_j}\\}\\right) \\\\ &amp;= \\sum\\limits_{j=1}^k \\mathbb{P}(\\{\\omega_{i_j}\\}) \\\\ &amp;= \\sum\\limits_{i=1}^k \\frac{1}{n} \\\\ &amp;= \\frac{k}{n} \\\\ &amp;= \\frac{\\text{Card }(A)}{\\text{Card }(\\Omega)} \\\\ \\end{align}\\] \\(\\square\\) Cette formule montre donc que, en situtation d’équiprobabilité, calculer des probabilités revient à dénombrer. C’est typiquement la façon dont les probabilités étaient introduites avant dans les classes de lycée, avec les applications classiques sur les cartes, les boules dans des urnes, etc. Remarque. Lorsqu’on effectue un tirage alétoire dans un ensemble fini de façon équiprobable, on dit souvent qu’on effectue un tirage au hasard. Exemples. i. Course hippique. Lors d’une course hippique, on suppose que les \\(n\\) chevaux au départ ont la même probabilité de gagner. Quelle est la probabilité d’avoir un tiercé gagnant avec un ticket : dans l’ordre d’arrivée ? dans l’ordre d’arrivée ou dans un ordre différent ? dans un ordre différent ? Solution. On calcule d’abord le nombre de tiercés possibles. C’est le nombre de façons ordonnées de choisir tros chevaux parmi un ensemble de \\(n\\) chevaux. Il y en a donc \\[\\begin{align} A_n^3 &amp;= \\frac{n!}{(n-3)!} \\\\ &amp;= n(n-1)(n-2) \\\\ \\end{align}\\] il n’y a qu’un seul tiercé gagnant dans l’ordre, la probabilité \\(p_1\\) cherchée est \\[p_1=\\frac{1}{n(n-1)(n-2)}\\] - le nombre de tiercés gagnant dans l’ordre ou dans un ordre différent est égal au nombre de permutations d’un ensemble à trois éléments, soit \\(3!=6\\). La probabilité \\(p_2\\) cherchée est donc égale à \\[p_2=\\frac{6}{n(n-1)(n-2)}\\] le nombre de tiercés gagnants dans le désordre est \\(6-1=5\\) donc la probabilité \\(p_3\\) cherchée est \\[p_3=\\frac{5}{n(n-1)(n-2)}\\] ii. Permutations avec \\(n-2\\) points fixes. Soit \\(n\\) entier naturel tel que \\(n\\geq 3\\). On tire au hasard une permutation aléatoire \\(\\sigma\\) de \\(\\mathfrak{S}_n\\). Quelle est la probablité qu’elle ait exactement \\(n-2\\) points fixes ? (on dit qu’un entier \\(i\\in\\{1,2,\\dots, n\\}\\) est un point fixe de \\(\\sigma\\) lorsque \\(\\sigma(i)=i\\)). Solution. Soit \\(A_n\\) l’événement La permutation \\(\\sigma\\) a exactement \\(n-2\\) points fixes. Choisir une telle permutation \\(\\sigma\\) revient exactement à : choisir \\(n-2\\) points fixes parmi les \\(n\\) entiers de \\(1\\) à \\(n\\) : il y a \\(\\binom{n}{n-2}=\\binom{n}{2}=\\frac{n(n-1)}{2}\\) choix possibles ; choisir les deux points non fixes : mais le choix des \\(n-2\\) points fixes à l’étape précédente détermine de façon unique celui des deux points restants ; choisir les images des entiers \\(1,2,\\dots, n\\). Mais une fois qu’on a fait les choix précédents, il n’y a plus qu’une seule permutation \\(\\sigma\\) qui convienne : pour les \\(n-2\\) points fixes \\(i_1, i_2,\\dots i_{n-2}\\) on a, par définition \\(\\sigma(i_j)=i_j\\) ; pour les deux points \\((i_{n-1}, i_n)\\) qui ne sont pas fixes, on a \\(\\sigma(i_{n-1})=i_n\\) et \\(\\sigma(i_n)=i_{n-1}\\). D’après le principe multiplicatif, il y a donc \\(\\frac{n(n-1)}{2}\\) permutations de \\(\\mathfrak{S}_n\\) à \\(n-2\\) points fixes. Par ailleurs, il y a exactement \\(n!\\) permutations de \\(\\mathfrak{S}_n\\). On en déduit que \\[\\mathbb{P}(A_n)=\\frac{n(n-1)}{2\\,n!}=\\frac{1}{2\\,(n-2)!}\\] 2.3.4 Indépendance Indépendance de deux événements Soit \\((\\Omega, \\mathcal{A},\\mathbb{P})\\) un espace probabilisé. Deux événements \\(A\\) et \\(B\\) sont dits indépendants si \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\,\\mathbb{P}(B)\\). Remarque. La notion d’indépendance est relative à un espace probabilisé \\((\\Omega, \\mathcal{A},\\mathbb{P})\\). Dit autrement, si on a deux espaces probabilisés \\((\\Omega, \\mathcal{A},\\mathbb{P})\\) et \\((\\Omega&#39;, \\mathcal{A}&#39;,\\mathbb{P}&#39;)\\) et \\(A\\) et \\(B\\) qui sont des événements pour ces deux espaces probabilisés à la fois, alors \\(A\\) et \\(B\\) peuvent être indépendants pour l’un des espaces mais pas pour l’autre. Cela peut notamment arriver si on prend deux mesures de probabilité dfférentes \\(\\mathbb{P}\\) et \\(\\mathbb{P}&#39;\\) pour un même espace probabilisable \\((\\Omega, \\mathcal{A})\\). Exemple. On lance deux dés cubiques équilibrés. On note : \\(A\\) l’événement La somme des deux numéros obtenus est 7 \\(B\\) l’événement Le produit des deux nombres obtenus est 6. Les événements \\(A\\) et \\(B\\) sont-ils indépendants ? Solution. L’univers \\(\\Omega\\) de cette expérience aléatoire est \\(\\Omega=\\{1,2,3,4,5,6\\}^2\\), de cardinal \\(36\\). Par ailleurs : \\[A=\\{(1,6),(2,5),(3,4),(4,3),(5,2), (6,1)\\}\\] \\[B=\\{(1,6), (2,3), (3,2), (6,1)\\}\\] \\[A\\cap B=\\{(1,6), (6,1)\\}\\] Les dés étant équililibrés, les événements élémentaires de \\(\\Omega\\) ont tous la même probabilité \\(p=\\frac{1}{36}\\), donc : \\[\\mathbb{P}(A\\cap B)=\\frac{2}{36}=\\frac{1}{18}\\] \\[\\mathbb{P}(A)\\mathbb{P}(B)=\\frac{6}{36}\\frac{4}{36}=\\frac{1}{54}\\] Donc, \\(\\mathbb{P}(A\\cap B)\\neq\\mathbb{P}(A)\\mathbb{P}(B)\\) : les événements \\(A\\) et \\(B\\) ne sont pas indépendants. Si on considère plus de deux événements, on peut définir deux notions d’indépendance : Indépendance mutuelle, indépendance deux à deux Soient \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé et \\(A_1,\\dots A_m\\) des événements. \\(A_1,\\dots, A_m\\) sont dits mutuellement indépendants lorsque pour toute partie \\(\\{i_1,\\dots i_p\\}\\subset\\{1,\\dots, m\\}\\) on a \\[\\mathbb{P}\\left(\\bigcap\\limits_{j=1}^p A_{i_j}\\right)=\\prod\\limits_{j=1}^p\\mathbb{P}(A_{i_j})\\] \\(A_1,\\dots, A_m\\) sont dits deux à deux indépendants lorsque pour tout couple \\((j,k)\\in\\{1,\\dots, m\\}^2\\) tel que \\(j\\neq k\\), on a \\[\\mathbb{P}(A_j\\cap A_k)=\\mathbb{P}(A_j)\\mathbb{P}(A_k)\\] Remarques. i. L’indépendance mutuelle est une notion plus forte que l’indépendance deux à deux : l’indépendance deux à deux dit que pour toute partie \\(\\{i_1, i_2\\}\\subset\\{1,\\dots, m\\}\\), \\(\\mathbb{P}(A_{i_1}\\cap A_{i_2})=\\mathbb{P}(A_{i_1})\\mathbb{P}(A_{i_2})\\), alors que l’indépendance mutuelle généralise ce type d’égalité à un nombre \\(p\\) quelconque d’événements, \\(p\\) étant compris entre \\(1\\) et \\(n\\). Autrement dit, l’indépendance mutuelle implique l’indépendance deux à deux. on peut trouver des exemples où trois événements \\(A_1, A_2, A_3\\) sont deux à deux indépendants sans être mutuellement indépendants. Si on lance une pièce équilibrée deux fois, et que l’on note \\(A_1\\) l’événement Obtenir PILE au premier lancer \\(A_2\\) l’événement Obtenir FACE au deuxième lancer \\(A_3\\) l’événement Obtenir le même résultat aux deux lancers alors on a \\[\\mathbb{P}(A_1)=\\mathbb{P}(A_2)=\\mathbb{P}(A_3)=\\frac{1}{2}\\] \\[\\mathbb{P}(A_1\\cap A_2)=\\mathbb{P}(A_1\\cap A_3)=\\mathbb{P}(A_2\\cap A_3)=\\frac{1}{4}\\] \\[\\mathbb{P}(A_1\\cap A_2\\cap A_3)=0\\] Ainsi, pour \\(i\\neq j\\) : \\[\\mathbb{P}(A_i\\cap A_j)=\\mathbb{P}(A_i)\\mathbb{P}(A_j)=\\frac{1}{4}\\] donc \\(A_1,A_2,A_3\\) sont deux à deux indépendants. Mais \\[\\mathbb{P}(A_1\\cap A_2\\cap A_3)=0\\neq\\mathbb{P}(A_1)\\mathbb{P}(A_2)\\mathbb{P}(A_3)=\\frac{1}{8}\\] donc \\(A_1, A_2, A_3\\) ne sont pas mutuellement indépendants. L’indépendance deux à deux n’implique donc pas nécessairement l’indépendance mutuelle. ii. Ces deux notions coïncident cependant de façon évidente pour deux événements. 2.3.5 Probabilités conditionnelles Supposons qu’on lance deux dés équilibrés et que l’on relève leurs numéros respectifs. La probabilité que leur somme soit égale à \\(8\\) est égale à \\(\\frac{5}{36}\\). Si maintenant quelqu’un nous dit que leur produit est égal à \\(12\\), alors avec la connaissance de cette information supplémentaire cette probabilité monte à \\(\\frac{1}{2}\\). Dans le premier cas, l’univers est \\(\\Omega=\\{1,2,3,4,5,6\\}^2\\) (de cardinal \\(36\\)) et l’événement \\(A=\\) La somme vaut \\(8\\) est réalisé par les couples \\((2,6), (3,5), (4,4), (5,3)\\) et \\((6,2)\\), et il est donc de cardinal \\(5\\). Donc on a bien \\(\\mathbb{P}(A)=\\frac{5}{36}\\). Dans le second cas, l’information sur le produit nous fait changer d’univers. Le nouvel univers est l’événement \\(B=\\) Le produit vaut 12 \\(=B=\\{(2,6), (3,4), (4,3), (6,2)\\}\\), de cardinal \\(4\\). Dans ce nouvel univers, l’événement \\(A\\) est réalisé par deux couples de \\(B\\) : \\((2, 6)\\) et \\((6, 2)\\). La probabilité de \\(A\\) est donc mise à jour suite à l’information apportée par la réalisation de \\(B\\). Cette nouvelle probabilité est notée \\(\\mathbb{P}(A|B)\\) et elle vaut donc \\(\\mathbb{P}(A|B)=\\frac{1}{2}\\). Comment l’a-t’on-calculée ? on a compté le cardinal \\(\\text{Card }(B)\\) du nouvel univers \\(B\\) ; on a compté le nombre d’événements élémentaires de \\(B\\) qui réalisent l’événement \\(A\\), autrement dit on a compté \\(\\text{Card }(A\\cap B)\\) ; étant en situation d’équiprobabilité, on a calculé \\(\\mathbb{P}(A|B)=\\frac{\\text{Card }(A\\cap B)}{\\text{Card }(B)}\\), que l’on peut aussi écrire \\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\] La définition qui suit est une extension de cette formule au cas général, i.e. sans faire d’hypothèse d’équirépartition des événements élémentaires. Probabilité conditionelle Soient \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé, et \\(A\\) et \\(B\\) deux événements tels que \\(\\mathbb{P}(B)&gt;0\\). On appelle probabilité conditionnelle de \\(B\\) sachant \\(A\\), ou encore probabilité de \\(B\\) conditionnellement à \\(A\\), le nombre \\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\] Cette probabilité conditionnelle est parfois aussi notée \\(\\mathbb{P}_B(A)\\). Une probabilité conditionnelle est en fait une probabilité comme une autre : Théorème. Avec les notations précédentes, l’application \\[\\mathbb{P}(\\,.\\,|B):A\\in\\mathcal{A}\\mapsto\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\] est une mesure de probabilité sur l’espace probabilisable \\((\\Omega, \\mathcal{A})\\) (et donc, \\((\\Omega, \\mathcal{A}, \\mathbb{P}(\\,.\\,|B))\\) est un espace probabilisé). Démonstation. Tout d’abord, on a \\[\\begin{align} \\mathbb{P}(\\Omega|B) &amp;= \\frac{\\mathbb{P}(\\Omega\\cap B)}{\\mathbb{P}(B)} \\\\ &amp;= \\frac{\\mathbb{P}(B)}{\\mathbb{P}(B)} \\\\ &amp;= 1 \\end{align}\\] Par ailleurs, pour \\((A_n)_n\\) une suite d’événements deux à deux incompatibles, on a \\[\\begin{align} \\mathbb{P}\\left(\\bigcup\\limits_{n=0}^{\\infty} A_n\\vert B\\right) &amp;=\\frac{\\mathbb{P}\\left(\\left(\\bigcup\\limits_{n=0}^{\\infty} A_n\\right)\\cap B\\right)}{\\mathbb{P}(B)} \\\\ &amp;=\\frac{\\mathbb{P}\\left(\\bigcup\\limits_{n=0}^{\\infty} \\left(A_n\\cap B\\right)\\right)}{\\mathbb{P}(B)} \\\\ &amp;= \\frac{\\sum\\limits_{n=0}^{\\infty}\\mathbb{P}\\left(A_n\\cap B\\right)}{\\mathbb{P}(B)} \\\\ &amp; \\text{(car les } A_n\\cap B \\text{ sont deux à deux incompatibles)} \\\\ &amp;=\\sum\\limits_{n=0}^{\\infty}\\frac{\\mathbb{P}\\left(A_n\\cap B\\right)}{\\mathbb{P}(B)} \\\\ &amp;=\\sum\\limits_{n=0}^{\\infty}\\mathbb{P}\\left(A_n | B\\right) \\end{align}\\] D’où le résultat. \\(\\square\\) Remarques. i. On vient de montrer qu’une probabilité conditionnelle est une probabilité. La réciproque est également vraie : une probabilité est une probabilité conditionnelle. Plus précisément, pour tout espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\), on a \\[\\mathbb{P}=\\mathbb{P}(\\,.\\,\\vert\\Omega)\\] Les probabilités et les probabilités conditionnelles sont donc exactement les mêmes objets. Conditionner par \\(A\\), c’est juste passer d’un univers \\(\\Omega\\) à l’univers \\(A\\). ii. Puisqu’une probabilité conditionnelle est une probabilité, toutes les propriétés des probabilités sont également vraies pour les probabilités conditionnelles. Par exemple, si \\(A\\) et \\(B\\) sont deux événements tels que \\(\\mathbb{P}(B)&gt;0\\) (ce qui autorise à conditionner par \\(A\\)), alors \\(\\mathbb{P}(\\overline{A}\\vert B)=1-\\mathbb{P}(A\\vert B)\\). iii. Dans l’expression \\(\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\), la partie \\(A\\) que l’on mesure apparaît uniquement au numérateur. Le terme au dénominateur est uniquement un facteur de normalisation, qui permet de s’assurer que l’application \\(A\\in\\mathcal{A}\\mapsto\\mathbb{P}(A\\vert B)\\) somme bien à \\(1\\). La notion d’indépendance présentée plus haut s’interprète de façon très intuitive : Théorème (indépendance et conditionnement). Soient \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé, et \\(A\\) et \\(B\\) deux événements tels que \\(\\mathbb{P}(A)&gt;0\\) et \\(\\mathbb{P}(B)&gt;0\\). Alors : \\[\\text{(}A \\text{ et } B \\text{ sont indépendants )} \\Leftrightarrow\\text{( }\\mathbb{P}(A\\vert B)=\\mathbb{P}(A) \\text{ et } \\mathbb{P}(B\\vert A)=\\mathbb{P}(B)\\text{ )}\\] Autrement dit, dire que \\(A\\) et \\(B\\) sont indépendants revient à dire que l’information apportée par la survenue de l’un de ces deux événements n’affecte en rien celle de la survenue de l’autre. Démonstration. Supposons \\(A\\) et \\(B\\) indépendants. Alors \\[\\begin{align} \\mathbb{P}(A\\vert B) &amp;= \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)} \\\\ &amp;= \\frac{\\mathbb{P}(A)\\mathbb{P}(B)}{\\mathbb{P}(B)} \\\\ &amp; \\text{(par indépendance de} A \\text{ et } B\\text{)} \\\\ &amp;= \\mathbb{P}(A) \\\\ \\end{align}\\] En inversant les rôles joués par \\(A\\) et \\(B\\) (et étant donné que \\(A\\cap B=B\\cap A\\)) on en déduit l’autre égalité \\(\\mathbb{P}(B\\vert A)=\\mathbb{P}(B)\\). Réciproquement, supposons que \\(\\mathbb{P}(A\\vert B)=\\mathbb{P}(A)\\) et \\(\\mathbb{P}(B\\vert A)=\\mathbb{P}(B)\\). De la première égalité on déduit que \\[\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}=\\mathbb{P}(A)\\] autrement dit \\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\mathbb{P}(B)\\] et \\(A\\) et \\(B\\) sont donc indépendants. \\(\\square\\) La formule \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A\\vert B)\\mathbb{P}(B)\\) (pour \\(B\\) tel que \\(\\mathbb{P}(B)&gt;0\\)), qui découle de la définition de \\(\\mathbb{P}(A\\vert B)\\), admet une généralisation au cas où l’on intersecte un nombre quelconque \\(n\\) événements. Théorème (formule des probabilités composées). Soient \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) un espace probabilisé et \\(A_1, A_2,\\dots, A_n\\) des événements tels que \\(\\mathbb{P}(A_1\\cap A_2\\cap\\dots A_n)&gt;0\\). Alors on a la formule de chaînage : \\[\\mathbb{P}(A_1\\cap A_2\\cap\\dots\\cap A_n)=\\mathbb{P}(A_1)\\mathbb{P}(A_2\\vert A_1)\\mathbb{P}(A_3\\vert A_1\\cap A_2)\\dots\\mathbb{P}(A_n\\vert A_1\\cap A_2\\dots\\cap A_{n-1})\\] Démonstration. Par récurrence sur \\(n\\in\\mathbb{N}^*\\) : pour \\(n=1\\) c’est évident (l’égalité s’écrit \\(\\mathbb{P}(A_1)=\\mathbb{P}(A_1)\\). Pour \\(n=2\\), c’est une conséquence de la définition de \\(\\mathbb{P}(A_1\\vert A_2)\\). supposons l’égalité vraie pour \\(n\\) événements quelconques, montrons qu’elle est vraie pour \\(n+1\\) événements quelconques. On considère donc \\(n+1\\) événements \\(A_1, A_2,\\dots, A_n, A_{n+1}\\). Alors \\[\\begin{align} \\mathbb{P}(A_1\\cap\\dots\\cap A_n\\dots\\cap A_{n+1}) &amp;= \\mathbb{P}((A_1\\cap A_2\\dots\\cap A_n)\\cap A_{n+1}) \\\\ &amp;= \\mathbb{P}(A_{n+1} \\cap (A_1\\cap\\dots\\cap A_n)) \\\\ &amp;= \\mathbb{P}(A_{n+1}\\vert A_1\\dots\\cap A_n)\\mathbb{P}(A_1\\cap\\dots\\cap A_n) \\\\ &amp; \\text{(formule pour deux événements)} \\\\ &amp;= \\mathbb{P}(A_{n+1}\\vert A_1\\cap\\dots\\cap A_n)\\mathbb{P}(A_1)\\mathbb{P}(A_2\\vert A_1)\\dots\\mathbb{P}(A_n\\vert A_1\\cap\\dots\\cap A_{n-1}) \\\\ &amp; \\text{(hypothèse de récurrence)} \\\\ &amp;= \\mathbb{P}(A_1)\\mathbb{P}(A_2\\vert A_1)\\dots\\mathbb{P}(A_n\\vert A_1\\cap\\dots\\cap A_{n-1})\\mathbb{P}(A_{n+1}\\vert A_1\\cap\\dots\\cap A_n) \\\\ \\end{align}\\] ce qui permet de conclure. \\(\\square\\) Définition Propriétés : les mêmes que pour une probabilité “non conditionnelle” Formule des probabilités composées Formule des probabilités totales Formule de Bayes "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
