[["index.html", "A Minimal Book Example Chapter 1 About 1.1 Usage 1.2 Render book 1.3 Preview book", " A Minimal Book Example John Doe 2023-10-28 Chapter 1 About This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports; for example, a math equation \\(a^2 + b^2 = c^2\\). 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["cross.html", "Chapter 2 Cross-references 2.1 Chapters and sub-chapters 2.2 Captioned figures and tables", " Chapter 2 Cross-references Cross-references make it easier for your readers to find and link to elements in your book. 2.1 Chapters and sub-chapters There are two steps to cross-reference any heading: Label the heading: # Hello world {#nice-label}. Leave the label off if you like the automated heading generated based on your heading title: for example, # Hello world = # Hello world {#hello-world}. To label an un-numbered heading, use: # Hello world {-#nice-label} or {# Hello world .unnumbered}. Next, reference the labeled heading anywhere in the text using \\@ref(nice-label); for example, please see Chapter 2. If you prefer text as the link instead of a numbered reference use: any text you want can go here. 2.2 Captioned figures and tables Figures and tables with captions can also be cross-referenced from elsewhere in your book using \\@ref(fig:chunk-label) and \\@ref(tab:chunk-label), respectively. See Figure 2.1. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Don’t miss Table 2.1. knitr::kable( head(pressure, 10), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! temperature pressure 0 0.0002 20 0.0012 40 0.0060 60 0.0300 80 0.0900 100 0.2700 120 0.7500 140 1.8500 160 4.2000 180 8.8000 "],["parts.html", "Chapter 3 Parts", " Chapter 3 Parts Bla bla bla You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. Add a numbered part: # (PART) Act one {-} (followed by # A chapter) Add an unnumbered part: # (PART\\*) Act one {-} (followed by # A chapter) Add an appendix as a special kind of un-numbered part: # (APPENDIX) Other stuff {-} (followed by # A chapter). Chapters in an appendix are prepended with letters instead of numbers. "],["footnotes-and-citations.html", "Chapter 4 Footnotes and citations 4.1 Footnotes 4.2 Citations", " Chapter 4 Footnotes and citations 4.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 4.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2023) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["blocks.html", "Chapter 5 Blocks 5.1 Equations 5.2 Theorems and proofs 5.3 Callout blocks", " Chapter 5 Blocks 5.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{5.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (5.1). 5.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 5.1. Theorem 5.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 5.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["statistique-inférentielle.html", "Chapter 6 Statistique inférentielle 6.1 Estimation 6.2 Tests statistiques", " Chapter 6 Statistique inférentielle 6.1 Estimation On s’intéresse à une loi probabiliste \\(\\mathcal{L}_{\\theta}\\), qui est entièrement décrite par la donnée d’un paramètre inconnu \\(\\theta\\). Pour mieux appréhender cette loi, il serait intéressant de connaître la valeur de \\(\\theta\\). Plutôt que de chercher à déterminer la valeur exacte de \\(\\theta\\), on peut essayer de l’approcher. Dans le cadre de la statistique inférentielle, on suppose qu’on dispose d’un échantillon i.i.d. de \\(\\mathcal{L}_{\\theta}\\), autrement dit d’un certain nombre de réalisations \\((Y_1,\\dots, Y_n)\\) indépendantes et identiquement distribuées de la loi \\(\\mathcal{L}_{\\theta}\\). La donnée d’un tel échantillon constitue un ensemble d’informations qui vont nous être utiles pour estimer le paramètre \\(\\theta\\). on fait donc bien ici de l’inférence - ou encore de l’induction - dans le sens où on part d’observations particulières (les réalisations \\(Y_1,\\dots, Y_n)\\) pour énoncer une règle générale (le fait que ces réalisations sont issues de la loi \\(\\mathcal{L}_{\\theta}\\)). 6.1.1 Premières définitions Définition : Soit \\(Y\\) une variable aléatoire de loi \\(\\mathcal{L}(Y)\\), paramétrée par un réel \\(\\theta\\) inconnu. Soit \\((Y_1,\\dots, Y_n)\\) un échantillon i.i.d. de loi \\(\\mathcal{L}(Y)\\). On appelle estimateur de \\(\\theta\\) toute fonction de \\(Y_1,\\dots, Y_n\\), i.e. \\[\\widehat{\\theta}_n=S(Y_1,\\dots, Y_n)\\] On veut estimer la moyenne d’une loi normale \\(\\mathcal{N}(\\mu\\,;\\,1)\\), à partir d’un échantillon d’observations i.i.d. \\((Y_1,\\dots, Y_n)\\) tirées sous cette loi. Une façon naturelle d’estimer \\(\\mu=\\mathbb{E}(Y_1)\\) est de poser \\(\\widehat{\\mu}_n=\\frac{Y_1+\\dots Y_n}{n}\\). Ici, on estime donc une moyenne théorique par sa contrepartie empirique. Définition : Soit \\(\\widehat{\\theta}_n\\) un estimateur de \\(\\theta\\) admettant un moment d’ordre \\(1\\). On appelle biais de \\(\\widehat{\\theta}_n\\) la quantité \\(b_{\\theta}(\\widehat{\\theta}_n)=\\mathbb{E}(\\widehat{\\theta}_n)-\\theta\\). Un estimateur est dit sans biais lorsque son biais est nul, i.e. \\(\\mathbb{E}(\\widehat{\\theta}_n)=\\theta\\). Il est dit asymptotiquement sans biais lorsque son biais tend vers \\(0\\), i.e. \\(b_{\\theta}(\\widehat{\\theta}_n)\\underset{n\\to +\\infty}{\\longrightarrow}0\\). Pour un estimateur des moments d’ordre \\(1\\) et \\(2\\), on appelle erreur quadratique moyenne la quantité (positive) \\(\\text{EQM}_{\\theta}(\\widehat{\\theta}_n)=\\mathbb{E}\\left(\\left(\\widehat{\\theta}_n-\\theta\\right)^2\\right)\\) L’erreur quadratique moyenne s’écrit à l’aide de l’espérance et de la variance : Soit \\(\\widehat{\\theta}_n\\) un estimateur de \\(\\theta\\) admettant des moments d’ordres \\(1\\) et \\(2\\). Son erreur quadratique moyenne peut se décomposer en biais au carré/variance : \\[\\text{EQM}_{\\theta}(\\widehat{\\theta}_n)=b_{\\theta}^2(\\widehat{\\theta}_n)+\\mathbb{V}(\\widehat{\\theta}_n)\\] Autrement dit, réduire l’erreur (quadratique moyenne) d’un estimateur revient à essayer de réduire son biais et/ou sa variance. En pratique, uune réduction du biais implique souvent une augmentation de la variance (et vice-versa) et il faut trouver un compromis entre les deux, i.e. un estimateur pour lequel la combinaison (biais, variance) implique une faible erreur quadratique moyenne. On parle alors de compromis biais-variance. 6.1.2 Convergence d’un estimateur Un estimateur \\(\\widehat{\\theta}_n\\) de \\(\\theta\\) est dit convergent lorsqu’il converge en probabilité vers \\(\\theta\\) i.e. lorsque \\[\\forall\\varepsilon &gt;0, \\mathbb{P}\\left(|\\widehat{\\theta}_n-\\theta|&gt;\\varepsilon\\right)\\longrightarrow 0\\] La convergence d’un estimateur sans biais peut se montrer à l’aide du critère pratique suivant : Un estimateur \\(\\widehat{\\theta}_n\\) sans biais de \\(\\theta\\) est convergent dès que sa variance tend vers \\(0\\), i.e. \\[\\left(\\mathbb{E}_{\\theta}(\\widehat{\\theta}_n)=0 \\text{ et } \\mathbb{V}_{\\theta}(\\widehat{\\theta}_n)\\longrightarrow 0\\right)\\Rightarrow \\left(\\widehat{\\theta}_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}\\theta\\right)\\] **Démonstration.** Compte-tenu du fait que \\(\\widehat{\\theta}_n\\) est un estimateur sans biais pour \\(\\theta\\), l’inégalité de Bieanymé-Tchebychev s’écrit \\(\\mathbb{P}(|\\widehat{\\theta}_n-\\theta|&gt;\\varepsilon)\\leq\\frac{\\mathbb{V}_{\\theta}(\\widehat{\\theta}_n)}{\\varepsilon^2}\\), ce qui permet de conclure. \\(\\square\\) On peut même affaiblir un peu l’hypothèse d’absence de biais par une hypothèse de biais asymptotiquement nul : Un estimateur \\(\\widehat{\\theta}_n\\) asymtotiquement sans biais de \\(\\theta\\) est convergent dès que sa variance tend vers \\(0\\), i.e. \\[\\left(\\mathbb{E}_{\\theta}(\\widehat{\\theta}_n)\\underset{n\\to +\\infty}{\\longrightarrow}\\theta \\text{ et } \\mathbb{V}_{\\theta}(\\widehat{\\theta}_n)\\longrightarrow 0\\right)\\Rightarrow \\left(\\widehat{\\theta}_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}\\theta\\right)\\] 6.1.3 Exemples classiques Quelques exemples très classiques d’estimateurs : **Exemple 1 : moyenne empirique.** Soit \\(X_1,\\dots X_n\\) une suite de \\(VAR\\) i.i.d. de même loi que \\(X\\), admettant une espérance \\(\\mu\\). La moyenne empirique est l’estimateur \\[\\overline{X_n}=\\frac{X_1+\\dots + X_n}{n}\\] Théorème : Quelle que soit la loi suivie par \\(X\\), la moyenne empirique \\(\\overline{X_n}\\) est un estimateur sans biais de l’espérance \\(\\mu=\\mathbb{E}(X)\\). Si, de plus, \\(X\\) admet une variance \\(\\sigma^2\\), alors \\(\\overline{X_n}\\) admet également une variance et celle-ci est donnée par \\(\\mathbb{V}(\\overline{X_n})=\\frac{\\sigma^2}{n}\\). **Démonstration.** Par linéarité de l’espérance : \\(\\mathbb{E}(\\overline{X_n})=\\frac{1}{n}\\sum\\limits_{i=1}^n\\mathbb{E}(X_i)=\\frac{1}{n}\\sum\\limits_{i=1}^n\\mu=\\mu\\). Si \\(X\\) admet une variance, alors \\(\\overline{X_n}\\) aussi et \\(\\mathbb{V}(\\overline{X_n})=\\frac{1}{n^2}\\sum\\limits_{i=1}^n\\mathbb{V}(X_i)=\\frac{\\sigma^2}{n}\\), par indépendance de \\(X_1,\\dots X_n\\). \\(\\square\\) Corollaire : La moyenne empirique est un estimateur convergent de l’espérance (lorsqu’elle existe). **Démonstration.** L’estimateur \\(\\overline{X_n}\\) est sans biais et \\(\\mathbb{V}(\\overline{X_n})=\\frac{\\sigma^2}{n}\\longrightarrow 0\\). C’est donc un estimateur convergent de l’espérance. \\(\\square\\). **Exemple 2 : estimation d’une proportion.** Au sein d’une population, une proportion \\(p\\) d’individus présente une caractéristique. On suppose que la présence de cette caractéristique est distribuée de façon identique et indépendante d’un individu à l’autre suivant la loi de Bernoulli de paramètre \\(p\\). On peut donc estimer la proportion \\(p\\) au niveau population par la proportion \\(\\widehat{p_n}\\) au niveau échantillon : cet estimateur est la moyenne empirique, il est sans biais et de variance (inconnue) \\(\\frac{p(1-p)}{n}\\). **Exemple 3 : variance empirique.** Si \\(X\\) admet une variance \\(\\sigma^2\\) et \\(X_1,\\dots, X_n\\) sont i.i.d. de même loi que \\(X\\), alors un estimateur de \\(\\sigma^2\\) est donné par la variance empirique \\(S_n^{&#39;2}=\\frac{1}{n}\\sum\\limits_{i=1}^n(X_i-\\overline{X_n})^2\\). La variance empirique \\(S_n^{&#39;2}\\) est un estimateur biaisé de la variance \\(\\sigma^2\\). Plus précisément, on a \\[\\mathbb{E}(S_n^{&#39;2})=\\frac{n-1}{n}\\sigma^2\\] **Démonstration.** \\[\\begin{align} \\mathbb{E}(S_n^{&#39;2}) &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n\\mathbb{E}(X_i^2)-\\frac{2\\overline{X_n}}{n}\\sum\\limits_{i=1}^n\\mathbb{E}(X_i)+\\frac{1}{n}\\sum\\limits_{i=1}^n\\mathbb{E}(\\overline{X_n}^2) \\\\ &amp;= \\mathbb{E}(X^2)-\\mathbb{E}(\\overline{X_n}^2) \\end{align}\\] Par ailleurs : \\[\\begin{align} \\mathbb{E}(\\overline{X_n}^2)&amp;=\\frac{1}{n^2}\\sum\\limits_{i=1}^n\\mathbb{E}(X_i^2)+\\frac{1}{n^2}\\sum\\limits_{i\\neq j}\\mathbb{E}(X_iX_j) \\\\ &amp;= \\frac{1}{n}^2\\sum\\limits_{i=1}^n\\mathbb{E}(X_i^2)+\\frac{1}{n^2}\\sum\\limits_{i\\neq j}\\mathbb{E}(X_i)\\mathbb{E}(X_j) \\\\ &amp;= \\frac{1}{n}\\mathbb{E}(X^2)+\\frac{n-1}{n}\\left(\\mathbb{E}(X)\\right)^2 \\end{align}\\] Avec la formule de Huygens \\(\\mathbb{V}(X)=\\mathbb{E}(X^2)-\\mathbb{E}(X)^2\\), on en déduit que \\[\\mathbb{E}(S_n^{&#39;2})=\\frac{n-1}{n}\\sigma^2\\] \\(\\square\\) **Remarque.** Le biais de l’estimateur \\(S_n^{&#39;2}\\) devient cependant très faible pour \\(n\\) suffisamment grand. Il s’agit d’un estimateur asymptotiquement sans biais de la variance \\(\\sigma^2\\) : \\(\\mathbb{E}(S_n^{&#39;2})\\longrightarrow \\sigma^2\\). **Exemple 4 : variance empirique corrigée.** En modifiant l’estimateur de la variance empirique par un petit facteur correctif, on obtient un estimateur sans biais de la variance. Il suffit de poser \\[S_n^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(X_i-\\overline{X_n})^2\\] Cet estimateur s’appelle la variance empirique corrigée. La variance empirique corrigée \\(S_n^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n (X_i-\\overline{X_n})^2\\) est un estimateur sans biais de la variance \\(\\sigma^2=\\mathbb{V}(X)\\) : \\[\\mathbb{E}(S_n^2)=\\sigma^2\\] 6.1.4 Méthodes de construction des estimateurs On présente ici deux méthodes classiques de construction des estimateurs ; la méthode des moments et la méthode du maximum de vraisemblance. 6.1.4.1 La méthode des moments La méthode des moments Soit \\(X\\) une variable aléatoire réelle de loi \\(\\mathcal{L}_{\\theta}\\), où \\(\\theta\\) est un paramètre inconnu. On considère une fonction \\(f\\) de \\(I\\subset\\mathbb{R}\\) dans \\(\\mathbb{R}\\) telle que \\(f(X)\\) admette une espérance. Comme la loi de \\(X\\) dépend de \\(\\theta\\), il en est de même de \\(\\mathbb{E}(f(X))\\). La méthode des moments suppose qu’on sait expliciter une telle dépendance, i.e. qu’on connaisse une fonction \\(g\\) telle que \\[\\mathbb{E}(f(X))=g(\\theta)\\] La contrepartie empirique du membre de gauche de cette égalité est \\(\\frac{1}{n}\\sum\\limits_{i=1}^n f(X_i)\\), et la méthode des moments consiste alors à résoudre l’équation en \\(\\widehat{\\theta}\\) : \\[g(\\widehat{\\theta})=\\frac{1}{n}\\sum\\limits_{i=1}^n f(X_i)\\] **Exemple 5 : estimation du paramètre d’une loi exponentielle.** Soit \\(X\\sim\\mathcal{E}(\\lambda)\\), où \\(\\lambda&gt;0\\) est un paramètre inconnu que l’on veut estimer. La variable aléatoire \\(X\\) admet une espérance, et celle-ci est donnée par \\(\\mathcal{E}(X)=\\frac{1}{\\lambda}\\). La méthode des moments consiste alors à résoudre l’équation \\[\\frac{1}{\\widehat{\\lambda_n}}=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\] Cette équation est très simple, elle admet pour solution \\[\\widehat{\\lambda_n}=\\frac{1}{\\overline{X_n}}\\] C’est l’estimateur que l’on obtient par la méthode des moments. **Remarque.** En reprenant les notations explicitées ci-dessus, on peut identifier les fonctions \\(f\\) et \\(g\\) : \\[f(x)=x\\] \\[g(x)=\\frac{1}{x}\\] et ici évidemment \\(\\theta=\\lambda\\). En général, la méthode des moments s’utilise de façon complètement intuitive sans qu’on ait même à expliciter forcément les fonctions \\(f\\) et \\(g\\). 6.1.4.2 La méthode du maximum de vraisemblance Une autre méthode de construction d’estimateurs est celle du maximum de vraisemblance. L’idée générale de cette méthode est la suivante. On suppose qu’on dispose de réalisations \\(x_1,\\dots x_n\\) d’une même variable aléatoire, dont la loi appartient à une famille paramétrique \\(\\left\\{\\mathcal{L}_{\\theta}\\,;\\,\\theta\\in\\Theta\\right\\}\\) et on cherche à estimer \\(\\theta\\). Si par exemple on dispose d’une série de cinq obersations \\((0.12, -0.65, 1.35, 1.04, -1.19, 0.08)\\) et qu’on veut inférer sur \\(\\theta\\) à partir de ces observations, on est enclin à penser que la valeur \\(\\theta=0\\) est plus plausible que la valeur \\(\\theta=-10\\). La vraisemblance est une formalisation de l’idée intuitive de plausibilité d’un paramètre à partir d’une observation ou d’un ensemble d’observations. A nouveau, \\(X\\) désigne une variable aléatoire de loi dépendant d’un paramètre inconnu \\(\\theta\\), et \\(x\\) une réalisation de \\(X\\). La vraisemblance \\(L(x,.)\\) est une fonction de \\(\\theta\\) définie par \\[L(x;\\theta)=\\left\\{ \\begin{array}{lll} \\mathbb{P}_{\\theta}(X=x) &amp;\\text{; si } X \\text{ est discrète} \\\\ f(x;\\theta) &amp;\\text{; si } X \\text{ est une continue de densité } f(.;\\theta) \\\\ \\end{array} \\right.\\] **Remarque :** D’autres notations existent dans la littérature, comme \\(L(x|\\theta), \\mathbb{P}(X=x|\\theta), f(x|\\theta)\\). Ces notations viennent de la statistique bayésienne (hors programme du concours) qui envisage \\(\\theta\\) comme une variable aléatoire de distribution inconnue. Dans ce cas, la vraisemblance s’interprète comme une probabilité ou une densité de probabilité. La définition précédente s’étend au cas d’un échantillon \\((X_1,\\dots X_n)\\) de VA de même loi que \\(X\\). Un cas particulier important est celui où ces VA sont i.i.d. Dans ce cas, la vraisemblance est définie par \\[L(x;\\theta)=L(x_1,\\dots,x_n ; \\theta) \\left\\{ \\begin{array}{lll} \\prod\\limits_{i=1}^n\\mathbb{P}_{\\theta}(X_i=x_i) &amp;\\text{; si } X \\text{ est discrète} \\\\ \\prod\\limits_{i=1}^n f(x_i,\\theta) &amp;\\text{; si } X \\text{ est une continue de densité } f(.;\\theta) \\\\ \\end{array} \\right.\\] La méthode du maximum de vraisemblance consiste juste à dire que si toute l’information dont on dispose sur la variable aléatoire \\(X\\) est l’observation de l’échantillon \\((x_1, \\dots, x_n)\\), alors la meilleure estimation que l’on puisse faire de \\(\\theta\\) à partir de cette information est celle qui maximise la fonction de vraisemblance. Autrement dit, on cherche la valeur de \\(\\theta\\) qui rend l’observation \\((x_1,\\dots, x_n)\\) la plus plausible. Formellement : Méthode du maximum de vraisemblance Etant donné une collection de \\(n\\) réalisations \\(x=(x_1,\\dots, x_n)\\) des VA \\((X_1,\\dots X_n)\\) de même loi \\(\\mathcal{L}_{\\theta}\\) de paramètre inconnu \\(\\theta\\), on appelle estimation du maximum de vraisemblance toute estimation \\(\\widehat{\\theta}_n=\\widehat{\\theta}_n(x_1,\\dots,x_n)\\) vérifiant \\[\\widehat{\\theta}_n\\in \\arg\\max\\limits_{\\theta\\in\\Theta}L(x;\\theta)\\] **Cas particulier :** si la fonction \\(\\theta\\mapsto L(x;\\theta)\\) est deux fois dérivable sur \\(\\Theta\\), alors on peut chercher à résoudre (en $) le système \\[\\left\\{ \\begin{array}{lll} \\frac{\\partial}{\\partial\\theta}L(x;\\theta)=0 \\\\ \\frac{\\partial^2}{\\partial\\theta^2}L(x;\\theta)&lt;0 \\\\ \\end{array} \\right.\\] Les solutions de ce système fournissent des estimations par maximum de vraisemblance. **Log-vraisemblance.** Il est souvent plus commode de considérer la log-vraisemblance \\(l(x;\\theta)=\\ln L(x;\\theta)=\\sum\\limits_{i=1}^n \\ln L(x_i;\\theta)\\). La fonction \\(\\ln\\) étant croissante sur \\(\\mathbb{R}_{+}^*\\), maximiser la vraisemblance équivaut à maximiser la log-vraisemblance. Méthode du maximum de vraisemblance (version log-vraisemblance) En supposant que \\(L(x;\\theta)&gt;0\\) pour tout \\(\\theta\\in\\Theta\\), on note \\(l(x;\\theta)=\\ln L(x;\\theta)\\) la log-vraisemblance. Sous les mêmes hypothèses que ci-dessus, on cherche \\[\\widehat{\\theta}_n\\in\\arg\\max\\limits_{\\theta\\in\\Theta}\\left(\\ln L(x;\\theta)\\right)\\] **Cas particulier :** si la fonction \\(\\theta\\mapsto L(x;\\theta)\\) est deux fois dérivable sur \\(\\Theta\\), alors la fonction \\(\\theta\\mapsto l(x;\\theta)\\) l’est aussi et on peut chercher à résoudre (en \\(\\theta\\)) le système \\[\\left\\{ \\begin{array}{lll} \\frac{\\partial}{\\partial\\theta}l(x;\\theta)=0 \\\\ \\frac{\\partial^2}{\\partial\\theta^2}l(x;\\theta)&lt;0 \\\\ \\end{array} \\right.\\] Les solutions de ce système fournissent des estimations par maximum de vraisemblance. **Exemple 6 : loi normale** \\(\\mathcal{N}(\\mu, 1)\\). On veut estimer le paramètre inconnu \\(\\mu\\) par maximum de vraisemblance. La vraisemblance est donnée par \\[\\begin{align} L(x;\\mu)&amp;=\\prod\\limits_{i=1}^n\\left(\\frac{e^{-\\frac{(x_i-\\mu)^2}{2}}}{\\sqrt{2\\pi}}\\right)\\\\ &amp;=\\frac{1}{(2\\pi)^{\\frac{n}{2}}}e^{-\\sum\\limits_{i=1}^n (x_i-\\mu)^2} \\end{align}\\] La log-vraisemblance est plus facile à manipuler : \\[\\begin{align} l(x;\\mu)&amp;=\\ln L(x;\\mu) \\\\ &amp;= -\\frac{n}{2}\\ln(2\\pi)-\\sum\\limits_{i=1}^n(x_i-\\mu)^2 \\end{align}\\] La fonction \\(\\mu\\mapsto l(x;\\mu)\\) est deux fois dérivable sur \\(\\mathbb{R}\\) et \\(\\frac{\\partial}{\\partial\\mu}l(x;\\mu)=2\\sum_{i=1}^n(\\mu-x_i)\\). Une seule valeur de \\(\\mu\\) l’annule : \\[\\widehat{\\mu}_n=\\frac{1}{n}\\sum\\limits_{i=1}^n x_i=\\overline{x}_n\\] Par ailleurs \\(\\frac{\\partial^2}{\\partial\\mu^2}l(x;\\mu)=2n&gt;0\\), et donc \\(\\widehat{\\mu}_n\\in\\arg\\max\\limits_{\\mu\\in\\mathbb{R}}l(x;\\mu)\\). Finalement, un estimateur par maximum de vraisemblance est donné par \\[\\widehat{\\mu}_n=\\overline{X}_n=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\] **Remarque.** Comme souvent en statistique, on commet un léger abus de notation en désignant par la même lettre l’estimateur \\(\\widehat{\\mu}_n=\\frac{X_1+\\dots+X_n}{n}=\\widehat{\\mu}_n(X_1,\\dots,X_n)\\) (qui est une statistique, i.e. une fonction de \\((X_1,\\dots,X_n\\)) et l’estimation \\(\\widehat{\\mu}_n=\\frac{x_1+\\dots+x_n}{n}=\\widehat{\\mu}_n(x_1,\\dots, X_n)\\) qui en est une réalisation. Conditionnellement à \\((X_1,\\dots,X_n)\\) (i.e. si l’on suppose que l’on observe \\((X_1,\\dots, X_n)\\)) ces deux objets sont bien les mêmes. **Exemple 7 : loi expoentielle.** Soit \\((X_1,\\dots,X_n)\\) un échantillon i.i.d. tiré selon une loi exponentielle \\(\\mathcal{E}(\\lambda)\\) de paramètre \\(\\lambda&gt;0\\) inconnu. La vraisemblance est donnée par \\[\\begin{align} L(x;\\lambda)&amp;=\\prod\\limits_{i=1}^n (\\lambda e^{-\\lambda x_i}\\mathbb{1}_{x_i\\geq 0}) \\\\ \\end{align}\\] Si l’un des \\(x_i\\) est négatif elle vaut \\(0\\). Sinon on calcule la log-vraisemblance \\[l(x;\\lambda)=n\\ln(\\lambda)-\\lambda\\sum_{i=1}^n x_i\\] La fonction \\(\\lambda\\in\\mathbb{R}_{+}^*\\mapsto l(x;\\lambda)\\) est deux fois dérivable et \\(\\frac{\\partial}{\\partial\\lambda}l(x;\\lambda)=\\frac{n}{\\lambda}-\\sum\\limits_{i=1}^n x_i\\), qui s’annule en \\(\\lambda=\\frac{n}{\\sum\\limits_{i=1}^n x_i}=\\frac{1}{\\overline{x}_n}\\). De plus, \\(\\frac{\\partial^2}{\\partial\\lambda^2}l(x;\\lambda)=-\\frac{n}{\\lambda^2}&lt;0\\) et donc à \\(x\\) fixé, \\(l(x;\\lambda)\\) atteint son maximum en \\(\\frac{1}{\\overline{x_n}}\\). L’estimateur du maximum de vraisemblance est donc \\(\\widehat{\\lambda}_n=\\frac{1}{\\overline{X}_n}\\). On remarque qu’on retrouve ici le même estimateur que celui obtenu par la méthode des moments. 6.1.5 Estimation des coefficients d’une régression linéaire 6.1.5.1 Présentation du modèle \\(X\\) et \\(Y\\) sont deux variables aléatoires pour lesquelles on dispose d’observations \\(x_1,\\dots, x_n\\) et \\(y_1,\\dots y_n\\). On considère le modèle \\[Y_i=aX_i+b+u_i\\] où \\((a,b)\\) est un couple de réels inconnus et \\(u_i\\) est un terme d’erreur (inconnu lui aussi). Le but est d’estimer des coefficients \\((a,b)\\) à partir de l’échantillons d’observations \\((x_i, y_i)\\) et de donner des propriétés des estimateurs obtenus sous certaines hypothèses. **Hypothèses du modèle.** On fait les hypothèses suivantes : (H1) : Les couples \\((X_i, Y_i)\\) sont i.i.d. (H2) : Les termes d’erreur \\(u_i\\) sont indépendants des \\(X_i\\) (H3) : \\(\\mathbb{E}(u_i|X_i)=0\\) (hypothèse d’exogénéité) (H4) : \\(\\mathbb{V}(u_i|X_i)=\\sigma_u^2\\) ne dépend pas de \\(X_i\\) (hypothèse d’homoscédasticité) (H5) : \\(u_i|X_i\\sim\\mathcal{N}(0, \\sigma_u^2)\\) (hypothèse de normalité des termes d’erreur) On présente deux approches différentes pour estimer \\(a\\) et \\(b\\) : par la méthode des moindres carrés et par maximum de vraisemblance. Bien que différentes, ces méthodes vont fournir les mêmes estimateurs. Avant cela, on rappelle quelques résultats classiques de statistique descriptive. 6.1.5.2 Rappels utiles Avant de présenter cette méthode, on rappelle des égalités qui àa la fois très utiles et très classiques. Il faut les connaître pour le concours et savoir les redémontrer. Moyenne, covariance, variance Pour \\(x=(x_1,\\dots, x_n)\\in\\mathbb{R}^n\\) on note \\(\\overline{x}_n=\\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\) la moyenne de \\(x\\) \\(\\sigma_x^2=\\frac{1}{n}\\sum\\limits_{i=1}^n(x_i-\\overline{x}_n)^2\\) la variance de \\(x\\) si de plus \\(y=(y_1,\\dots, y_n)\\), \\(\\sigma_{xy}=\\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)(y_i-\\overline{y}_n)\\) est la covariance de \\(x\\) et \\(y\\). On a alors les égalités suivantes : 1. \\(\\sigma_{xx}=\\sigma_x^2\\) 2. \\(\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)=0\\) 3. Différentes formules de la covariance : \\[\\begin{align} \\sigma_{xy} &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)(y_i-\\overline{y}_n) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n(x_i-\\overline{x}_n)y_i \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n x_i(y_i-\\overline{y}_n) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n x_iy_i-\\overline{x}_n\\overline{y}_n \\end{align}\\] 4. Différentes formules de la variance : \\[\\begin{align} \\sigma_x^2 &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)^2 \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n x_i^2-(\\overline{x}_n)^2 \\end{align}\\] Démonstration. **1.** Evidente 2. \\[\\begin{align} \\sum\\limits_{i=1}^n (x_i-\\overline{x}_n) &amp;= \\sum\\limits_{i=1}^n x_i -n\\overline{x}_n \\\\ &amp;= n\\overline{x}_n-n\\overline{x}_n \\\\ &amp; =0 \\end{align}\\] 3. \\[\\begin{align} \\sigma_{xy} &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)(y_i-\\overline{y}_n) \\\\ &amp;=\\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)y_i-\\frac{\\overline{y}_n}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)y_i \\end{align}\\] d’après l’égalité 2. Par symétrie des rôles joués par \\(x\\) et \\(y\\) on a donc aussi \\(\\sigma_{xy}=\\frac{1}{n}\\sum\\limits_{i=1}^n x_i(y_i-\\overline{y}_n)\\). On montre la dernière égalité : \\[\\begin{align} \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)(y_i-\\overline{y}_n) &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)y_i \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{i=1}^n x_iy_i-\\overline{x}_n\\frac{1}{n}\\sum\\limits_{i=1}^n y_i \\\\ &amp;=\\frac{1}{n}\\sum\\limits_{i=1}^n x_iy_i-\\overline{x}_n\\overline{y}_n \\end{align}\\] 4. On applique la dernière égalité de 4 dans le cas particulier où \\(x=y\\). On obtient alors \\[\\frac{1}{n}\\sum\\limits_{i=1}^n (x_i-\\overline{x}_n)^2=\\frac{1}{n}\\sum\\limits_{i=1}^n x_i^2-\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n x_i\\right)^2\\] \\(\\square\\) 6.1.5.3 Estimation de \\(a\\) et \\(b\\) par la méthode des moindres carrés On montre maintenant les formules des estimateurs de \\(a\\) et \\(b\\) par application de la méthode des moindres carrés : Estimation de \\(a\\) et \\(b\\) par la méthode des moindres carrés La méthode des moindres carrés consiste à minimiser l’erreur quadratique globale \\[E(\\alpha,\\beta)\\equiv\\sum\\limits_{i=1}^n (Y_i-\\alpha X_i-\\beta)^2\\] qui représente l’erreur globale faite en approchant \\(Y_i\\) par \\(\\alpha X_i+\\beta\\). Cette méthode fournit les estimateurs suivants de \\(a\\) et \\(b\\) : \\[\\begin{align} \\left\\{ \\begin{array}{ll} \\widehat{a} &amp;= \\frac{\\overline{\\sigma_{XY}}}{\\overline{\\sigma^2_X}} \\\\ \\widehat{b} &amp;= \\overline{Y}_n-\\widehat{a}\\overline{X}_n \\end{array} \\right. \\end{align}\\] où on note \\(\\overline{\\sigma_{XY}}=\\frac{1}{n}\\sum\\limits_{i=1}^n(X_i-\\overline{X}_n)(Y_i-\\overline{Y}_n)\\) et \\(\\overline{\\sigma^2_X}=\\overline{\\sigma_{XX}}=\\frac{1}{n}\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2\\). **Démonstration.** La fonction \\((\\alpha, \\beta)\\mapsto E(\\alpha, \\beta)\\) est deux fois dérivable par rapport à chacune de ses variables. Conditions de premier ordre (CPO) : Les conditions du premier ordre s’écrivent \\[\\begin{align} \\left\\{ \\begin{array}{ll} \\frac{\\partial}{\\partial\\alpha} E(\\alpha, \\beta) &amp;=0 \\\\ \\frac{\\partial}{\\partial\\beta} E(\\alpha, \\beta) &amp;=0 \\\\ \\end{array} \\right. \\end{align}\\] i.e. \\[\\begin{align} \\left\\{ \\begin{array}{ll} \\sum\\limits_{i=1}^n X_i Y_i-\\alpha\\sum\\limits_{i=1}^n X_i^2-\\beta\\sum\\limits_{i=1}^n X_i&amp;=0 \\\\ \\sum\\limits_{i=1}^n Y_i-\\alpha\\sum\\limits_{i=1}^n X_i-n\\beta &amp;= 0 \\\\ \\end{array} \\right. \\end{align}\\] Il s’agit d’un système de deux équations à deux inconnues \\((\\alpha, \\beta)\\). Sa résolution donne \\[\\begin{align} \\left\\{ \\begin{array}{ll} \\alpha &amp;= \\frac{\\frac{1}{n}\\sum\\limits_{i=1}^n X_iY_i-\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\right)\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n Y_i\\right)}{\\frac{1}{n}\\sum\\limits_{i=1}^n X_i^2-\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\right)^2} \\\\ \\beta &amp;= \\overline{Y}_n-\\alpha\\overline{X}_n \\end{array} \\right. \\end{align}\\] soit encore \\[\\begin{align} \\left\\{ \\begin{array}{ll} \\alpha &amp;= \\frac{\\overline{\\sigma_{XY}}}{\\overline{\\sigma^2_X}} \\\\ \\beta &amp;= \\overline{Y}_n-\\alpha\\overline{X}_n \\end{array} \\right. \\end{align}\\] Par ailleurs, pour tout couple \\((x, y)\\) de réels, la fonction \\((\\alpha, \\beta)\\mapsto (y-\\alpha x-\\beta)^2\\) est convexe. Le point critique trouvé ci-dessus est donc un minimum. On en déduit le résultat. \\(\\square\\) 6.1.5.4 Estimation de \\(a\\) et \\(b\\) par la méthode du maximum de vraisemblance La méthode par maximum de vraisemblance requiert une information supplémentaire : celle de la distribution de la variable de terme d’erreur \\(u\\). Or, une telle information est justement donnée ici par l’hypothèse (H5) de distribution normale du terme d’erreur. Estimation de \\(a\\) et \\(b\\) par la méthode du maximum de vraisemblance. Sous l’hypothèse \\((H5)\\) de distribution normale des termes d’erreur, la méthode par maximum de vraisemblance fournit les mêmes estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) que la méthode des moindres carrés. **Démonstration.** La vraisemblance est donnée par \\[L((\\alpha,\\beta);u)=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi}\\sigma_u}e^{-\\frac{(Y_i-\\alpha X_i-\\beta)^2}{2\\sigma_u^2}}\\] On passe à la log-vraisemblance, qui est plus simple à dériver \\[l((\\alpha,\\beta);u)=-n\\ln(\\sqrt{2\\pi}\\sigma_u^2)-\\frac{(Y_i-\\alpha X_i-\\beta)^2}{2\\sigma_u^2}\\] On résout alors en \\((\\alpha, \\beta)\\) le système d’équations \\[\\begin{align} \\frac{\\partial l}{\\partial\\alpha}l((\\alpha,\\beta);u) &amp;= 0 \\\\ \\frac{\\partial l}{\\partial\\beta}l((\\alpha,\\beta);u) &amp;= 0 \\\\ \\end{align}\\] soit \\[\\begin{align} \\frac{X_i(Y_i-\\alpha X_i-\\beta)}{2\\sigma_u^2} &amp;= 0 \\\\ \\frac{Y_i-\\alpha X_i-\\beta}{2\\sigma_u^2} &amp;= 0 \\\\ \\end{align}\\] On vérifie facilement qu’on obtient le même couple de solution qu’avec la méthode des moindres carrés, et que ce couple constitue bien un maximum de la log-vraisemblance. \\(\\square\\) **Remarque :** Dans des approches plus générales que celle présentée ici, aucune hypothèse n’est faite sur la distribution des termes d’erreur. Dans ce cas, la méthode par maximum de vraisemblance n’est plus applicable. On peut cependant toujours utiliser la méthode des moindres carrés. 6.1.5.5 Absence de biais des estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) Les estimateurs \\[\\widehat{a}=\\frac{\\overline{\\sigma_{XY}}}{\\overline{\\sigma_X^2}}\\] et \\[\\widehat{b}=\\overline{Y}_n-\\widehat{a}\\overline{X}_n\\] sont des estimateurs sans biais de \\(a\\) et \\(b\\). **Démonstration.** On remarque d’abord qu’avec l’hypothèse d’exogénéité (H3) \\(\\mathbb{E}(u_i|X_i)=0\\) on a \\(\\mathbb{E}(Y_i|X_i)=aX_i+b\\) et donc \\(\\mathbb{E}(Y_i-\\overline{Y}_n|X_1,\\dots, X_n)=a(X_i-\\overline{X}_n)\\). D’où \\[\\begin{align} \\mathbb{E}(\\widehat{a}|X_1,\\dots, X_n) &amp;= \\mathbb{E}\\left(\\left.\\frac{\\frac{1}{n}\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)(Y_i-\\overline{Y}_n)}{\\overline{\\sigma_X^2}}\\right|X_1,\\dots, X_n\\right) \\\\ &amp;=\\frac{1}{\\overline{\\sigma_X^2}}\\frac{1}{n}\\sum\\limits_{i=1}^n(X_i-\\overline{X}_n)\\mathbb{E}(\\left. Y_i-\\overline{Y}_n\\right|X_1\\,\\dots,X_n) \\\\ &amp;= a\\frac{1}{\\overline{\\sigma_X^2}}\\frac{1}{n}\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2 \\\\ &amp; =a\\frac{\\overline{\\sigma_X^2}}{\\overline{\\sigma_X^2}} \\\\ &amp; =a \\end{align}\\] Par ailleurs \\[\\begin{align} \\mathbb{E}(\\widehat{b}|X_1,\\dots,X_n)&amp;=\\mathbb{E}(\\overline{Y}_n-\\widehat{a}\\overline{X}_n|X_1,\\dots,X_n) \\\\ &amp;=\\frac{1}{n}\\sum\\limits_{i=1}^n \\mathbb{E}(Y_i|X_1,\\dots,X_n)-\\overline{X}_n\\mathbb{E}(\\widehat{a}|X_1,\\dots,X_n) \\\\ &amp;=\\frac{1}{n}\\sum\\limits_{i=1}^n (aX_i+b)-a\\overline{X}_n \\\\ &amp;= a\\overline{X}_n+b-a\\overline{X}_n \\\\ &amp;= b \\end{align}\\] \\(\\square\\) 6.1.5.6 Variance des estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) Variance des estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) Les estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) ont pour variances \\[\\begin{align} \\mathbb{V}(\\widehat{a}|X_1,\\dots, X_n) &amp;= \\frac{\\sigma_u^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2} \\\\ \\mathbb{V}(\\widehat{b}|X_1,\\dots,X_n) &amp;=\\sigma_u^2\\left(\\frac{1}{n}+ \\frac{\\overline{X}_n^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}\\right) \\end{align}\\] **Démonstration.** On remarque tout d’abord que \\[\\mathbb{V}(Y_i|X_1,\\dots,X_n)=\\sigma_u^2\\] En effet \\[\\begin{align} \\mathbb{V}(Y_i|X_1,\\dots,X_n) &amp;= \\mathbb{V}(aX_i+b+u_i|X_1,\\dots, X_n) \\\\ &amp;= \\mathbb{V}(u_i|X_1,\\dots,X_n) \\\\ &amp;= \\sigma_u^2 \\end{align}\\] Le passage de la première à la deuxième ligne vient du fait qu’à \\(X_1,\\dots, X_n\\) fixées, \\(aX_i+b\\) est considérée comme une constante, et donc ce terme a une contribution à la variance conditionnellement à \\(X_1,\\dots X_n\\). On a donc \\[\\begin{align} \\mathbb{V}(\\widehat{a}|X_1,\\dots X_n) &amp;= \\mathbb{V}\\left(\\left.\\frac{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)Y_i}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}\\right|X_1,\\dots, X_n\\right) \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2\\mathbb{V}(Y_i|X_1,\\dots,X_n)}{\\left(\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2\\right)^2} \\\\ &amp; \\text{ (somme de VA i.i.d.)} \\\\ &amp;= \\frac{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2\\sigma_u^2}{\\left(\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2\\right)^2} \\\\ &amp;= \\frac{\\sigma_u^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2} \\end{align}\\] et \\[\\begin{align} \\mathbb{V}(\\widehat{b}|X_1,\\dots,X_n) &amp;= \\mathbb{V}(\\overline{Y}_n-\\widehat{a}\\overline{X}_n|X_1,\\dots,X_n) \\\\ &amp;= \\mathbb{V}(a\\overline{X}_n+b+\\overline{u}_n-\\widehat{a}\\overline{X}_n|X_1,\\dots,X_n) \\\\ &amp;= \\mathbb{V}((a-\\widehat{a}\\overline{X}_n)+b+\\overline{u}_n|X_1,\\dots, X_n) \\\\ &amp;= \\overline{X}_n^2\\underbrace{\\mathbb{V}(\\widehat{a}|X_1,\\dots,X_n)}_{=\\frac{\\sigma_u^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}}+\\underbrace{\\mathbb{V}(\\overline{u}_n|X_1,\\dots,X_n)}_{=\\frac{\\sigma_u^2}{n} \\text{ car } u_i \\text{ i.i.d. de variance } \\sigma_u^2} \\\\ &amp;= \\overline{X}_n^2\\frac{\\sigma_u^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}+\\frac{\\sigma_u^2}{n} \\\\ &amp;= \\sigma_u^2\\left(\\frac{1}{n}+\\frac{\\overline{X}_n^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}\\right) \\end{align}\\] \\(\\square\\) 6.1.5.7 Résidus La variance \\(\\sigma_u^2\\) des termes d’erreur \\(u_i\\) n’est pas connue. Cependant, elle peut être estimée. Pour cela, on introduit la notion de résidu. Le résidu \\(\\widehat{u}_i\\) est défini comme l’écart entre la vraie valeur \\(Y_i\\) et sa prédiction \\(\\widehat{Y}_i=\\widehat{a}X_i+\\widehat{b}\\) : \\[\\widehat{u}_i\\equiv Y_i-\\widehat{Y}_i\\] On a donc \\[\\widehat{u}_i=Y_i-\\widehat{a}X_i-\\widehat{b}\\] Il s’agit d’une estimation (sans biais) de la vraie erreur \\[u_i=Y_i-aX_i-b\\] Estimation de la variance \\(\\sigma_u^2\\) La variance \\(\\sigma_u^2\\) est estimée par \\[s^2=\\frac{1}{n-2}\\sum\\limits_{i=1}^n \\widehat{u}_i^2\\] 6.1.5.8 Distributions des estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) On admet alors le résultat suivant Sous l’hypothèse de normalité des termes d’erreur \\(u_i\\), on a \\[\\frac{(n-2)s^2}{\\sigma_u^2}\\sim\\chi^2_{(n-2)}\\] et les statistiques \\[\\frac{\\widehat{a}-a}{s\\sqrt{\\frac{1}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}}}\\] et \\[\\frac{\\widehat{b}-b}{s\\sqrt{\\frac{1}{n}+\\frac{\\overline{X}_n^2}{\\sum\\limits_{i=1}^n (X_i-\\overline{X}_n)^2}}}\\] suivent une loi de Student à \\(n-2\\) degrés de liberté. 6.1.5.9 Convergence des estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) Si on suppose que les \\(X_i\\) admettent des moments d’ordre \\(1\\) et \\(2\\), alors on peut montrer que les estimateurs \\(\\widehat{a}\\) et \\(\\widehat{b}\\) sont des estimateurs convergents. On sait déjà qu’ils sont sans biais, il suffit donc de démontrer que leurs variances tendent vers \\(0\\). Or, comme \\(X_i\\) admet des moments d’ordres \\(1\\) et \\(2\\) on a, conditionnellement à \\(X_1,\\dots,X_n\\) : \\[\\overline{X}_n\\approx\\mathbb{E}(X)\\] et \\[\\sum\\limits_{i=1}^n(X_i-\\overline{X}_n)^2\\approx n\\mathbb{V}(X_1)\\] On en déduit que \\[\\mathbb{V}(\\widehat{a}|X_1,\\dots, X_n)\\approx\\frac{\\sigma_u^2}{n\\mathbb{V}(X_1)}\\longrightarrow 0\\] et \\[\\mathbb{V}(\\widehat{b}|X_1,\\dots,X_n)\\approx\\sigma_u^2\\left(\\frac{1}{n}+\\frac{\\overline{X}_n^2}{n\\mathbb{V}(X_1)}\\right)\\longrightarrow 0\\] \\(\\widehat{a}\\) et \\(\\widehat{b}\\) sont des estimateurs sans biais de \\(a\\) et \\(b\\) de variances asymptotiquement nulles. Ce sont donc des estimateurs convergents de \\(a\\) et \\(b\\). 6.1.6 Intervalles de confiance jusqu’à présent : estimation ponctuelle on veut être plus informatif et calculer la précision de cette estimation ponctuelle –&gt; intervalle de confiance méthode de construction classique d’un intervalle de confiance : utiliser le TCL permet de se ramener à une hypothèse de quasi-normalité donner plusieurs exemples 6.2 Tests statistiques 6.2.1 Définition et principes approche très intuitive (d’après le programme du concours) 6.2.2 Exemples de test le but est avant tout de montrer la démarche générale d’un test statistique à travers quelques exemples simples "],["references.html", "References", " References "],["introduction.html", "Chapter 7 Introduction", " Chapter 7 Introduction Ce cours de probabilités et statistique est destiné aux candidats au concours interne d’Administrateur de l’Insee. Il est encore en chantier : à ce stade, seule la partie Statistique Inférentielle est écrite. Si vous repérez une erreur ou une coquille, n’hésitez pas me la signaler. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
