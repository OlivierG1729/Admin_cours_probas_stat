[["convergence.html", "Chapitre 5 Convergence 5.1 Différents modes de convergence 5.2 Lois des grands nombres 5.3 Théorème Central Limite (TCL) 5.4 Variantes du TCL (hors-programme)", " Chapitre 5 Convergence Dans tout ce qui suit, toutes les variables aléatoires considérées sont définies sur un espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) donné, et sont à valeurs dans \\(\\mathbb{R}\\) (ou éventuellement \\(\\mathbb{R}^d\\), avec \\(d\\in\\mathbb{R}^{*}\\)). 5.1 Différents modes de convergence 5.1.1 Convergence en probabilité Convergence en probabilité Soient \\((X_n)_{n\\in\\mathbb{N}}\\) une suite de variables aléatoires. On dit que \\((X_n)_n\\) converge en probabilité vers une variable aléatoire \\(X\\) lorsque : \\[\\forall\\varepsilon &gt;0,\\,\\lim\\limits_{n\\to +\\infty}\\mathbb{P}\\left(|X_n-X|\\geq\\varepsilon\\right)=0\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}X\\] Remarques. De façon équivalente, \\((X_n)\\) converge en probabilité vers \\(X\\) lorsque \\[\\forall\\varepsilon &gt;0,\\,\\lim\\limits_{n\\to +\\infty}\\mathbb{P}\\left(|X_n-X|&lt;\\varepsilon\\right)=1\\] Exemple. Soit \\(X:\\Omega\\longrightarrow\\mathbb{R}\\) une variable aléatoire. On considère la suite de variables aléatoires \\((X_n)_n\\) définies par \\(X_n=\\frac{E(nX)}{n}\\), où \\(E(.)\\) désigne la fonction partie entière. Démontrons que \\((X_n)_n\\) converge en probabilité vers \\(X\\). On a \\[\\begin{align*} \\left|X-\\frac{E(nX)}{n}\\right|&amp;=\\left|nX-\\frac{E(nX)}{n}\\right| \\\\ &amp;\\leq\\frac{1}{n} \\end{align*}\\] car tout réel \\(x\\) est à une distance au plus \\(1\\) de sa partie entière : \\(\\forall x\\in\\mathbb{R}, |x-E(x)|\\leq 1\\). Soit \\(\\varepsilon &gt;0\\). Comme \\(\\lim\\limits_{n\\to +\\infty}\\frac{1}{n}=0\\), il existe un entier naturel \\(n_0\\) non nul tel que \\[\\forall n\\in\\mathbb{N}, n\\geq n_0\\Rightarrow\\frac{1}{n}&lt;\\varepsilon\\] et donc en particulier \\[\\forall n\\in\\mathbb{N}, n\\geq n_0\\Rightarrow\\left|X-\\frac{E(nX)}{n}\\right|&lt;\\varepsilon\\] d’où \\[\\forall n\\in\\mathbb{N}, n\\geq n_0\\Rightarrow\\mathbb{P}\\left(\\left|X-\\frac{E(nX)}{n}\\right|&lt;\\varepsilon\\right)=1\\] et donc finalement \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{P}\\left(\\left|X-\\frac{E(nX)}{n}\\right|&lt;\\varepsilon\\right)=1\\] autrement dit \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}X\\] \\(\\square\\) Les inégalités de concentration constituent souvent un outil efficace pour démontrer une convergence en probabilité : Inégalité de Markov pour démontrer que \\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}X\\) On suppose que : chacune des variables \\(X_n\\) et \\(X\\) admet une espérance \\(\\mathbb{E}(|X_n-X|^p)\\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\), pour un certain \\(p&gt;0\\) Le premier point permet d’écrire l’inégalité de Markov (appliquée à la variable aléatoire positive \\(|X_n-X|^p\\)) : \\[\\begin{align*} \\mathbb{P}\\left(|X_n-X|\\geq\\varepsilon\\right)&amp;=\\mathbb{P}\\left(|X_n-X|^p\\geq\\varepsilon^p\\right) \\\\ &amp;\\leq\\frac{\\mathbb{E}\\left(|X_n-X|^p\\right)}{\\varepsilon^p} \\end{align*}\\] la première égalité venant du fait que l’application \\(x\\mapsto x^p\\) est strictement croissante sur \\(\\mathbb{R}_+\\). Le deuxième point permet alors de conclure que \\[\\forall\\varepsilon &gt;0,\\, \\mathbb{P}(|X_n-m|\\geq\\varepsilon) \\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\] autrement dit que \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}X\\] Remarque. On vient de démontrer que la convergence dans \\(L^p\\) (ce mode de convergence est défini juste après) pour \\(p&gt;0\\) implique la convergence en probabilité. Exemple. [Max de lois uniformes indépendantes]. Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. de loi uniforme \\(\\mathcal{U}([0\\,;\\,1])\\). On pose \\[Y_n=\\max(X_1,\\dots,X_n)\\] Démontrons que \\((Y_n)_n\\) converge en probabilité vers \\(1\\) en utilisant l’inégalité de Markov appliquée à la variable aléatoire positive \\(1-Y_n\\). Pour cela, on a besoin de l’espérance de \\(1-Y_n\\), et donc de l’espérance de \\(Y_n\\). Pour calculer \\(\\mathbb{E}(Y_n)\\) on commence par calculer la fonction de répartition \\(F_{Y_n}\\) de \\(Y_n\\). Soit \\(y\\) un réel. Si \\(y\\leq 0\\) on a \\(F_{Y_n}(y)=0\\) et si \\(y&gt;1\\) on a \\(F_{Y_n}(y)=0\\). Soit maintenant \\(y\\in [0,1]\\). Alors, on a \\[\\begin{align*} F_{Y_n}(y)&amp;=\\mathbb{P}(Y_n\\leq y) \\\\ &amp;=\\mathbb{P}(\\max(X_1,\\dots, X_n)\\leq y) \\\\ &amp;=\\mathbb{P}\\left(\\bigcap\\limits_{i=1}^n \\{X_i\\leq y\\} \\right) \\\\ &amp;=\\prod\\limits_{i=1}^n\\mathbb{P}(X_i\\leq x) \\\\ &amp;=\\prod\\limits_{i=1}^n x \\\\ &amp;=x^n \\end{align*}\\] donc la densité \\(f_{Y_n}\\) de \\(Y_n\\) est donnée par \\[f_{Y_n}(y)=nx^{n-1}.1_{[0,1]}(y)\\] D’où l’espérance de \\(Y_n\\) : \\[\\begin{align*} \\mathbb{E}(Y_n)&amp;=\\int_0^1 y.f_{Y_n}(y)\\,dy \\\\ &amp;=n\\int_0^1 y^n\\,dy \\\\ &amp;=\\frac{n}{n+1} \\end{align*}\\] Soit \\(\\varepsilon &gt;0\\). L’inégalité de Markov appliquée à la variable \\(|Y_n-1|\\) donne \\[\\begin{align*} \\mathbb{P}(|Y_n-1|\\geq\\varepsilon)\\leq\\frac{\\mathbb{E}(1-Y_n)}{\\varepsilon^2} \\\\ &amp;=\\frac{1-\\frac{n}{n+1}}{\\varepsilon^2} \\\\ &amp;=\\frac{1}{(n+1)\\varepsilon^2} \\end{align*}\\] Or, \\(\\frac{1}{(n+1)\\varepsilon^2}\\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\), ce qui permet de conclure que \\((Y_n)_n\\) converge bien en probabilité vers \\(1\\). \\(\\square\\) Inégalité de Bienaymé-Tchebytchev pour démontrer que \\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}m\\) Cette inégalité peut être utilisée pour démontrer la convergence en probabilité vers une constante. On suppose que : les variables aléatoires \\(X_n\\) admettent une espérance commune \\(\\mathbb{E}(X_n)=m\\) chacune d’elles admet également une variance \\(\\mathbb{V}(X_n)\\) \\(\\lim\\limits_{n\\to +\\infty}\\mathbb{V}(X_n)=0\\). Les deux premiers points permettent d’écrire l’inégalité de Bienaymé-Tchebytchev : \\[\\forall\\varepsilon &gt;0, \\,\\mathbb{P}\\left(|X_n-m|\\geq\\varepsilon\\right)\\leq\\frac{\\mathbb{V}(X_n)}{\\varepsilon^2}\\] Le dernier point permet de conclure que \\[\\forall\\varepsilon &gt;0,\\, \\mathbb{P}(|X_n-m|\\geq\\varepsilon) \\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\] autrement dit que \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}m\\] Exemple. [Convergence en probabilité de la moyenne empirique]. Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. de loi \\(\\mathcal{E}(\\lambda)\\), où \\(\\lambda&gt;0\\). On considère, pour tout entier naturel \\(n\\geq 1\\) la moyenne empirique \\[\\overline{X_n}=\\frac{1}{n}\\sum\\limits_{i=1}^n X_i\\] Démontrons que \\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}\\frac{1}{\\lambda}\\). Ce résultat est une conséquence immédiate de la loi faible des grands nombres (que l’on énonce un peu plus loin), mais ici nous allons procéder autrement en utilisant l’inégalité de Bienaymé-Tchebytchev. Tout d’abord, la limite supposée \\(\\frac{1}{\\lambda}\\) est une constante, ce qui incite à penser à cette inégalité. Ensuite, les \\((X_n)_n\\) admettent toutes une espérance et une variance - ce qui confirme que nous pouvons utiliser cette inégalité - et \\(\\forall n\\in\\mathbb{N}^*,\\,\\mathbb{E}(X_n)=\\frac{1}{\\lambda}\\) \\(\\forall n\\in\\mathbb{N}^*, \\mathbb{V}(X_n)=\\frac{1}{\\lambda^2}\\) On en déduit par linéarité que \\(\\mathbb{E}(\\overline{X_n})=m\\) et par indépendance des \\(X_n\\) que \\[\\begin{align*} \\mathbb{V}\\left(\\overline{X_n}\\right)&amp;=\\frac{1}{n^2}\\sum\\limits_{i=1}^n \\mathbb{V}(X_i) \\\\ &amp;=\\frac{1}{n^2}\\sum\\limits_{i=1}^n\\frac{1}{\\lambda^2} \\\\ &amp;=\\frac{1}{n\\lambda^2} \\end{align*}\\] L’utilisation de l’inégalité de Bienaymé-Tchebytchev donne donc, pour tout \\(\\varepsilon &gt;0\\) : \\[\\begin{align} \\mathbb{P}\\left(\\left|\\overline{X_n}-\\frac{1}{\\lambda}\\right|\\geq\\varepsilon\\right)&amp;\\leq\\frac{\\mathbb{V}\\left(\\overline{X_n}\\right)}{\\varepsilon^2} \\\\ &amp;=\\frac{1}{\\varepsilon^2 n\\lambda^2\\varepsilon^2} \\end{align}\\] et donc \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{P}\\left(\\left|\\overline{X_n}-\\frac{1}{\\lambda}\\right|\\geq\\varepsilon\\right)=0\\] ce qui permet de conclure que \\(X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}\\frac{1}{\\lambda}\\). Le résultat suivant fournit un critère simple pour démontrer une convergence en probabilité. Il s’agit d’une condition suffisante, qui fonctionne assez souvent en pratique. Convergence en probabilité : conditions suffisantes Soient \\(a\\) un nombre réel et \\((X_n)_{n}\\) une suite de variables aléatoires admettant une espérance et une variance. Si \\[\\mathbb{E}(X_n) \\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}a\\] et \\[\\mathbb{V}(X_n) \\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\] alors \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathbb{P}}{\\longrightarrow}}a\\] Démonstration. Exemple. Parler de Markov et de Bieanymé-Tchebytchev 5.1.2 Convergence dans les espaces \\(L^1\\) et \\(L^2\\) A nouveau, sans que cela ne soit plus mentionné par la suite, nous supposons ici que toutes les variables aléatoires évoquées sont définies sur un même espace probabilisé \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\). Par ailleurs, ces variables sont à valeurs dans \\(\\mathbb{R}\\). On pourrait toutefois facilement étendre les définitions qui suivent à des variables à valeurs dans \\(\\mathbb{R}^d\\) en remplaçant les valeurs absolues \\(|X_n-X|\\) par des normes \\(||X_n-X||\\) définies dans \\(\\mathbb{R}^d\\). 5.1.2.1 Les ensembles \\(L^1(\\Omega)\\) et \\(L^2(\\Omega)\\) Espaces \\(L^1(\\Omega)\\) et \\(L^2(\\Omega)\\) Soit \\(X\\) une variable aléatoire définie sur \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) et à valeurs dans \\(\\mathbb{R}\\). Espace \\(L^1(\\Omega)\\) On dit que \\(X\\in L^1(\\Omega)\\), ou encore que \\(X\\in L^1(\\Omega, \\mathcal{A}, \\mathbb{P})\\) lorsque \\[\\mathbb{E}(|X|)&lt;+\\infty\\] ce qui revient à dire que \\(X\\) admet une espérance. Selon le cas (discret fini, discret infini, à densité), cette condition s’écrit aussi sous l’une des formes suivantes : \\[\\sum\\limits_{n=0}^N \\mathbb{P}(X=x_n).|x_n|&lt;+\\infty\\] \\[\\sum\\limits_{n=0}^{+\\infty} \\mathbb{P}(X=x_n).|x_n|&lt;+\\infty\\] \\[\\int_{-\\infty}^{+\\infty}|x|.f(x)\\,dx&lt;+\\infty\\] avec \\(f\\) la densité de \\(X\\) dans le cas à densité. Espace \\(L^2(\\Omega)\\) On dit que \\(X\\in L^2(\\Omega)\\), ou encore que \\(X\\in L^2(\\Omega, \\mathcal{A}, \\mathbb{P})\\) lorsque \\[\\mathbb{E}(X^2)&lt;+\\infty\\] ce qui revient à dire que \\(X\\) admet un moment d’ordre 2. Selon le cas (discret fini, discret infini, à densité), cette condition s’écrit aussi sous l’une des formes suivantes : \\[\\sum\\limits_{n=0}^N \\mathbb{P}(X=x_n).x_n^2&lt;+\\infty\\] \\[\\sum\\limits_{n=0}^{+\\infty} \\mathbb{P}(X=x_n).x_n^2&lt;+\\infty\\] \\[\\int_{-\\infty}^{+\\infty}x^2.f(x)\\,dx&lt;+\\infty\\] avec \\(f\\) la densité de \\(X\\) dans le cas à densité. Remarques. i. Cette définition est suffisante pour le concours ii. Plus généralement, pour tout \\(p&gt;0\\) on peut définir l’ensemble \\(L^p(\\Omega)\\) comme l’ensemble des variables aléatoires réelles définies sur \\((\\Omega, \\mathcal{A}, \\mathbb{P})\\) telles que \\[\\mathbb{E}(|X|^p)&lt;+\\infty\\] et cette condition s’écrit, selon les cas, sous l’une des formes suivantes : \\[\\sum\\limits_{n=0}^N \\mathbb{P}(X=x_n).|x_n|^p&lt;+\\infty\\] \\[\\sum\\limits_{n=0}^{+\\infty} \\mathbb{P}(X=x_n).|x_n|^p&lt;+\\infty\\] \\[\\int_{-\\infty}^{+\\infty}|x|^p.f(x)\\,dx&lt;+\\infty\\] Le résultat qui suit est très classique : Théorème. On a l’inclusion \\[L^2(\\Omega)\\subset L^1(\\Omega)\\] autrement dit dès qu’une variable aléatoire admet un moment d’ordre \\(1\\) (une espérance), elle admet aussi un moment d’ordre \\(2\\) et donc une variance. Démonstration. Soit \\(X\\in L^2(\\Omega)\\), i.e. \\(\\mathbb{E}(X^2)&lt;+\\infty\\). On a \\[(|X|-1)^2\\geq 0\\] d’où \\[|X|\\leq\\frac{1+X^2}{2}\\] On déduit facilement de cette dernière inégalité que \\(\\mathbb{E}(|X|)&lt;+\\infty\\), i.e. que \\(X\\in L^1(\\Omega)\\). \\(\\square\\) Remarque. On peut aussi définir une notion d’espace \\(L^p(E, \\mathcal{A}, \\mu)\\) pour \\((E, \\mathcal{A}, \\mu)\\) un espace mesuré (hors-programme), mais dans ce cas l’inclusion précédente n’est plus vraie en toute généralité, autrement dit l’inclusion \\[L^2(E, \\mathcal{A}, \\mu)\\subset L^1(E, \\mathcal{A}, \\mu)\\] n’est pas automatique pour des espaces mesurés qui ne sont pas des espaces probabilisés (en fait, elle devient fausse si la mesure n’est pas finie, i.e. si \\(\\mu(E)=+\\infty\\)). On peut démontrer que \\(L^1(\\Omega)\\) et \\(L^2(\\Omega)\\) sont des espaces-vectoriels : Théorème. \\(L^1(\\Omega)\\) et \\(L^2(\\Omega)\\) sont des \\(\\mathbb{R}\\)-espaces-vectoriels. Démonstration. Il suffit de démontrer que \\(L^1(\\Omega)\\) et \\(L^2(\\Omega)\\) sont des sous-espaces-vectoriels du \\(\\mathbb{R}\\)-espace-vectoriel \\(\\mathbb{R}^{\\Omega}\\) des applications de \\(\\Omega\\) dans \\(\\mathbb{R}\\). Tout d’abord, l’application \\(0\\) qui à tout \\(\\omega\\) dans \\(\\Omega\\) associe le réel \\(0\\) admet clairement une espérance et un moment d’ordre \\(2\\) (tous les deux nuls), donc elle appartient à \\(L^1(\\Omega)\\) et à \\(L^2(\\Omega)\\). Soient \\(X,Y\\in L^1(\\Omega)\\) et \\(\\lambda\\in\\mathbb{R}\\). Alors : \\[\\begin{align} \\mathbb{E}\\left(|X+\\lambda Y|\\right) &amp;\\leq \\mathbb{E}\\left(|X|+|Y|\\right) \\\\ &amp;=\\mathbb{E}(|X|)+\\mathbb{E}(|Y|) \\\\ &amp;&lt;+\\infty \\end{align}\\] donc \\(X+\\lambda Y\\in L^1(\\Omega)\\) et \\(L^1(\\Omega)\\) est un sous-espace vectoriel de \\(\\mathbb{R}^{\\Omega}\\). Soient \\(X,Y\\in L^2(\\Omega)\\) et \\(\\lambda\\in\\mathbb{R}\\). Alors : \\[\\begin{align} \\mathbb{E}\\left((X+\\lambda Y)^2\\right) &amp;= \\mathbb{E}\\left(X^2+2\\lambda XY+\\lambda^2 Y^2 \\right) \\\\ &amp;=\\mathbb{E}(X^2)+2\\lambda\\mathbb{E}(XY)+\\lambda^2\\mathbb{E}(Y^2) \\\\ &amp;\\leq\\mathbb{E}(X^2)+2\\lambda\\sqrt{\\mathbb{E}(X^2).\\mathbb{E}(Y^2)}+\\lambda^2\\mathbb{E}(Y^2) \\\\ &amp; \\text{(d&#39;après l&#39;inégalité de Cauchy-Schwarz)} \\\\ &amp;&lt;+\\infty \\end{align}\\] donc \\(X+\\lambda Y\\in L^2(\\Omega)\\), ce qui permet de conclure que \\(L^2(\\Omega)\\) est un sous-espace vectoriel de \\(\\mathbb{R}^{\\Omega}\\). Convergence en moyenne d’ordre \\(1\\), en moyenne d’ordre \\(2\\) Soient \\(X\\) une variable aléatoire et \\((X_n)_{n\\in\\mathbb{N}}\\) une suite de variables aléatoires. Convergence dans \\(L^1\\). On suppose que \\(X\\) et les \\(X_n\\) admettent des espérances : \\(\\mathbb{E}(|X_n|)&lt;+\\infty\\) pour tout entier naturel \\(n\\), et \\(\\mathbb{E}(|X|)&lt;+\\infty\\). On dit alors que \\((X_n)_n\\) converge vers \\(X\\) dans \\(L^1\\), ou que \\((X_n)\\) converge en moyenne d’ordre \\(1\\) vers \\(X\\), lorsque : \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{E}\\left(|X_n-X|\\right)=0\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{L^1}{\\longrightarrow}}X\\] Convergence dans \\(L^2\\). On suppose que \\(X\\) et les \\(X_n\\) admettent des moments d’ordre \\(2\\) : \\(\\mathbb{E}(X_n^2)&lt;+\\infty\\) pour tout entier naturel \\(n\\), et \\(\\mathbb{E}(X^2)&lt;+\\infty\\). On dit alors que \\((X_n)_n\\) converge vers \\(X\\) dans \\(L^2\\), ou que \\((X_n)\\) en moyenne d’ordre \\(2\\) vers \\(X\\), lorsque : \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{E}\\left((X_n-X)^2\\right)=0\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{L^2}{\\longrightarrow}}X\\] Exemples. i. Soit \\(X_n\\) une variable aléatoire telle que \\[\\mathbb{P}(X_n=0)=1-\\frac{1}{n^2}\\] \\[\\mathbb{P}(X_n=n)=\\frac{1}{n^2}\\] Alors, \\((X_n)_n\\) converge vers \\(0\\) dans \\(L^1(\\Omega)\\). En effet : \\[\\begin{align*} \\mathbb{E}(|X_n|)&amp;=\\mathbb{E}(X_n) \\\\ &amp;=0.\\mathbb{P}(X_n=0)+n.\\mathbb{P}(X_n=n) \\\\ &amp;=n\\mathbb{P}(X_n=n) \\\\ &amp;=n.\\frac{1}{n^2} \\\\ &amp;=\\frac{1}{n} \\end{align*}\\] et donc \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{E}(|X_n|)=0\\] ii. Soient \\((p_n)_n\\) une suite de nombres réels de \\([0\\,;\\,1]\\) et \\((X_n)_n\\) une suite de variables aléatoires telles que \\[\\mathbb{P}(X_n=0)=1-p_n\\] \\[\\mathbb{P}(X_n=\\sqrt{n})=p_n\\] Déterminons une condition nécessaire et suffisante pour que \\((X_n)_n\\) converge vers \\(0\\) dans \\(L^2(\\Omega)\\), autrement dit pour que \\(\\mathbb{E}(X_n^2)\\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\). Tout d’abord, \\(X_n\\) a un univers fini, donc elle est dans \\(L^2(\\Omega)\\). Ensuite : \\[\\begin{align*} \\mathbb{E}(X_n^2)&amp;=(\\sqrt{n})^2.p_n \\\\ &amp;=np_n \\end{align*}\\] Donc, \\((X_n)_n\\) converge vers \\(0\\) dans \\(L^2(\\Omega)\\) si et seulement si \\(\\lim\\limits_{n\\to +\\infty} np_n=0\\), autrement dit si \\(p_n=o\\left(\\frac{1}{n}\\right)\\) lorsque \\(n\\) tend vers l’infini. \\(\\square\\) Le théorème suivant donne un lien entre convergence en moyenne quadratique (dans \\(L^2\\)) ey convergence en moyenne d’ordre \\(1\\) : Théorème (CV dans \\(L^2\\) implique CV dans \\(L^1\\)). Soit \\((X_n)_n\\) une suite de variables aléatoires réelles et \\(X\\) une variable aléatoire réelle, toutes dans \\(L^2(\\Omega)\\). Si \\[X_n \\underset{n \\to +\\infty}{\\overset{L^2}{\\longrightarrow}}X\\] alors \\[X_n \\underset{n \\to +\\infty}{\\overset{L^1}{\\longrightarrow}}X\\] Démonstration. La fonction \\(x\\in\\mathbb{R}\\mapsto x^2\\) est convexe, donc d’après l’inégalité de Jensen, on a \\[\\left(\\mathbb{E}|X_n-X|\\right)^2\\leq\\mathbb{E}((X_n-X)^2)\\] donc \\[0\\leq \\mathbb{E}(|X_n-X|)\\leq\\sqrt{\\mathbb{E}\\left((X_n-X)^2\\right)}\\] Si \\(X_n \\underset{n \\to +\\infty}{\\overset{L^2}{\\longrightarrow}}X\\) alors le membre de droite de l’encadrement ci-dessus tend vers \\(0\\) lorsque \\(n\\) tend vers l’infini. D’après le théorème d’encadrement, on a donc \\(\\lim\\limits_{n\\to +\\infty}\\mathbb{E}\\left(|X_n-X|\\right)=0\\), et donc \\(X_n \\underset{n \\to +\\infty}{\\overset{L^1}{\\longrightarrow}}X\\). \\(\\square\\) Remarque. Il existe plus généralement une notion de convergence dans \\(L^p\\), ou convergence en moyenne d’ordre \\(p\\), pour \\(p&gt;0\\), mais seuls les cas \\(p=1\\) et \\(p=2\\) sont au programme. Convergence en moyenne d’ordre \\(p\\) (hors-programme pour \\(p\\not\\in\\{1,2\\}\\)) Soit \\(p\\) un réel strictement positif. Soient \\(X\\) une variable aléatoire et \\((X_n)_{n\\in\\mathbb{N}}\\) une suite de variables aléatoires. On suppose que \\(X\\) et les \\(X_n\\) admettent des moments d’ordre \\(p\\) : \\(\\mathbb{E}(|X_n|^p)&lt;+\\infty\\) pour tout entier naturel \\(n\\), et \\(\\mathbb{E}(|X|^p)&lt;+\\infty\\). On dit alors que \\((X_n)_n\\) converge vers \\(X\\) dans \\(L^p\\), ou que \\((X_n)\\) en moyenne d’ordre \\(p\\) vers \\(X\\), lorsque : \\[\\lim\\limits_{n\\to +\\infty}\\mathbb{E}\\left(|X_n-X|^p\\right)=0\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{L^p}{\\longrightarrow}}X\\] 5.1.3 Convergence en loi Convergence en loi Soient \\(X\\) une variable aléatoire de fonction de répartition \\(F\\), et \\((X_n)_{n\\in\\mathbb{N}}\\) une suite de variables aléatoires de fonctions de répartition \\(F_n\\). On dit que \\((X_n)_n\\) converge en loi vers \\(X\\) lorsque, en tout point \\(x\\) en lequel \\(F\\) est continue, on a \\[\\lim\\limits_{n\\to +\\infty}F_n(x)=F(x)\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}}{\\longrightarrow}}X\\] Exemples. i. Pour tout entier naturel \\(n\\), on note \\(X_n\\) une variable aléatoire positive de densité \\(f_n\\) définie par \\(f_n(x)=ne^{-nx}\\) pour \\(x&gt;0\\). Démontrons que \\((X_n)_n\\) converge en loi vers \\(0\\). On note \\(F_n\\) la fonction de répartition de \\(X_n\\). On obtient alors \\[F_n(x)=(1-e^{-nx}).1_{\\mathbb{R}_+^{*}}(x)\\] La suite de fonctions \\((F_n)_n\\) converge simplement vers \\[F(x)=1_{\\mathbb{R}_+^{*}}(x)\\] qui est la fonction de répartition de la variable aléatoire certaine égale à \\(0\\). Ainsi \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}}{\\longrightarrow}}0\\] ii. Pour \\(n\\) entier naturel, on considère la variable aléatoire certaine \\(X_n=n\\). Etudions la convergence en loi de la suite \\((X_n)_n\\). On note \\(F_n\\) la fonction de répartition de \\(X_n\\). On a \\[F_n(x)=1_{[n\\,;\\,+\\infty[}(x)\\] et donc la suite \\((F_n)_n\\) converge simplement vers la fonction constante égale à \\(0\\). Mais cette fonction n’est pas une fonction de répartition, car \\(\\lim\\limits_{x\\to +\\infty} F(x)\\neq 1\\). Donc, la suite \\((X_n)_n\\) ne converge pas en loi. Remarque. La convergence simple de la suite des fonctions de répartition ne suffit pas pour établir une convergence en loi. Il faut, en plus, s’assurer que la limite est bien une fonction de répartition. 5.1.4 Convergence presque-sûre (hors-prgramme ?) Convergence presque sûre Soient \\(X\\) une variable aléatoire et \\((X_n)_{n\\in\\mathbb{N}}\\) une suite de variables aléatoires. On dit que \\((X_n)_n\\) converge presque sûrement vers \\(X\\) lorsque : \\[\\mathbb{P}\\left(\\{\\omega\\in\\Omega, X_n(\\omega)\\longrightarrow X(\\omega)\\right\\})=1\\] Notation : \\[X_n \\underset{n \\to +\\infty}{\\overset{p.s.}{\\longrightarrow}}X\\] 5.1.5 Liens entre les différents modes de convergence Liens entre les différents modes de convergence. Les liens entre les différents modes de convergence sont donnés par le schéma ci-dessous : Remarque. D’après le schéma ci-dessus, la convergence en loi est donc le mode de convergence le plus faible. Dit autrement, pour qu’une suite de variables aléatoires converge sous un mode ou un autre, elle doit a minima converger en loi. 5.1.6 Approximations Théorème. i. Soit \\((X_n)_n\\) une suite de variables aléatoires telle que, pour tout entier naturel \\(n\\), on ait \\(X_n\\sim\\mathcal{B}(n, p_n)\\). On suppose que \\(p_n\\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}0\\) et \\(np_n\\underset{n \\to +\\infty}{\\overset{}{\\longrightarrow}}\\lambda\\), où \\(\\lambda\\) est un réel. Alors \\[X_n \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}}{\\longrightarrow}}\\mathcal{P}(\\lambda)\\] ii. Soit \\((X_n)_n\\) une suite i.i.d. de variables aléatoires suivant une loi de Bernoulli de paramètre \\(p\\in ]0\\,;\\,1[\\) : \\[\\forall i\\in\\mathbb{N},\\, X_i\\sim\\mathcal{B}(p)\\] Pour tout entier naturel \\(n\\), on pose \\(S_n=\\sum\\limits_{i=1}^n X_i\\). Alors \\[\\frac{S_n-np}{\\sqrt{npq}} \\underset{n \\to +\\infty}{\\overset{\\mathcal{L}}{\\longrightarrow}}\\mathcal{N}(0\\,;\\,1)\\] Le deuxième résultat nous dit que la loi binomiale \\(\\mathcal{B}(n\\,;\\,p)\\) peut être approchée par la loi normale \\(\\mathcal{N}(np\\,;\\,npq)\\). On considère généralement que cette approximation est bonne pour \\(n\\geq 30, np\\geq 5\\) et \\(nq\\geq 5\\). L’exemple suivant montre un intérêt pratique de ces appoximations. Exemple (contrôle de fabrication). On souhaite effectuer un contrôle de fabrication sur un lot de \\(1\\,000\\) pièces. On sait que la probabilité qu’une pièce soit défectueuse est \\(p=0,02\\). On suppose que les pièces sont défectueuses ou non indépendamment des autres pièces. On souhaite évaluer la probabilité que le lot contienne entre \\(18\\) et \\(22\\) pièces défectueuses. On note \\(X\\) le nombre de pièces défectueuses. Alors, \\(X\\) suit une loi binomiale de paramètres \\(n=1\\,000\\) et \\(p=0,02\\) : \\[X\\sim\\mathcal{B}(1\\,000\\,;\\,0,02)\\] Le calcul de \\(\\mathbb{P}(X\\in[18\\,;\\,22])\\) est trop compliqué pour être envisagé de manière exacte, on va donc fournir une valeur approchée. Nous allons envisager à la fois une approximation par une loi de Poisson et par une loi normale. Approximation 1 : par une loi de Poisson. On a : \\(n\\=1\\,000\\geq 30\\) \\(np=20\\), donc \\(np\\) n’est pas inférieur à 5 Même si dans le cas présent la deuxième condition n’est pas vérifiée, nous calculons tout de même l’approximation par la loi de Poisson \\(\\mathcal{P}(20)\\). Avec une table de la loi de Poisson, on obtient, en notant \\(Y\\) une variable aléatoire telle que \\(Y\\sim\\mathcal{P}(20)\\) : \\[\\begin{align} \\mathbb{P}(18\\leq X \\leq 22) &amp;\\approx\\mathbb{P}(18\\leq Y\\leq 22) \\\\ &amp;\\approx 0,42 \\\\ \\end{align}\\] Approximation 2 : par une loi normale. On a : \\(n=1\\,000\\geq 30\\) \\(np=20\\geq 5\\) \\(nq=980\\geq 5\\) On peut donc approcher la loi de \\(X\\) par la loi normale de paramètres \\(np=20\\) et \\(npq=19,6\\) : \\[X\\approx\\mathcal{N}(20\\,;\\,19,6)\\] Pour calculer \\(\\mathbb{P}(X\\in [18\\,;\\,22])\\), on doit se ramener à une loi normale standard \\(\\mathcal{N}(0\\,;\\,1)\\) pour laquelle on dispose de tables. Pour cela, on centre et on réduit \\(X\\) : \\[Y=\\frac{X-20}{\\sqrt{19,6}}\\] On a donc \\[\\begin{align*} \\mathbb{P}\\left(18\\leq X\\leq 22\\right)&amp;=\\mathbb{P}\\left(\\frac{18-20}{\\sqrt{19,6}}\\leq Y\\leq\\frac{22-20}{\\sqrt{19,6}}\\right) \\\\ &amp;\\approx\\mathbb{P}(-0,45\\leq X\\leq 0,45) \\\\ &amp;\\approx 0,35 \\end{align*}\\] Calcul avec la vraie loi binomiale. On obtient \\(\\mathbb{P}(18\\leq X\\leq 22)\\approx 0,43\\). Dans cet exemple, l’approximation par la loi de Poisson est donc meilleure que celle par la loi normale. 5.2 Lois des grands nombres Le programme du concours mentionne deux lois des grands nombres, qui permettent de montrer la convergence de la moyenne empirique vers la moyenne théorique : la loi faible des grands nombres, où la convergence est une convergence en probabilités ; la loi (forte) des grands nombres dans \\(L^2\\), où la convergence est une convergence presque sûre, donc en un sens plus fort que celle de la loi faible. Loi faible des grands nombres (théorème de Khintchine). Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. et admettant une espérance \\(\\mu=\\mathbb{E}(X_1)\\), i.e. \\(X_1\\in L^1(\\Omega, \\mathcal{A}, \\mathbb{P})\\). Alors, la moyenne empirique \\[\\overline{X_n}=\\frac{X_1+\\dots+X_n}{n}\\] converge en probabilité vers \\(\\mu=\\mathbb{E}(X_1)\\). Autrement dit : \\[\\forall\\varepsilon &gt;0, \\mathbb{P}\\left(\\left|\\overline{X_n}-\\mu\\right|\\geq\\varepsilon\\right)\\longrightarrow 0\\] La loi des grands nombres dans \\(L^2\\) permet d’obtenir une convergence en un sens plus fort (presque sûre plutôt qu’en probabilités), au prix d’une hypothèse plus forte (variables dans \\(L^2\\) plutôt que dans \\(L^1\\)) : Loi des grands nombres dans \\(L^2\\). Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. telle que \\(\\mathbb{E}(X_1^2)&lt;+\\infty\\), i.e. \\(X_1\\in L^2(\\Omega, \\mathcal{A}, \\mathbb{P})\\). Alors, la moyenne empirique \\[\\overline{X_n}=\\frac{X_1+\\dots+X_n}{n}\\] converge presque sûrement vers \\(\\mathbb{E}(X_1)\\). On a en fait même un peu mieux. En effet, l’hypothèse d’appartenance à \\(L^2\\) n’est pas nécessaire pour obtenir une convergence presque sûre : une appartenance à \\(L^1\\) est suffisante. Ce resultat est connu usuellement sous le nom de loi forte des grands nombres. Loi forte des grands nombres (hors-programme ?). Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. telle que \\(\\mathbb{E}(|X_1|)&lt;+\\infty\\), i.e. \\(X_1\\in L^1(\\Omega, \\mathcal{A}, \\mathbb{P})\\). Alors, la moyenne empirique \\[\\overline{X_n}=\\frac{X_1+\\dots+X_n}{n}\\] converge presque sûrement vers \\(\\mathbb{E}(X_1)\\). Ce résultat n’étant pas mentionné explicitemnt dans la notice du concours, il est donc a priori hors-programme. Conséquence : sauf indication contraire de l’énoncé, pour démontrer une convergence presque sûre de la moyenne empirique vers la moyenne théorique, il faut au préalable s’assurer que les \\(X_i\\) sont dans \\(L^2\\) (\\(L^1\\) ne suffit pas dans ce cadre). énoncé théorique exemples avec illustration en R 5.3 Théorème Central Limite (TCL) Théorème central limite. Soit \\((X_n)_n\\) une suite de variables aléatoires i.i.d. admettant un moment d’ordre 2. On pose \\[\\mu=\\mathbb{E(X_1)}\\] \\[\\sigma^2=\\mathbb{V}(X_1)\\] On pose \\[S_n=X_1+\\dots+X_n\\] Alors, la variable aléatoire \\[Z_n=\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}}\\] converge en loi vers la loi normale standard \\(\\mathcal{N}(0,1)\\). énoncé théorique exemples avec illustration en R 5.4 Variantes du TCL (hors-programme) "]]
