[["exercices.html", "Chapitre 9 Exercices 9.1 Dénombrement et probabilités 9.2 Variables aléatoires discrètes 9.3 Variables à densité 9.4 Annales des oraux", " Chapitre 9 Exercices 9.1 Dénombrement et probabilités On commence par quelques exercices de dénombrement, très classiques lorqu’on découvre le calcul des probabilités. Si ce type d’exercices a en soi très peu de chances de tomber au concours, on peut toujours imaginer un sujet dans lequel quelques questions isolées nécessitent des réflexes d’analyse combinatoire. Exercice 2.1. Dans un jeu de 52 cartes, on tire 5 cartes sans remise. Quelle est la probabilité de tirer : 1. 5 coeurs ? 2. 2 piques et 3 coeurs ? 3. 5 trèfles ou 5 coeurs ? 4. 5 cartes de la même couleur (pique, coeur, carreau, trèfle) ? 5. 3 cartes d’une couleur et 2 d’une autre ? 6. les 4 as et une autre carte ? Solution. Le nombre de main à \\(5\\) cartes dans un jeu de \\(52\\) cartes est \\(\\binom{52}{5}\\). 1. Il y a \\(13\\) coeurs dans un jeu de \\(52\\) cartes, donc il y a \\(\\binom{13}{5}\\) mains à \\(5\\) coeurs. La probabilité de tirer \\(5\\) coeurs est donc \\(\\frac{\\binom{13}{5}}{\\binom{52}{5}}\\). 2. Le nombre de façons de tirer \\(2\\) piques et \\(3\\) coeurs est \\(\\binom{8}{2}\\times\\binom{8}{3}\\). La probabilité d’un tel tirage est donc \\(\\frac{\\binom{8}{2}\\times\\binom{8}{3}}{\\binom{52}{5}}\\). 3. Les événements obtenir 5 trèfles et obtenir 5 coeurs étant incompatibles, la probabilité que l’un ou l’autre soit réalisé est la somme des probabilités, soit \\(\\frac{\\binom{13}{5}}{\\binom{52}{5}}+\\frac{\\binom{13}{5}}{\\binom{52}{5}}=\\frac{2\\,\\binom{13}{5}}{\\binom{52}{5}}\\). 4. Avec le même raisonnement qu’à la question 3., la probabilité d’avoir \\(5\\) cartes de la même couleur est \\(\\frac{4\\,\\binom{13}{5}}{\\binom{52}{5}}\\). 5. choisir deux couleurs parmi quatre : \\(A_4^2\\) possibilités (l’ordre des couleurs compte, puisque les nombres de cartes de chaque couleur sont différents) ; pour une couleur choisir \\(3\\) cartes : \\(\\binom{13}{3}\\) possibilités ; pour l’autre couleur choisir \\(2\\) cartes : \\(\\binom{13}{2}\\) possibilités. Donc, en vertu du principe mulitplicatif, il y a \\(\\binom{4}{2}\\times\\binom{13}{3}\\times\\binom{13}{2}\\) mains de \\(5\\) cartes, avec \\(3\\) cartes d’une couleur et \\(2\\) cartes d’une autre. La probabilité d’un tel tirage est donc égale à \\(\\frac{\\binom{4}{2}\\times\\binom{13}{3}\\times\\binom{13}{2}}{\\binom{52}{5}}\\). 6. choisir les quatre as : une seule possibilité ; choisir n’importe quelle carte parmi les \\(48\\) cartes restantes : \\(48\\) possibilités. Donc, la probabilité de tirer les quatre as et une autre carte (quelconque) est égale à \\(\\frac{48}{\\binom{13}{5}}\\). Exercice 2.2. On s’intéresse à une famille et on considère les événements suivants : \\(A\\) : la famille a des enfants des deux sexes \\(B\\) : la famille a au plus un garçon 1. Démontrer que si la famille a deux enfants, alors \\(A\\) et \\(B\\) sont dépendants. 2. Démontrer que si la famille a trois enfants, alors \\(A\\) et \\(B\\) sont indépendants. Solution. On suppose que la probabilité d’avoir une fille et celle d’avoir un garçon sont égales à \\(\\frac{1}{2}\\). On note \\(G_i\\) l’événement : l’enfant numéro \\(i\\) est un garçon ; \\(F_i\\) l’événement : l’enfant numéro \\(i\\) est une fille. 1. On vérifie facilement que \\(A\\cap B=A=(F_1\\cap G_2)\\cup (F_2\\cap G_1)\\). Il s’agit d’une réunion disjointe de deux événements de probabilité commune \\(\\frac{1}{4}\\), donc \\(\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)=\\frac{1}{2}\\). Comme \\(\\mathbb{P}(B)&gt;0\\), on en déduit que \\(\\mathbb{P}(A\\cap B)\\neq\\mathbb{P}(A)\\mathbb{P}(B)\\), et donc \\(A\\) et \\(B\\) sont dépendants. 2. \\(\\overline{A}=(F_1\\cap F_2\\cap F_3)\\cup(G_1\\cap G_2\\cap G_3)\\) donc \\[\\mathbb{P}(A)=1-\\mathbb{P}(F_1\\cap F_2\\cap F_3)-\\mathbb{P}(G_1\\cap G_2\\cap G_3)=1-\\frac{1}{8}-\\frac{1}{8}=\\frac{3}{4}\\] \\(\\overline{B}=(F_1\\cap F_2\\cap F_3)\\cup(G_1\\cap F_2\\cap F_3)\\cup (G_1\\cap F_2\\cap F_3)\\cup (G_1\\cap F_2\\cap F_3)\\).Il s’agit d’une réunion disjointe de quatre événements de probabilité commune \\(\\frac{1}{8}\\) donc \\(\\mathbb{P}(\\overline{B})=\\frac{4}{8}=\\frac{1}{2}\\). D’où \\(\\mathbb{P}(B)=\\frac{1}{2}\\). \\(A\\cap B=(G_1\\cap F_2\\cap F_3)\\cup (F_1\\cap G_2\\cap F_3)\\cup (F_1\\cap F_2\\cap G_3)\\). Il s’agit d’une réunion disjointe de trois événements de probabilité commune \\(\\frac{1}{8}\\). Donc, \\(\\mathbb{P}(A\\cap B)=\\frac{3}{8}=\\mathbb{P}(A)\\mathbb{P}(B)\\). Les événements \\(A\\) et \\(B\\) sont donc indépendants. Exercice 2.3 (coefficients binomiaux). 1. Soit \\(1\\leq p\\leq n\\). On considère \\(n\\) boules et deux boîtes \\(A\\) et \\(B\\) vides. Un échantillon est constitué d’une boule que l’on met dans la boîte \\(A\\) et \\(p-1\\) boules que l’on met dans la boîte \\(B\\). En dénombrant de deux façons différentes l’ensemble de tous les échantillons que l’on peut ainsi obtenir, établir la formule \\[n\\binom{n-1}{p-1}=p\\binom{n}{p}\\] Démontrer cette formule par le calcul. 2. Démontrer que pour \\(1\\leq p\\leq n\\) on a la formule \\[\\sum\\limits_{k=p}^n\\binom{k}{p}=\\binom{n+1}{p+1}\\] 3. (Extension de la formule de Pascal). Soient \\(m, p, q\\) des entiers naturels tels que \\(q\\leq p\\leq m\\). Démontrer par un dénombrement la formule : \\[\\sum\\limits_{j=0}^q \\binom{q}{j}\\binom{m-q}{p-j}=\\binom{m}{p}\\] 4. Soit \\(n\\) un entier tel que \\(n\\geq 1\\). Démontrer par un dénombrement la formule : \\[\\sum\\limits_{k=0}^n \\binom{n}{k}^2=\\binom{2n}{n}\\] Solution. 1. Première méthode de dénombrement des échantillons : choisir une boule à mettre dans la boîte \\(A\\) : \\(n\\) choix possibles ; choisir \\(p-1\\) boules parmi les \\(n-1\\) boules restantes, à mettre dans la boîte \\(B\\) : \\(\\binom{n-1}{p-1}\\) choix possibles. En appliquant le principe multiplicatif, on en déduit qu’il y a donc \\(n\\times\\binom{n-1}{p-1}\\) échantillons. Deuxième méthode de dénombrement des échantillons : choisir \\(p\\) boules parmi \\(n\\) boules à mettre dans l’ensemble des deux boîtes \\(A\\) et \\(B\\) : \\(\\binom{n}{p}\\) choix possibles ; choisir une boule parmi ces \\(p\\) boules à mettre dans la boîte \\(A\\) (les autres vont ditrectement dans la boîte \\(B\\)) : \\(p\\) choix possibles. En appliquant le principe multiplicatif, on en déduit qu’il y a donc \\(p\\times\\binom{n}{p}\\) échantillons. Ces deux méthodes comptent le même nombre d’échantillons. On en déduit que \\[n\\times\\binom{n-1}{p-1}=p\\times\\binom{n}{p}\\] Approche calculatoire. On peut démontrer cette formule par le calcul (c’est plus simple mais moins élégant) : \\[\\begin{align} n\\times\\binom{n-1}{p-1} &amp;= n\\frac{(n-1)!}{(p-1)!(n-p)!} \\\\ &amp;= \\frac{n!}{(p-1)!(n-p)!} \\\\ &amp;=p\\frac{n!}{p!(n-p)!} \\\\ &amp;=p\\times\\binom{n}{p} \\\\ \\end{align}\\] 2. On choisit \\(p+1\\) nombres parmi les entiers \\(1,2,\\dots,n+1\\) : \\(\\binom{n+1}{p+1}\\) choix possibles. On peut aussi dénombrer le nombre de choix possibles de la manière suivante : fixer le maximum \\(k\\) des \\(p+1\\) nombres tirés : ce maximum est nécessairement compris entre \\(p+1\\) et \\(n+1\\) (inclus) ; choisir les \\(p\\) nombres restants parmi \\(1,2,\\dots, k-1\\) : à \\(k\\) fixé, on a \\(\\binom{p}{k-1}\\) choix possibles. On en déduit que le nombre de façons de choisir \\(p+1\\) nombres parmi les entiers \\(1,2,\\dots n+1\\) est égal à \\(\\sum\\limits_{k=p+1}^{n+1}\\binom{p}{k-1}=\\sum\\limits_{k=p}^n\\binom{p}{k}\\). Ces deux méthodes de dénombrement doivent mener au même résultat, donc \\[\\sum\\limits_{k=p}^n\\binom{p}{k}=\\binom{n+1}{p+1}\\] 3. On choisit \\(p\\) entiers parmi les entiers \\(1,2,\\dots,m\\) : \\(\\binom{m}{p}\\) choix possbles. Pour choisir ces \\(p\\) entiers on peut aussi procéder de la manière suivante : on choisit \\(j\\) entiers parmi les entiers \\(1,2,\\dots,q\\). Cela n’est évidemment possible que pour \\(0\\leq j\\leq q\\), et pour chaque entier \\(j\\) vérifiant cet encadrement on a \\(\\binom{q}{j}\\) façons différentes de choisir ces \\(j\\) entiers. on complète notre sélection en choisissant les \\(p-j\\) entiers restants parmi \\(q+1,q+2,\\dots, m\\) : \\(\\binom{m-q}{p-j}\\) choix possibles. En vertu du principe multiplicatif, à \\(j\\) fixé compris entre \\(0\\) et \\(q\\) inclus, on a donc \\(\\binom{q}{j}\\binom{m-q}{p-j}\\) façons de choisir \\(p\\) entiers parmi \\(1,2,\\dots m\\) de sorte que \\(j\\) de ces entiers soient compris entre \\(1\\) et \\(q\\) et les \\(p-j\\) entiers restants soient compris entre \\(q+1\\) et \\(m\\). Par ailleurs, l’entier \\(j\\) peut prendre n’importe quelle valeur comprise entre \\(0\\) et \\(q\\), et pour deux valeurs de \\(j\\) distinctes on a deux sélections d’entiers distinctes. En vertu du principe additif, le nombre de façons de choisir \\(p\\) entiers parmi \\(1,2,\\dots, q\\) est donc finalement égal à \\(\\sum\\limits_{j=0}^q \\binom{q}{j}\\binom{m-q}{p-j}\\). On a dénombré de deux façons différentes le même ensemble, ces deux approches doivent aboutir au même résultat. On a donc \\[\\sum\\limits_{j=0}^q \\binom{q}{j}\\binom{m-q}{p-j}=\\binom{m}{p}\\] 4. On applique la formule de la question précédente avec \\(q=n\\), \\(p=n\\) et \\(m=2n\\) : \\[\\sum\\limits_{k=0}^n \\binom{n}{k}\\binom{n}{n-k}=\\binom{2n}{n}\\] Par la formule de symétrie des coefficients binomiaux \\(\\binom{n}{n-k}=\\binom{n}{k}\\), on en déduit que \\[\\sum\\limits_{k=0}^n \\binom{n}{k}^2=\\binom{2n}{n}\\] Exercice 2.4 (Interne, 2023). 1. Montrer que pour tout entier naturel \\(n\\), on a \\(\\sum\\limits_{k=0}^n\\binom{2n+1}{k}=\\sum\\limits_{k=0}^n\\binom{2n+1}{n+k+1}\\), et calculer la valeur commune de ces sommes. 2. Montrer que pour tout entier naturel non nul \\(n\\), on a \\(\\sum\\limits_{k=0}^{n-1}\\binom{2n}{k}=\\sum\\limits_{k=1}^n\\binom{2n}{n+k}=2^{2n-1}-\\frac{1}{2}\\binom{2n}{n}\\). 3. Déduire de ce qui précède le résultat suivant : \\(\\forall n\\in\\mathbb{N}^{*}, \\sum\\limits_{k=1}^n k\\binom{2n}{n+k}=\\frac{n}{2}\\binom{2n}{n}\\). Solution. 1. \\[\\begin{align} \\sum\\limits_{k=0}^n\\binom{2n+1}{k}&amp;=\\sum\\limits_{k=0}^n\\binom{2n+1}{n-k} \\\\ &amp; \\text{(changement de variable } k\\mapsto n-k \\text{)} \\\\ &amp;=\\sum\\limits_{k=0}^n\\binom{2n+1}{(2n+1)-(n-k)} \\\\ &amp; \\text{(symétrie des coefficients binomiaux : } \\binom{n}{k}=\\binom{n}{n-k} \\text{)} \\\\ &amp;=\\sum\\limits_{k=0}^n\\binom{2n+1}{n+k+1} \\\\ \\end{align}\\] On pose \\(S_n:=\\sum\\limits_{k=0}^n\\binom{2n+1}{k}\\). On a donc \\[\\begin{align} 2S_n&amp;=\\sum\\limits_{k=0}^n\\binom{2n+1}{k}+\\sum\\limits_{k=0}^n\\binom{2n+1}{n+k+1} \\\\ &amp;=\\sum\\limits_{k=0}^{2n+1}\\binom{2n+1}{k} \\\\ &amp;=2^{2n+1} \\\\ \\end{align}\\] D’où : \\[S_n=2^{2n}\\] 2. L’égalité \\(\\sum\\limits_{k=0}^{n-1}\\binom{2n}{k}=\\sum\\limits_{k=1}^n\\binom{2n}{n+k}\\) se démontre de façon tout à fait analogue. On note \\(S&#39;_n\\) la valeur commune de ces deux sommes. Alors \\[\\begin{align} 2S&#39;_n &amp;= \\sum\\limits_{k=0}^{n-1}\\binom{2n}{k}+\\sum\\limits_{k=1}^n\\binom{2n}{n+k} \\\\ &amp;= \\sum\\limits_{k=0}^{2n}\\binom{2n}{k}-\\binom{2n}{n} \\\\ &amp;= 2^{2n}-\\binom{2n}{n} \\\\ \\end{align}\\] On en déduit que \\[S&#39;_n=2^{2n-1}-\\frac{1}{2}\\binom{2n}{n}\\] 3. Pour tout réel \\(x\\), on a d’après la formule du binôme de Newton \\((1+x)^{2n}=\\sum\\limits_{k=0}^{2n}\\binom{2n}{k}x^k\\). En dérivant par rapport à \\(x\\) on obtient donc \\[2n(1+x)^{2n-1}=\\sum\\limits_{k=1}^{2n}k\\binom{2n}{k}x^{k-1}\\] On évalue cette égalité en \\(x=1\\) : \\[n2^{2n}=\\sum\\limits_{k=1}^{2n}k\\binom{2n}{k}\\] D’où \\[\\begin{align} n2^{2n}&amp;=\\sum\\limits_{k=1}^{n}k\\binom{2n}{k}+\\sum\\limits_{k=n+1}^{2n}k\\binom{2n}{k} \\\\ &amp;=\\sum\\limits_{k=1}^{n}k\\binom{2n}{k}+\\sum\\limits_{k=1}^{n}(n+k)\\binom{2n}{n+k} \\\\ &amp;=\\sum\\limits_{k=1}^{n}k\\binom{2n}{k}+n\\sum\\limits_{k=1}^{n}\\binom{2n}{n+k}+\\sum\\limits_{k=1}^{n}k\\binom{2n}{n+k} \\\\ &amp;=\\sum\\limits_{k=1}^{n}k\\binom{2n}{k}+n\\left(2^{2n-1}-\\frac{1}{2}\\binom{2n}{n}\\right)+\\sum\\limits_{k=1}^{n}k\\binom{2n}{n+k} \\end{align}\\] d’après 2. Par ailleurs, on a facilement \\[k\\binom{2n}{k}=2n\\binom{2n-1}{k-1}\\] D’où \\[\\begin{align} \\sum\\limits_{k=1}^n k\\binom{2n}{k}&amp;=2n\\sum\\limits_{k=1}^n \\binom{2n-1}{k-1} \\\\ &amp;= 2n\\sum\\limits_{k=0}^{n-1}\\binom{2n-1}{k} \\\\ &amp;= n2^{2n-1} \\\\ \\end{align}\\] Ainsi \\[n2^{2n}=n2^{2n-1}+n2^{2n-1}-\\frac{n}{2}\\binom{2n}{n}+\\sum\\limits_{k=1}^n k\\binom{2n}{n+k}\\] et donc \\[\\sum\\limits_{k=1}^n k\\binom{2n}{n+k}=\\frac{n}{2}\\binom{2n}{n}\\] Exercice 2.5 (Interne, 2012). On considère trois événements \\(A, B, C\\) d’un espace probabilisé tels que \\[\\mathbb{P}(A)=0,4 \\hspace{1cm} \\mathbb{P}(B)=0,2 \\hspace{1cm} \\mathbb{P}(C)=0,3\\] \\[\\mathbb{P}(A\\cap B)=0,2 \\hspace{1cm} \\mathbb{P}(A\\cap C)=0,12 \\hspace{1cm} \\mathbb{P}(B\\cap C)=0\\] 1. Parmi les événements \\(A, B\\) et \\(C\\), quels sont les deux événements indépendants ? 2. La réalisation de l’un des trois événements implique la réalisation de l’un des deux autres. De quel événement s’agit-il ? 3. Déterminer la probabilité \\(\\mathbb{P}(A\\cap B\\cap C)\\). 4. Calculer la probabilité qu’au moins l’un des événements \\(A, B, C\\) se réalise. 5. On considère maintenant l’expérience aléatoire pour laquelle les événements \\(A, B\\) ou \\(C\\), et seulement eux, peuvent se réaliser. On note \\(X\\) le nombre d’événements qui se sont réalisés, parmi \\(A, B, C\\). Pour quelles valeurs de \\(i\\) a-t-on \\(\\mathbb{P}(X=i)&gt;0\\) ? Pour chacune de ces valeurs \\(i\\), déterminer \\(\\mathbb{P}(X=i)\\). Solution. 1. \\(\\mathbb{P}(A\\cap C)=\\mathbb{P}(A)\\mathbb{P}(C)=0,12\\), donc \\(A\\) et \\(C\\) sont indépendants. 2. \\(\\mathbb{P}(A\\vert B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}=\\frac{0,2}{0,2}=1\\), donc sachant que \\(B\\) est réalisé, \\(A\\) est réalisé avec probabilité \\(1\\). 3. \\(A\\cap B\\cap C\\subset B\\cap C\\), et \\(\\mathbb{P}(B\\cap C)=0\\). D’après la propriété de croissance des probabilités, on en déduit que \\(\\mathbb{P}(A\\cap B\\cap C)=0\\). 4. On veut calculer \\(\\mathbb{P}(A\\cup B\\cup C)\\). On peut utiliser la formule de Poincaré : \\[\\begin{align} \\mathbb{P}(A\\cup B\\cup C)&amp;=\\mathbb{P}(A)+\\mathbb{P}(B)+\\mathbb{P}(C)-\\mathbb{P}(A\\cap B)-\\mathbb{P}(A\\cap C)-\\mathbb{P}(B\\cap C)+\\mathbb{P}(A\\cap B\\cap C) \\\\ &amp;=0,4+0,2+0,3-0,2-0,12-0+0 \\\\ &amp;=0,58 \\\\ \\end{align}\\] 5. Remarquons d’abord qu’a priori \\(X\\in\\{0,1,2,3\\}\\). Par ailleurs : \\[A\\cup B\\cup C=(X\\geq 1)\\] \\[A\\cap B\\cap C=(X=3)\\] On en déduit que \\[\\mathbb{P}(X=0\\vert A\\cup B\\cup C)=\\mathbb{P}(X=0\\vert X\\geq 1)=0\\] et, d’après 3 : \\[\\mathbb{P}(X=3\\vert A\\cup B\\cup C)=\\frac{\\mathbb{P}(X=3, X\\geq 1)}{\\mathbb{P}(X\\geq 1)}=\\frac{\\mathbb{P}(X=3)}{\\mathbb{P}(X\\geq 1)}=\\frac{\\mathbb{P}(A\\cap B\\cap C)}{\\mathbb{P}(X\\geq 1)}=0\\] De plus \\[\\begin{align} \\mathbb{P}(X=2\\vert A\\cup B\\cup C)&amp;=\\frac{\\mathbb{P}(X=2\\vert X\\geq 1)}{\\mathbb{P}(X\\geq 1)} \\\\ &amp;=\\frac{\\mathbb{P}(X=2, X\\geq 1)}{\\mathbb{P}(X\\geq 1)} \\\\ &amp;=\\frac{\\mathbb{P}(X=2)}{\\mathbb{P}(X\\geq 1)} \\text{ ; car } (X=2)\\subset (X\\geq 1) \\\\ &amp;=\\frac{\\mathbb{P}(A\\cap B\\cap\\overline{C})+\\mathbb{P}(A\\cap \\overline{B}\\cap C)+\\mathbb{P}(\\overline{A}\\cap B\\cap C)}{\\mathbb{P}(X\\geq 1)} \\\\ &amp;=\\frac{\\mathbb{P}(A\\cap B)+\\mathbb{P}(A\\cap C)+\\mathbb{P}(B\\cap C)}{\\mathbb{P}(X\\geq 1)} \\text{ ; car } \\mathbb{P}(A\\cap B\\cap C)=0 \\\\ &amp;=\\frac{0,32}{0,58} \\\\ &amp;=\\frac{16}{29} \\\\ \\end{align}\\] On en déduit enfin que \\[\\begin{align} \\mathbb{P}(X=1\\vert A\\cup B\\cup C)&amp;=\\mathbb{P}(X\\geq 1\\vert A\\cup B\\cup C)-\\mathbb{P}(X=2\\vert A\\cup B\\cup C)-\\mathbb{P}(X=3\\vert A\\cup B\\cup C) \\\\ &amp;= \\frac{0,58}{0,29}-\\frac{0,32}{0,29}-0 \\\\ &amp;= \\frac{0,26}{0,39} \\\\ &amp;=\\frac{2}{3} \\\\ \\end{align}\\] Commentaire sur cette dernière question. Il s’agit typiquement du genre de question qui peut facilement devenir assez chronophage si l’on n’est pas un peu astucieux. Il y a plusieurs astuces / points à ne pas manquer pour s’économiser du temps de calcul : bien traduire \\((X\\geq 1)\\) par \\(A\\cup B\\cup C\\) et \\(X=3\\) par \\(A\\cap B\\cap C\\). constater que le calcul de \\(\\mathbb{P}(X=1\\vert A\\cup B\\cup C)\\) et \\(\\mathbb{P}(X=2\\vert A\\cup B\\cup C)\\) se ramène en fait au calcul d’un seul de ces deux termes. Pour voir cela, il faut penser à utiliser l’égalité \\(\\mathbb{P}(X\\geq 1\\vert A\\cup B\\cup C)=\\mathbb{P}(X=1\\vert A\\cup B\\cup C)+\\mathbb{P}(X=2\\vert A\\cup B\\cup C)\\) qui résulte de \\(\\mathbb{P}(X=3\\vert A\\cup B\\cup C)=0\\). remarquer que \\(\\mathbb{P}(X=1\\vert A\\cup B\\cup C)\\) est plus rapide à calculer que \\(\\mathbb{P}(X=2\\vert A\\cup B\\cup C)\\). Pour s’en convaincre, il suffit de calculer directement \\(\\mathbb{P}(X=2\\vert A\\cup B\\cup C)\\) : cela n’a rien de difficile, mais c’est plus long. Exercice 2.6 (Interne, 2011). 1. Soit \\(\\Omega\\) un ensemble fondamental tel que \\(\\Omega=A_1\\cup A_2\\), où les \\(A_i\\) (\\(i=1,2\\)) sont des événements donnés. On suppose \\(\\Omega\\) muni d’une probabilité \\(\\mathbb{P}\\) telle que \\(\\mathbb{P}(A_1)=0,8\\) et \\(\\mathbb{P}(A_2)=0,5\\). Donner la valeur de \\(\\mathbb{P}(A_1\\cap A_2)\\). 2. Un individu oublie sa carte bancaire 10 % du temps, son chéquier 5 % du temps et ces deux instruments de paiement 2 % du temps. Calculer : la probabilité pour que, à un moment donné, il détienne ces deux instruments sur lui ; la probabilité qu’il trouve sa carte bancaire dans ses poches sachant qu’il a déjà trouvé son chéquier. 3. Une automobile est classée comme étant “de luxe” si son moteur possède au moins 6 cylindres ou une cylindrée d’au moins 3 litres. On observe que 15 % des véhicules ont au moins 6 cylindres, que 10 % au moins 3 litres de cylindrées et que 80 % des véhicules d’au moins 3 litres ont au moins 6 cylindres. Calculer la proportion de véhicules de luxe dans le parc automobile. 4. Soient \\(\\Omega\\) un ensemble fondamental et \\(A\\) et \\(B\\) deux parties de \\(\\Omega\\). Montrer que \\[\\mathbb{P}(A\\cap B)\\leq\\mathbb{P}(A)\\leq\\mathbb{P}(A\\cup B)\\leq\\mathbb{P}(A)+\\mathbb{P}(B)\\] 5. Soient \\(A\\) et \\(B\\) deux événements indépendants en probabilité et tels que \\(\\mathbb{P}(B)=2\\,\\mathbb{P}(A)\\) et \\(\\mathbb{P}(A\\cup B)=5/8\\). Calculer \\(\\mathbb{P}(A)\\) et \\(\\mathbb{P}(B)\\). 6. Chaque jour ouvrable de la semaine, le professeur Statys reçoit du courrier au collège avec une probabilité de \\(1/3\\). Par ailleurs, il a annoncé qu’il viendra aujourd’hui avec une probabilité de \\(2/5\\). Or, on observe que sa boite aux lettres est vide. Sachant cela, calculer la probabilité que Statys soit venu aujourd’hui. 7.a. Démontrer que l’on peut généraliser (une récurrence est indiquée) la formule de base \\[\\mathbb{P}(A_1\\cup A_2)=\\mathbb{P}(A_1)+\\mathbb{P}(A_2)-\\mathbb{P}(A_1\\cap A_2)\\] au cas de \\(N\\) événements \\(A_n\\) (\\(n=1\\dots N\\)), autrement dit \\[\\begin{align} \\mathbb{P}(A_1\\cup\\dots A_n)&amp;=\\sum\\limits_{n=1}^N\\mathbb{P}(A_n)-\\sum\\limits_{i&lt;j}\\mathbb{P}(A_i\\cap A_j)+\\sum\\limits_{i&lt;j&lt;k}\\mathbb{P}(A_i\\cap A_j\\cap A_k) \\\\ &amp; +\\dots+(-1)^{N-1}\\mathbb{P}\\left(\\bigcap\\limits_{n=1}^N A_n\\right) \\\\ \\end{align}\\] b. Que se passe-t-il : si la suite \\((A_n)_{n=1\\dots N}\\) est emboitée de façon décroissante (\\(A_1\\supset A_2\\supset\\dots\\supset A_N\\)) ? si la suite \\((A_n)_{n=1\\dots N}\\) est emboitée de façon croissante (\\(A_1\\subset A_2\\subset\\dots\\subset A_N\\)) ? Solution. 1. \\(\\mathbb{P}(A_1\\cup A_2)=\\mathbb{P}(A_1)+\\mathbb{P}(A_2)-\\mathbb{P}(A_1\\cap A_2)=0,8+0,5-1=0,3\\). 2. On note \\(A\\) et \\(B\\) les événements \\(A\\) : l’individu a sa carte bancaire \\(B\\) : l’individu a son chéquier On doit donc déterminer \\(\\mathbb{P}(A\\cap B)\\) et \\(\\mathbb{P}(A\\vert B)\\). On a : \\[\\begin{align} \\mathbb{P}(A\\cap B)&amp;=1-\\mathbb{P}(\\overline{A}\\cup\\overline{B}) \\\\ &amp;=1-\\left(\\mathbb{P}(\\overline{A})+\\mathbb{P}(\\overline{B})-\\mathbb{P}(\\overline{A}\\cap\\overline{B})\\right) \\\\ &amp;=0,87 \\end{align}\\] et \\[\\begin{align} \\mathbb{P}(A\\vert B)&amp;=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)} \\\\ &amp;= \\frac{\\mathbb{P}(A\\cap B)}{1-\\mathbb{P}(\\overline{B})} \\\\ &amp;= \\frac{0,87}{1-0,05} \\\\ &amp;\\approx 0,92 \\\\ \\end{align}\\] 3. On note : \\((N\\geq 6)\\) l’événement : la voiture a au moins \\(6\\) cylindres ; \\((C\\geq 3)\\) l’événement : la voiture a une cylindrée d’au moins \\(3\\) litres ; \\(L\\) l’événement : la voiture est un véhicule de luxe. On a donc \\(L=(N\\geq 6)\\cup (C\\geq 3)\\), et \\[\\begin{align} \\mathbb{P}(L)&amp;=\\mathbb{P}\\left((N\\geq 6)\\cup (C\\geq 3)\\right) \\\\ &amp;= \\mathbb{P}(N\\geq 6)+\\mathbb{P}(C\\geq 3)-\\mathbb{P}(N\\geq 6, C\\geq 3) \\\\ &amp;= \\mathbb{P}(N\\geq 6)+\\mathbb{P}(C\\geq 3)-\\mathbb{P}(N\\geq 6\\vert C\\geq 3)\\mathbb{P}(C\\geq 3) \\\\ &amp;= 0,15+0,10-0,80\\times 0,10 \\\\ &amp;= 0,17 \\\\ \\end{align}\\] La proportion de véhicules de luxe dans le parc automobile est donc égale à \\(17\\,\\%\\). 4. \\((A\\cap B)\\subset A\\subset (A\\cup B)\\) donc \\(\\mathbb{P}(A\\cap B)\\leq\\mathbb{P}(A)\\leq\\mathbb{P}(A\\cup B)\\). De plus, \\(\\mathbb{P}(A\\cup B)=\\mathbb{P}(A)+\\mathbb{P}(B)-\\mathbb{P}(A\\cap B)\\leq\\mathbb{P}(A)+\\mathbb{P}(B)\\), d’où le résultat. 5. Par indépendance de \\(A\\) et \\(B\\) et étant donné la relation \\(\\mathbb{P}(B)=2\\mathbb{P}(A)\\), on a \\[\\begin{align} \\frac{5}{8}&amp;= \\mathbb{P}(A\\cup B) \\\\ &amp;=\\mathbb{P}(A)+\\mathbb{P}(B)-\\mathbb{P}(A\\cap B) \\\\ &amp;=\\mathbb{P}(A)+\\mathbb{P}(B)-\\mathbb{P}(A)\\mathbb{P}(B) \\\\ &amp;=3\\mathbb{P}(A)-2\\mathbb{P}(A)^2 \\\\ \\end{align}\\] Posons \\(x=\\mathbb{P}(A)\\). Alors, \\(x\\) est solution de l’équation \\[3x-2x^2=\\frac{5}{8}\\] soit \\[16x^2-24x+5=0\\] Il s’agit d’une équation du second degré, qui admet deux racines réelles : \\[x_1=\\frac{1}{4} \\text{ et } x_2=\\frac{5}{4}\\] Comme \\(\\frac{5}{4}&gt;1\\), on en déduit que \\(\\mathbb{P}(A)=\\frac{1}{4}\\), et donc \\(\\mathbb{P}(B)=\\frac{1}{2}\\). 6. La question a été posée telle quelle lors des écrits du concours interne de 2011. Toutefois, l’énoncé ne semble pas très clair, et en l’état il semble difficile de répondre. On peut commencer par noter \\(V\\) et \\(C\\) les événements \\(V\\) : M.Statys vient au collège ; \\(C\\) : M.Statys reçoit du courrier. On veut calculer \\(\\mathbb{P}(V\\vert \\overline{C})\\) : \\[\\begin{align} \\mathbb{P}(V\\vert\\overline{C})&amp;=\\frac{\\mathbb{P}(V\\cap\\overline{C})}{\\mathbb{P}(\\overline{C})} \\\\ &amp;=\\frac{\\mathbb{P}(V)-\\mathbb{P}(V\\cap C)}{1-\\mathbb{P}(C)} \\\\ \\end{align}\\] On connaît \\(\\mathbb{P}(V)\\) et \\(\\mathbb{P}(C)\\), mais pas \\(\\mathbb{P}(V\\cap C)\\). Cette probabilité n’est pas donnée dans l’énoncé, et sa détemination semble impossible en l’absence d’hypothèse supplémentaire… Voici deux hypothèses que l’on pourrait envisager, et leurs conséquences sur le calcul de \\(\\mathbb{P}(V\\vert\\overline{C})\\) : Hypothèse 1 : \\(V\\) et \\(C\\) sont indépendants. Dans ce cas, \\(V\\) et \\(\\overline{C}\\) aussi sont indépendants, et l’exercice devient trivial : \\(\\mathbb{P}(V\\vert\\overline{C})=\\mathbb{P}(V)=\\frac{2}{5}\\). Hypothèse 2 : \\(C\\subset V\\). On peut imaginer - mais c’est un peu tiré par les cheveux - que l’expression M. Statys reçoit du courrier comme M. Statys ouvre son courrier. Dans ce cas, pas le choix : s’il ouvre son courrier (le jour où il le reçoit), c’est qu’il est présent ! Et donc on a bien \\(C\\subset V\\). Alors \\[\\begin{align} \\mathbb{P}(V\\vert\\overline{C})&amp;=\\frac{\\mathbb{P}(V)-\\mathbb{P}(V\\cap C)}{1-\\mathbb{P}(C)} \\\\ &amp;=\\frac{\\mathbb{P}(V)-\\mathbb{P}(C)}{1-\\mathbb{P}(C)} \\\\ &amp;=\\frac{\\frac{2}{5}-\\frac{1}{3}}{1-\\frac{1}{3}} \\\\ &amp;=\\frac{1}{10} \\\\ \\end{align}\\] L’hypothèse \\(2\\) me semble plus crédible que l’hypothèse \\(1\\), peut-être est-ce ainsi qu’il fallait interpréter l’énoncé ? 7.a. Voir cours. b. Si la suite \\((A_n)_{1\\leq n\\leq N}\\) est décroissante, alors \\(\\bigcup\\limits_{n=1}^N A_n=A_1\\), et donc dans le membre de droite de la formule de Poincaré, toutes les probablités s’annulent après simplification, sauf \\(\\mathbb{P}(A_1)\\). De même, si la suite \\((A_n)_{1\\leq n\\leq N}\\) est croissante, alors \\(\\bigcup\\limits_{n=1}^N A_n=A_N\\), et donc dans le membre de droite de la formule de Poincaré, toutes les probablités s’annulent après simplification, sauf \\(\\mathbb{P}(A_N)\\). Exercice 2.7 (Interne, 2012). On considère une population dont les individus sont susceptibles de présenter une maladie \\(\\mathcal{M}\\). Pour chaque individu de la population considérée, on envisage trois facteurs de risque possibles de cette maladie : \\(A, B\\) et \\(C\\). Grâce à une étude statistique, on dispose des renseignements suivants : \\(\\mathbb{P}(A)=0,4 \\hspace{1cm} \\mathbb{P}(B)=0,06 \\hspace{1cm} \\mathbb{P}(A\\cap B)=0,06 \\hspace{1cm} \\mathbb{P}(A\\cap C)= 0,1\\) \\(A\\) et \\(C\\) sont indépendants \\(B\\) et \\(C\\) sont incompatibles 1. Déterminer \\(\\mathbb{P}(C)\\), \\(\\mathbb{P}(B\\cap C)\\) et \\(\\mathbb{P}(A\\cap B\\cap C)\\). 2. Calculer la probabilité pour qu’au moins un individu présente un facteur de risque. 3. On note \\(X\\) le nombre de facteurs de risque présents chez un individu. Quelles valeurs \\(X\\) peut-elle prendre ? Pour chacune de ces valeurs \\(i\\), calculer \\(\\mathbb{P}(X=i)\\). 4. On note \\(M\\) l’événement L’individu est atteint de la maladie \\(\\mathcal{M}\\). On donne les probabilités conditionelles suivantes : \\[\\mathbb{P}_{\\overline{A}}(M)=0 \\hspace{1cm} \\mathbb{P}_{A\\cap B}(M)=0,06 \\hspace{1cm} \\mathbb{P}_{A\\cap C}(M)=0,05 \\hspace{1cm} \\mathbb{P}_{A\\cap \\overline{B}\\cap\\overline{C}}(M)=0\\] a. Interpréter ces quatre valeurs. b. Calculer la probabilité \\(\\mathbb{P}(M)\\). Solution. Remarquons d’abord que cet exercice, posé lors des épreuves écrites du concours interne de 2012, est très proche dans l’esprit de l’exercice 5, lui aussi posé lors de ces épreuves ! Ce type d’exercices, de niveau lycée, est peu présent dans les sujets des précédentes années. 1. - Comme \\(A\\) et \\(C\\) sont indépendants, \\(\\mathbb{P}(C)=\\frac{\\mathbb{P}(A\\cap C)}{\\mathbb{P}(A)}=\\frac{1}{4}\\). \\(B\\cap C=\\emptyset\\), donc \\(\\mathbb{P}(B\\cap C)=0\\). \\((A\\cap B\\cap C)\\subset (B\\cap C)=\\emptyset\\), donc \\(A\\cap B\\cap C=\\emptyset\\), d’où \\(\\mathbb{P}(A\\cap B\\cap C)=0\\). 2. On calcule \\(\\mathbb{P}(A\\cup B\\cup C)\\). On utilise la formule de Poincaré : \\[\\begin{align} \\mathbb{P}(A\\cup B\\cup C) &amp;= \\mathbb{P}(A)+\\mathbb{P}(B)+\\mathbb{P}(C) \\\\ &amp; \\hspace{0.2cm} -\\mathbb{P}(A\\cap B)-\\mathbb{P}(A\\cap C)-\\mathbb{P}(B\\cap C) \\\\ &amp; \\hspace{0.2cm} + \\mathbb{P}(A\\cap B\\cap C) \\\\ &amp;= 0,4+0,06+0,25-0,06-0,1-0+0 \\\\ &amp;= 0,55 \\\\ \\end{align}\\] 3. On a a priori \\(X\\in\\{0, 1, 2, 3\\}\\). On a \\(\\mathbb{P}(X=3)=\\mathbb{P}(A\\cap B\\cap C)=0\\). \\(\\mathbb{P}(X=0)=1-\\mathbb{P}(X\\geq 1)=1-\\mathbb{P}(A\\cup B\\cup C)=1-0,55=0,45\\). Il est ensuite plus facile de calculer d’abord \\(\\mathbb{P}(X=2)\\) : \\[\\begin{align} \\mathbb{P}(X=2)&amp;=\\mathbb{P}(\\overline{A}\\cap B\\cap C)+\\mathbb{P}(A\\cap \\overline{B}\\cap C)+\\mathbb{P}(A\\cap B\\cap \\overline{C}) \\\\ &amp;= 0+\\mathbb{P}(A\\cap C)+\\mathbb{P}(A\\cap B) \\\\ &amp;= 0,16 \\\\ \\end{align}\\] et d’en déduire \\(\\mathbb{P}(X=1)\\) : \\[\\begin{align} \\mathbb{P}(X=1)&amp;=\\mathbb{P}(X\\geq 1)-\\mathbb{P}(X=2) \\text{ ; car } $\\mathbb{P}(X=3)=0$ \\\\ &amp;=0,55-0,16 \\\\ &amp;=0,39 \\\\ \\end{align}\\] 4.a. \\(\\mathbb{P}_{\\overline{A}}(M)=0\\) : si l’individu ne présente pas le facteur de risque \\(A\\), alors la probabilité qu’il soit malade est nulle. \\(\\mathbb{P}_{A\\cap B}(M)=0,06\\) : la présence simultanée des facteurs de risque \\(A\\) et \\(B\\) implique une probabilité d’être malade de \\(6\\,\\%\\). \\(\\mathbb{P}_{A\\cap C}(M)=0,05\\) : la présence simultanée des facteurs de risque \\(A\\) et \\(C\\) implique une probabilité d’être malade de \\(5\\,\\%\\). \\(\\mathbb{P}_{A\\cap \\overline{B}\\cap\\overline{C}}(M)=0\\) : si un individu présente le facteur de risque \\(A\\), mais pas les facteurs de risque \\(B\\) et \\(C\\), alors la probabilité qu’il soit malade est nulle. b. Comme \\(B\\) et \\(C\\) sont incompatibles, \\(\\{A\\cap B, A\\cap C, A\\cap\\overline{B}\\cap\\overline{C}, \\overline{A}\\}\\) est un système complet d’événements. D’après la formule des probabilités totales on a alors \\[\\begin{align} \\mathbb{P}(M)&amp;=\\mathbb{P}_{\\overline{A}}(M)\\mathbb{P}(\\overline{A})+\\mathbb{P}_{A\\cap B}(M)\\mathbb{P}(A\\cap B)+\\mathbb{P}_{A\\cap C}(M)\\mathbb{P}(A\\cap C)+\\mathbb{P}_{A\\cap\\overline{B}\\cap\\overline{C}}(M)\\mathbb{P}(A\\cap\\overline{B}\\cap\\overline{C}) \\\\ &amp;= 0+0,06\\times 0,06+0,05\\times 0,1+0 \\\\ &amp;= 0,0086 \\\\ \\end{align}\\] Exercice 2.8 (propagation d’une rumeur). Une information binaire (du type vrai/faux) se propage au sein d’une population. Lorsqu’une personne reçoit l’information : elle la retransmet telle quelle à la personne suivante avec probabilité \\(p\\) ; elle la retransmet de façon erronée à la personne suivante avec probabilité \\(1-p\\). On note \\(p_n\\) la probabilité pour que, après \\(n\\) transmissions, l’information soit correcte. On suppose que \\(p_0=1\\). 1. Donner une relation de récurrence entre \\(p_{n+1}\\) et \\(p_n\\). 2. En déduire une expression de \\(p_n\\) en fonction de \\(p\\) et de \\(n\\). 3. En déduire la valeur de \\(\\lim_{n\\to\\infty} p_n\\). Solution. On note \\(I_n\\) : l’information est correcte après \\(n\\) transmissions. On a donc \\(p_n=\\mathbb{P}(I_n)\\). 1. D’après la formule des probabilités totales : \\(\\mathbb{P}(I_{n+1})=\\mathbb{P}(I_{n+1}\\vert I_n)\\mathbb{P}(I_n)+\\mathbb{P}(I_{n+1}\\vert\\overline{I_n})\\mathbb{P}(\\overline{I_n})\\), autrement dit \\[p_{n+1}=pp_n+(1-p)(1-p_n)\\] soit encore \\[p_{n+1}=(2p-1)p_n+1-p\\] 2. On reconnait une suite arithmético-géométrique \\(p_{n+1}=ap_n+b\\), avec \\(a=2p-1\\) et \\(b=1-p\\). si \\(2p-1=1\\), autrement dit si \\(p=1\\), alors \\((p_n)_{n}\\) est une suite constante, et donc pour tout entier naturel \\(n\\) on a \\(p_n=p_0=1\\). sinon, la suite \\((p_n)_n\\) converge vers une limite \\(l\\) telle que \\(l=al+b\\), soit \\(l=\\frac{b}{1-a}=\\frac{1-p}{2(1-p)}=\\frac{1}{2}\\). On a alors \\[p_{n+1}-\\frac{1}{2}=(2p-1)\\left(p_n-\\frac{1}{2}\\right)\\] donc la suite \\((p_n-l)_n\\) est géométrique de raison \\(2p-1\\). D’où \\(p_n-l=(2p-1)^n(p_0-l)\\), et donc \\[p_n=\\frac{1}{2}+\\frac{1}{2}(2p-1)^n\\] Commentaire. Le jour du concours, vous avez tout à fait le droit d’utiliser directement la formule explicitant le terme générique d’une suite arithmético-géométrique. A minima, si (comme moi…) vous ne le connaissez pas par coeur, il faut savoir la redémontrer rapidement en utilisant la méthode ci-dessus. 3. - si \\(p=1\\), alors la suite \\((p_n)_n\\) est constante égale à 1, donc \\(\\lim\\limits_{n\\to\\infty}p_n=1\\). si \\(p=0\\), alors \\(p_n=\\frac{1}{2}(1+(-1)^n)\\). Les suites extraites \\((p_{2n})_n\\) et \\((p_{2n+1})_n\\) convergent donc respectivement vers \\(1\\) et \\(0\\) (elles sont mêmes constantes), donc la suite \\((p_n)_n\\) admet deux valeurs d’adhérence distictes : elle ne converge pas. si \\(0&lt;p&lt;1\\), alors \\(-1&lt;2p-1&lt;1\\), et donc \\(\\lim\\limits_{n\\to\\infty}(2p-1)^n=0\\) : on en déduit que \\(\\lim\\limits_{n\\to\\infty}p_n=\\frac{1}{2}\\). Remarque. Ce dernier résultat nous dit que si d’une date à la date suivante l’information n’est ni presque sûrement transmise correctement, ni presque sûrement transmise de façon erronée, alors à une date asymptotitque il y a une chance sur deux qu’elle soit vraie. Un tel résultat était prévisible, car les événements \\(I_n\\) et \\(\\overline{I_n}\\) jouent des rôles parfaitement symétriques, et il n’y a donc aucune raison que l’un des deux soit asymptotiquement plus probable que l’autre. Exercice 2.9. Le gérant d’un magasin d’informatique a reçu un lot de clés USB. 5 % des boites sont abîmées. Le gérant estime que : 60 % des boites abîmées contiennent au moins une clé défectueuse ; 98 % des boites non abîmées ne contiennent aucune clé défectueuse. Un client achète une boite du lot. On désigne par \\(A\\) l’événement : la boite est abîmée ; \\(D\\) l’événement : la boite achetée contient au moins une clé défectueuse. 1. Donner les probabilités \\(\\mathbb{P}(A)\\), \\(\\mathbb{P}(\\overline{A})\\), \\(\\mathbb{P}(D\\vert A)\\), \\(\\mathbb{P}(D\\vert\\overline{A})\\), \\(\\mathbb{P}(\\overline{D}\\vert A)\\) et \\(\\mathbb{P}(\\overline{D}\\vert\\overline{A})\\). En déduire \\(\\mathbb{P}(D)\\). 2. Le client constate qu’une des clés achetées est défectueuse. Quelle est la probabilité pour qu’il ait acheté une boite abîmée ? Solution. 1. \\(\\mathbb{P}(A)=0,05\\) ; \\(\\mathbb{P}(\\overline{A})=0,95\\) d’après ce qui précède ; \\(\\mathbb{P}(D\\vert A)=0,6\\) ; \\(\\mathbb{P}(\\overline{D}\\vert A)=0,4\\) d’après ce qui précède ; \\(\\mathbb{P}(\\overline{D}\\vert\\overline{A})=0,98\\) ; On en déduit, d’après la formule des probabilités totales et la formule de Bayes, que \\[\\begin{align} \\mathbb{P}(D)&amp;=\\mathbb{P}(D\\vert A)\\mathbb{P}(A)+\\mathbb{P}(D\\vert\\overline{A})\\mathbb{P}(\\overline{A}) \\\\ &amp;=0,6\\times 0,05+0,02\\times 0,95 \\\\ &amp;\\approx 0,049 \\\\ \\end{align}\\] 2. On veut déterminer \\(\\mathbb{P}(A\\vert D)\\). D’après la formule de Bayes : \\[\\begin{align} \\mathbb{P}(A\\vert D)&amp;=\\frac{\\mathbb{P}(D\\vert A)\\mathbb{P}(A)}{\\mathbb{P}(D)} \\\\ &amp;\\approx\\frac{0,6\\times 0,05}{0,049} \\\\ &amp;\\approx 0,61 \\\\ \\end{align}\\] Exercice 2.10 (test de dépistage). Une maladie est présente dans la population, avec une prévalence d’une personne malade sur \\(10\\,000\\). Un nouveau test de dépistage vient d’être mis au point. On constate que : lorsqu’une personne est malade, le test est positif dans \\(99\\,\\%\\) des cas ; lorsqu’une personne n’est pas malade, le test est positif dans \\(0,1\\,\\%\\) des cas. Selon vous, ce test doit-il être commercialisé ? Solution. On note \\(M\\) et \\(P\\) les événements : \\(M\\) : la personne est malade ; \\(P\\) : le test est positif. On calcule \\(\\mathbb{P}(M\\vert P)\\) avec la fomrule de Bayes : \\[\\begin{align} \\mathbb{P}(M\\vert P)&amp;=\\frac{\\mathbb{P}(P\\vert M)\\mathbb{P}(M)}{\\mathbb{P}(P\\vert M)\\mathbb{P}(M)+\\mathbb{P}(P\\vert\\overline{M})\\mathbb{P}(\\overline{M})} \\\\ &amp;= \\frac{0,99\\times 0,0001}{0,99\\times 0,0001+0,001\\times 0,9999} \\\\ &amp;\\approx 0,09 \\end{align}\\] La probabilité qu’une personne dont le test est positif soit malade est donc seulement égale à \\(9\\,\\%\\). Il faut donc éviter de commercialiser ce test. Remarque. Ce résultat qui peut paraître surprenant s’explique par le fait que la maladie étudiée est très rare (\\(1\\) cas sur \\(10\\,000\\)), et donc même en cas de test positif la probabilité que la personne soit vraiment malade reste limitée. 9.2 Variables aléatoires discrètes Exercice 3.1. 1. Soit \\(X\\) une variable aléatoire discrète de support \\(X(\\Omega)=\\mathbb{N}^*\\) et telle que pour tout entier naturel \\(n\\geq 1\\) on ait \\[\\mathbb{P}(X=n)=\\frac{1}{2^n}\\] a. Vérifier que l’on définit bien une loi de probabilité. b. \\(X\\) admet-elle une espérance ? Si oui, la calculer. c. \\(X\\) admet-elle une variance ? Si oui, la calculer. 2. Soient \\(\\alpha\\) un réel et \\(Y\\) une variable aléatoire discrète de support \\(Y(\\Omega)=\\mathbb{N}\\) et telle que, pour tout entier naturel \\(n\\) on ait \\[\\mathbb{P}(Y=n)=\\frac{\\alpha}{n!}\\] a. Déterminer la valeur de \\(\\alpha\\) pour qu’on ait bien ainsi défini une loi de probabilité. b. \\(Y\\) admet-elle une espérance ? Si oui, la calculer. 3. \\(Z\\) une variable aléatoire discrète de support \\(Y(\\Omega)=\\mathbb{N}^*\\) et telle que, pour tout entier naturel \\(n\\geq 1\\) on ait \\[\\mathbb{P}(Z=n)=\\frac{1}{n(n+1)}\\] a. Vérifier que l’on définit bien une loi de probabilité. b. \\(Z\\) admet-elle une espérance ? Si oui, la calculer. Solution. 1.a. La série \\(\\sum\\limits_n \\frac{1}{2^n}\\) est convergente (série géométrique de raison \\(\\frac{1}{2}\\in\\left]-1\\,;\\,1\\right[\\)) et \\[\\begin{align} \\sum\\limits_{n=1}^{+\\infty}\\frac{1}{2^n}&amp;=\\sum\\limits_{n=0}^{+\\infty}\\frac{1}{2^n}-1 \\\\ &amp;=\\frac{1}{1-\\frac{1}{2}}-1 \\\\ &amp;=1 \\end{align}\\] On a donc bien défini une loi de probabilité. b. On commence par quelques rappels d’analyse, utiles pour cette question et la suivante : Rappels d’analyse. Pour cette question, comme pour la question suivante, on va passer par les séries dérivées de la série géométrique \\(\\sum\\limits_{n}x^n\\). Cette série est convergente sur \\(]-1\\,;\\,1[\\) et sa somme est indéfiniment dérivable sur cet intervalle. Les dérivées successives de cette dernière s’obtiennent alors par dérivations successives du terme général. Autrement dit, en posant, pour tout réel \\(x\\) dans \\(]-1\\,;\\,1[\\) : \\[f(x)=\\sum\\limits_{n=0}^{+\\infty}x^n\\] on a \\[\\begin{align} f(x)&amp;=\\sum\\limits_{n=0}^{+\\infty}x^n=\\frac{1}{1-x} \\\\ f&#39;(x)&amp;=\\sum\\limits_{n=0}^{+\\infty}n\\,x^{n-1}=\\frac{1}{(1-x)^2} \\\\ f&#39;&#39;(x)&amp;=\\sum\\limits_{n=0}^{+\\infty}n(n-1)\\,x^{n-2}=\\frac{2}{(1-x)^3} \\\\ \\end{align}\\] On montre maintenant que \\(X\\) admet une espérance. Cela revient à montrer que la série \\(\\sum\\limits_{n}\\frac{n}{2^{n}}\\) est convergente. Or, avec ce qui précède : \\[\\begin{align} f&#39;\\left(\\frac{1}{2}\\right)&amp;=\\sum\\limits_{n=0}^{+\\infty}\\frac{n}{2^{n-1}} \\\\ &amp;=\\sum\\limits_{n=1}^{+\\infty}\\frac{n}{2^{n-1}} \\\\ &amp;=\\frac{1}{\\left(1-\\frac{1}{2}\\right)^2} \\\\ &amp;=4 \\\\ \\end{align}\\] Donc, la série \\(\\sum\\limits_{n}\\frac{n}{2^{n}}\\) est bien convergente, et \\[\\begin{align} \\sum\\limits_{n=0}\\frac{n}{2^{n}}&amp;=\\frac{1}{2}\\sum\\limits_{n=0}^{+\\infty}\\frac{n}{2^{n-1}} \\\\ &amp;=2 \\end{align}\\] Ainsi, \\(X\\) admet une espérance et \\(\\mathbb{E}(X)=2\\). c. On obtient de même : \\[\\begin{align} f&#39;&#39;\\left(\\frac{1}{2}\\right)&amp;=\\sum\\limits_{n=2}^{+\\infty}\\frac{n(n-1)}{2^{n-2}} \\\\ &amp;=\\frac{2}{\\left(1-\\frac{1}{2}\\right)^2} \\\\ &amp;= 16 \\end{align}\\] La série \\(\\sum\\limits_{n}\\frac{n}{2^{n-2}}=2\\sum\\limits_{n}\\frac{n}{2^{n-1}}\\) converge et la somme \\(\\sum\\limits_{n=2}\\frac{n}{2^{n-2}}\\) vaut \\(2\\left(\\sum\\limits_{n=1}\\frac{n}{2^{n-1}}-1\\right)=6\\) (question précédente). Par conséquent, la série \\(\\sum\\limits_{n}\\frac{n^2}{2^{n}}\\) converge et \\[\\begin{align} \\sum\\limits_{n=1}^{+\\infty}\\frac{n^2}{2^{n}}&amp;=\\frac{1}{2}+\\sum\\limits_{n=2}^{+\\infty}\\frac{n^2}{2^{n}} \\\\ &amp;=\\frac{1}{2}+\\frac{1}{4}\\sum\\limits_{n=2}^{+\\infty}\\frac{n^2}{2^{n-2}} \\\\ &amp;=\\frac{1}{2}+\\frac{1}{4}\\left(\\sum\\limits_{n=2}^{+\\infty}\\frac{n(n-1)}{2^{n-2}}+\\sum\\limits_{n=2}^{+\\infty}\\frac{n}{2^{n-2}}\\right) \\\\ &amp;=\\frac{1}{2}+\\frac{1}{4}(16+6) \\\\ &amp;=6 \\\\ \\end{align}\\] Donc, \\(X\\) admet un moment d’ordre \\(2\\), et donc une variance, et \\(\\mathbb{E}(X^2)=6\\). On en déduit sa variance par la formule de Koenig-Huygens : \\[\\begin{align} \\mathbb{V}(X)&amp;=\\mathbb{E}(X^2)-\\mathbb{E}(X)^2 \\\\ &amp;=6-4 \\\\ &amp;=2 \\end{align}\\] Ainsi, \\(X\\) admet une variance et \\(\\mathbb{V}(X)=2\\). 2.a. La série \\(\\sum\\limits_{n}\\frac{\\alpha}{n!}\\) converge et \\(\\sum\\limits_{n=0}^{+\\infty}\\frac{\\alpha}{n!}=\\alpha\\,e\\). Pour qu’on ait bien défini une loi de probabilité, cette somme doit être égale à \\(1\\), ce qui impose de poser \\(\\alpha=\\frac{1}{e}\\). b. \\(Y\\) admet une espérance si et seulement si la série \\(\\sum\\limits_{n}n\\,\\frac{1}{e\\,n!}\\) est convergente. Or, pour tout entier naturel \\(N\\) : \\[\\begin{align} \\sum\\limits_{n=0}^N n\\,\\frac{1}{e\\,n!}&amp;=\\sum\\limits_{n=1}^N\\frac{1}{e\\,(n-1)!} \\\\ &amp;=\\frac{1}{e}\\sum\\limits_{n=0}^{N-1}\\frac{1}{n!} \\\\ &amp;\\underset{N\\to +\\infty}{\\longrightarrow} 1\\\\ \\end{align}\\] \\(Y\\) admet donc une espérance et \\(\\mathbb{E}(Y)=1\\). 3.a. Soit \\(N\\) un entier naturel tel que \\(N\\geq 1\\). On a : \\[\\begin{align} \\sum\\limits_{n=1}^N\\frac{1}{n(n+1)}&amp;=\\sum\\limits_{n=1}^N\\left(\\frac{1}{n}-\\frac{1}{n+1}\\right) \\\\ &amp;=\\sum\\limits_{n=1}^N\\frac{1}{n}-\\sum\\limits_{n=1}^N\\frac{1}{n+1} \\\\ &amp;=\\sum\\limits_{n=1}^N\\frac{1}{n}-\\sum\\limits_{n=2}^{N+1}\\frac{1}{n} \\\\ &amp;=1-\\frac{1}{N+1} \\\\ &amp;\\underset{N\\to +\\infty}{\\longrightarrow} 1 \\\\ \\end{align}\\] ce qui permet de conclure. Commentaires. i. La première égalité résulte d’une décomposition en éléments simples. Une fois qu’on l’a écrite, on peut se contenter, le jour du concours, d’évoquer une somme télescopique pour justifier directement le passage à l’expression \\(1-\\frac{1}{N+1}\\), sans écrire, donc, les deux lignes intermédiaires. Pour rappel, une somme télescopique est - à un changement éventuel de signe près - une somme du type \\(\\sum\\limits_{n=p}^q (a_{n+1}-a_{n})\\). Dans une telle somme, tous les termes s’annulent deux à deux à l’exception de ceux situés aux extremités. On a donc \\[\\sum\\limits_{n=p}^{q} (a_{n+1}-a_{n})=a_{q+1}-a_{p}\\] ii. Un exemple d’approche incorrecte. Il peut être tentant de dire que la série \\(\\sum\\limits_{n}\\frac{1}{n(n+1)}\\) est convergente (par exemple, son terme général est positif et majoré par \\(\\frac{1}{n^2}\\), qui est celui d’une série convergente) et d’écrire ensuite \\[\\begin{align} \\sum\\limits_{n=1}^{+\\infty}\\frac{1}{n(n+1)}&amp;=\\sum\\limits_{n=1}^{+\\infty}\\left(\\frac{1}{n}-\\frac{1}{n+1}\\right) \\\\ &amp;=\\sum\\limits_{n=1}^{+\\infty}\\frac{1}{n}-\\sum\\limits_{n=1}^{+\\infty}\\frac{1}{n+1} \\\\ \\end{align}\\] La première égalité est correcte, puisqu’on a juste remplacé \\(\\frac{1}{n(n+1)}\\) par \\(\\frac{1}{n}-\\frac{1}{n+1}\\), qui sont deux termes égaux. En revanche, l’égalité \\(\\sum\\limits_{n=1}^{+\\infty}\\left(\\frac{1}{n}-\\frac{1}{n+1}\\right)=\\sum\\limits_{n=1}^{+\\infty}\\frac{1}{n}-\\sum\\limits_{n=1}^{+\\infty}\\frac{1}{n+1}\\) est incorrecte à cause du terme de droite. Celui-ci correspond à la différence entre les sommes de deux séries divergentes (autrement dit, quelque chose du type \\((+\\infty)-(+\\infty))\\)). En l’état, il n’a donc a priori pas de sens et ne peut être écrit tel quel. On a ici utilisé (à tort) une formule de linéarité \\[\\sum\\limits_{n=p}^{+\\infty}(u_n+v_n)=\\sum\\limits_{n=p}^{+\\infty}u_n+\\sum\\limits_{n=p}^{+\\infty}v_n\\] mais l’utilisation d’une telle formule suppose de vérifier au préalable que tous les termes en jeu sont bien définis. Autrement dit, on doit non seulement s’assurer de la convergence de la série \\(\\sum\\limits_n(u_n+v_n)\\), mais aussi de celle des séries \\(\\sum\\limits_n u_n\\) et \\(\\sum\\limits_n v_n\\), et c’est justement la convergence de ces deux séries qui pose problème dans notre exemple. La seule approche correcte dans cette question est donc bien de se placer d’abord dans le cas fini en sommant de \\(1\\) à \\(N\\), d’utiliser ensuite la linéarité (qui est licite dans le cas fini), puis de terminer en faisant tendre \\(N\\) vers l’infini. b. \\(Z\\) admet une espérance (finie) si et seulement si la série \\(\\sum\\limits_n\\frac{n}{n(n+1)}=\\sum\\limits_n\\frac{1}{n+1}\\) est convergente. Or, cette série diverge (série harmonique), donc \\(Z\\) n’admet pas d’espérance (finie). Exercice 3.2 (à retenir). Soit \\(X\\) une variable aléatoire discrète à valeurs dans \\(\\mathbb{N}\\) et d’espérance finie. 1. Démontrer que, pour tout entier naturel \\(n\\), on a \\[\\sum\\limits_{k=0}^n\\mathbb{P}(X&gt;k)=\\sum\\limits_{k=0}^n k\\,\\mathbb{P}(X=k)+(n+1)\\,\\mathbb{P}(X&gt;n)\\] 2. En déduire que \\[\\mathbb{E}(X)=\\sum\\limits_{k=0}^{\\infty}\\mathbb{P}(X&gt;k)\\] Commentaire. Cette formule donne donc une autre expression de l’espérance pour une variable aléatoire (dans le cas général, elle peut être fausse). On peut presque la considérer comme du cours, donc il faut penser à l’utiliser. Solution. 1. On procède par récurrence sur \\(n\\in\\mathbb{N}\\). pour \\(n=0\\) : \\[\\begin{align} \\sum\\limits_{k=0}^0 k\\,\\mathbb{P}(X=k)+(0+1)\\,\\mathbb{P}(X&gt;0)&amp;=0.\\mathbb{P}(X=0)+\\mathbb{P}(X&gt;0) \\\\ &amp;=\\mathbb{P}(X&gt;0) \\\\ &amp;=\\sum\\limits_{k=0}^0\\mathbb{P}(X&gt;k) \\end{align}\\] donc l’égalité est vraie au rang \\(n=0\\). supposons maintenant cette égalité vraie au rang \\(n\\), pour un entier naturel \\(n\\) donné, et démontrons qu’elle est alors vraie au rang \\(n+1\\). On a donc \\[\\begin{align} \\sum\\limits_{k=0}^{n+1}\\mathbb{P}(X&gt;k)&amp;=\\sum\\limits_{k=0}^{n}\\mathbb{P}(X&gt;k)+\\mathbb{P}(X&gt;n+1) \\\\ &amp;=\\sum\\limits_{k=0}^{n}k\\,\\mathbb{P}(X=k)+(n+1)\\mathbb{P}(X&gt;n)+\\mathbb{P}(X&gt;n+1) \\\\ &amp;\\text{(par hypothèse de réccurence)} \\\\ &amp;=\\sum\\limits_{k=0}^{n}k\\,\\mathbb{P}(X=k)+(n+1)\\mathbb{P}(X=n+1)+(n+1)\\mathbb{P}(X&gt;n+1)+\\mathbb{P}(X&gt;n+1) \\\\ &amp;=\\sum\\limits_{k=0}^{n+1}k\\,\\mathbb{P}(X=k)+(n+2)\\mathbb{P}(X&gt;n+1) \\end{align}\\] ce qui permet de conclure. 2. D’après la question précédente, il suffit de démontrer que \\(\\lim\\limits_{n\\to +\\infty}(n+1)\\,\\mathbb{P}(X&gt;n)=0\\). Or, on a \\[\\begin{align} 0&amp;\\leq (n+1)\\,\\mathbb{P}(X&gt;n) \\\\ &amp;=\\sum\\limits_{k=n+1}^{+\\infty}(n+1)\\mathbb{P}(X=k) \\\\ &amp;\\leq\\sum\\limits_{k=n+1}^{+\\infty}\\mathbb{P}(X=k) \\\\ \\end{align}\\] Or, ce dernier terme est le reste de la série \\(\\sum\\limits_{n}n\\,\\mathbb{P}(X=n)\\), qui est une série convergente de somme \\(\\mathbb{E}(X)\\). Par conséquent, ce terme tend vers \\(0\\) lorsque \\(n\\) tend vers l’infini, et on conclut en appliquant le théorème des gendarmes. Commentaires. L’égalité de la question 2 a été établie à partir d’une égalité entre sommes sur des ensembles d’indices finis. Cette égalité a été démontrée par récurrence dans la question 1. Comme souvent, une démonstration par récurrence présente l’avantage de réduire essentiellement la question en un simple jeu d’écriture, ce qui peut s’avérer plus efficace le jour du concours. On présente dans ce qui suit une approche complètement différente, qui demande davantage d’intuition et est plus visuelle. On commence par présenter son heuristique, avant de formaliser l’idée principale en un raisonnement plus rigoureux. Méthode 2 (calcul direct). Heuristique. On peut partir de la définition de l’espérance \\[\\mathbb{E}(X)=\\sum\\limits_{i=1}^{+\\infty}i\\,\\mathbb{P}(X=i)\\] et écrire le terme général de cette série comme somme de \\(k\\) termes égaux : \\[i\\,\\mathbb{P}(X=i)=\\mathbb{P}(X=i)+\\mathbb{P}(X=i)+\\dots\\mathbb{P}(X=i)\\] On construit alors un tableau infini, dont la ligne numéro \\(i\\) contient exactement \\(i\\) termes non nuls, tous égaux à \\(\\mathbb{P}(X=i)\\) : Ainsi : en sommant tous les éléments de la ligne \\(i\\), on obtient \\(i\\,\\mathbb{P}(X=i)\\) ; en sommant toutes ces sommes par ligne, on obtient \\(\\sum\\limits_{i=0}^{+\\infty}i\\,\\mathbb{P}(X=i)\\), soit \\(\\mathbb{E}(X)\\). On regarde maintenant ce qui se passe si, plutôt que de sommer les sommes par lignes, on somme les sommes par colonnes : la somme des éléments de la colonne \\(j\\) est égale à \\(\\mathbb{P}(X=j)+\\mathbb{P}(X=j+1)+\\dots+\\mathbb{P}(X=n)+\\dots\\), soit encore \\(\\mathbb{P}(X&gt;j-1)\\). en sommant les sommes par colonnes, on obtient donc \\(\\sum\\limits_{j=1}^{+\\infty}\\mathbb{P}(X&gt;j-1)=\\sum\\limits_{j=0}^{+\\infty}\\mathbb{P}(X&gt;j)\\). On s’attend à ce que ces deux façons de calculer amènent au même résultat, ce qui se traduit exactement par l’égalité \\(\\mathbb{E}(X)=\\sum\\limits_{j=0}^{+\\infty}\\mathbb{P}(X&gt;j)\\). Il s’agit maintenant d’écrire tout cela un peu plus rigoureusement. Ce qui précède s’appuie sur une seule idée : pour sommer tous les éléments d’un tableau, on peut soit sommer les sommes par lignes, soit sommer les sommes par colonnes. Notons de manière générale \\(a_{i,j}\\) l’élément situé sur la ligne \\(i\\) et la colonne \\(j\\) du tableau, que l’on suppose dans un premier temps fini (à \\(p\\) lignes et \\(q\\) colonnes). La somme des sommes par lignes s’écrit alors \\(\\sum\\limits_{i=1}^p\\sum\\limits_{j=1}^q a_{i,j}\\) et la somme des sommes par colonnes s’écrit \\(\\sum\\limits_{j=1}^q\\sum\\limits_{i=1}^p a_{i,j}\\), si bien que dans le cas d’un tableau fini, l’équivalence entre ces deux procédés de calcul se traduit par l’égalité \\[\\sum\\limits_{i=1}^p\\sum\\limits_{j=1}^q a_{i,j}=\\sum\\limits_{j=1}^q\\sum\\limits_{i=1}^p a_{i,j}\\] Autrement dit, passer d’une sommation de sommes par lignes à une sommation de sommes par colonnes revient exactement à intervertir deux symboles \\(\\Sigma\\), ce qui dans le cas où les ensembles d’indices sur lesquels on somme sont tous finis est trivialement vrai (c’est une conséquence directe des propriétés de commutativité et d’associativité de l’addition dans \\(\\mathbb{R}\\)). Dans le cas infini, il faut être un peu plus prudent : l’égalité \\[\\sum\\limits_{i\\in I}\\sum\\limits_{j\\in J}a_{ij}=\\sum\\limits_{j\\in J}\\sum\\limits_{i\\in I}a_{ij}\\] n’est pas nécessairement vraie si l’un au moins des ensembles \\(I\\) et \\(J\\) est infini. Cependant, pour \\(I=J=\\mathbb{N}\\) on a le résultat suivant : Théorème de Fubini pour les séries. Soit \\((a_{m,n})_{(m,n)\\in\\mathbb{N}^2}\\) une suite à double indice. Si \\[\\sum\\limits_{i=0}^{+\\infty}\\sum\\limits_{j=0}^{+\\infty}|a_{i,j}|&lt;+\\infty\\] alors \\[\\sum\\limits_{i=0}^{+\\infty}\\sum\\limits_{j=0}^{+\\infty}a_{i,j}=\\sum\\limits_{i=0}^{+\\infty}\\sum\\limits_{j=0}^{+\\infty}a_{i,j}\\] (étant entendu que toutes les séries en jeu sont convergentes) Avec ce résultat, on peut maintenant rédiger une solution plus formalisée des idées précédentes. Solution formalisée. On pose \\(a_{i,j}=\\mathbb{P}(X=i).1_{j\\leq i}\\). On a \\[\\begin{align} \\sum\\limits_{i=1}^{+\\infty}\\sum\\limits_{j=1}^{+\\infty}|a_{i,j}|&amp;=\\sum\\limits_{i=1}^{+\\infty}\\sum\\limits_{j=1}^{+\\infty}\\mathbb{P}(X=i).1_{j\\leq i} \\\\ &amp;=\\sum\\limits_{i=1}^{+\\infty}\\sum\\limits_{j=1}^{i}\\mathbb{P}(X=i) \\\\ &amp;=\\sum\\limits_{i=1}^{+\\infty}i\\,\\mathbb{P}(X=i) \\\\ &amp;=\\mathbb{E}(X) \\\\ &amp;&lt;+\\infty \\end{align}\\] D’après le théorème de Fubini, la série double \\(\\sum\\limits_j\\sum\\limits_i\\mathbb{P}(X=i).1_{j\\leq i}\\) est donc convergente et \\[\\begin{align} \\mathbb{E}(X)&amp;=\\sum\\limits_{i=1}^{+\\infty}\\sum\\limits_{j=1}^{+\\infty}\\mathbb{P}(X=i).1_{j\\leq i} \\\\ &amp;=\\sum\\limits_{j=1}^{+\\infty}\\sum\\limits_{i=1}^{+\\infty}\\mathbb{P}(X=i).1_{j\\leq i} \\\\ &amp;=\\sum\\limits_{j=1}^{+\\infty}\\sum\\limits_{i=j}^{+\\infty}\\mathbb{P}(X=i) \\\\ &amp;=\\sum\\limits_{j=1}^{+\\infty}\\mathbb{P}(X\\geq j) \\\\ &amp;=\\sum\\limits_{j=0}^{+\\infty}\\mathbb{P}(X&gt; j) \\\\ \\end{align}\\] :::: Exercice 3.3. Soient \\(n\\in\\mathbb{N}^*\\) et \\(a\\in\\mathbb{R}\\). On considère une variable aléatoire discrète de support \\(\\{0,1,\\dots, n\\}\\) et telle que, pour tout entier \\(0\\leq k\\leq n\\) : \\[\\mathbb{P}(X=k)=\\frac{a}{k+1}\\binom{n}{k}\\] 1. Déterminer le réel \\(a\\). 2.a. Calculer les espérance \\(\\mathbb{E}(X+1)\\) et \\(\\mathbb{E}(X(X+1))\\). b. En déduire les valeurs de \\(\\mathbb{E}(X)\\) et \\(\\mathbb{V}(X)\\). Solution. 1. Pour déterminer le réel \\(a\\), on utilise l’égalité \\[\\sum\\limits_{k=0}^n\\mathbb{P}(X=k)=1\\] soit encore \\[a\\sum\\limits_{k=0}^n\\frac{\\binom{n}{k}}{k+1}=1\\] Or, on a l’égalité \\[\\frac{\\binom{n}{k}}{k+1}=\\frac{\\binom{n+1}{k+1}}{n+1}\\] Celle-ci a déjà été démontrée dans l’exercice 2.3. sur les coefficients binomiaux (question 1). On en déduit que \\[\\begin{align} a&amp;=\\frac{1}{\\sum\\limits_{k=0}^n\\frac{\\binom{n}{k}}{k+1}} \\\\ &amp;=\\frac{n+1}{\\sum\\limits_{k=0}^n\\binom{n+1}{k+1}} \\\\ &amp;=\\frac{n+1}{\\sum\\limits_{k=0}^{n+1}\\binom{n+1}{k}-1} \\\\ &amp;=\\frac{n+1}{2^{n+1}-1} \\end{align}\\] 2.a. D’après le théorème de transfert, on a \\[\\begin{align} \\mathbb{E}(X+1)&amp;=\\sum\\limits_{k=0}^n (k+1)\\,\\mathbb{P}(X=k) \\\\ &amp;=\\sum\\limits_{k=0}^n (k+1)\\,\\frac{a}{k+1}\\binom{n}{k} \\\\ &amp;=a\\sum\\limits_{k=0}^n\\binom{n}{k} \\\\ &amp;=a\\,2^n \\\\ &amp;=\\frac{n+1}{2^{n+1}-1}\\,2^n \\end{align}\\] De façon analogue, on a \\[\\begin{align} \\mathbb{E}(X(X+1))&amp;=\\sum\\limits_{k=0}^n (k+1)\\,\\mathbb{P}(X=k) \\\\ &amp;=\\sum\\limits_{k=0}^n (k+1)\\frac{a}{k+1}\\binom{n}{k} \\\\ &amp;=a\\sum\\limits_{k=0}^n k\\,\\binom{n}{k} \\\\ &amp;=a\\sum\\limits_{k=1}^n k\\,\\binom{n}{k} \\\\ &amp;=a\\,n\\sum\\limits_{k=1}^n \\binom{n-1}{k-1} \\\\ &amp;=a\\,n\\sum\\limits_{k=0}^{n-1} \\binom{n-1}{k} \\\\ &amp;=a\\, n2^{n-1} \\\\ &amp;=\\frac{n(n+1)}{2^{n+1}-1}\\,2^{n-1} \\\\ \\end{align}\\] b. On en déduit que \\[\\begin{align} \\mathbb{E}(X)&amp;=\\mathbb{E}(X+1)-1 \\\\ &amp;=\\frac{n+1}{2^{n+1}-1}\\,2^n-1 \\end{align}\\] et \\[\\begin{align} \\mathbb{V}(X)&amp;=\\mathbb{E}(X^2)-\\mathbb{E}(X)^2 \\\\ &amp;=\\mathbb{E}(X(X+1))-\\mathbb{E}(X)-\\mathbb{E}(X)^2 \\\\ &amp;=\\mathbb{E}(X(X+1))-\\mathbb{E}(X).(\\mathbb{E}(X)+1) \\\\ &amp;=\\frac{n(n+1)}{2^{n+1}-1}\\,2^{n+1}-\\left(\\frac{n+1}{2^{n+1}-1}2^n-1\\right)\\frac{n+1}{2^{n+1}-1}2^n \\end{align}\\] Commentaire. Pour calculer l’espérance et la variance de \\(X\\), on décide donc dans cet exercice de commencer par calculer les espérances de \\(X\\) et \\(X(X+1)\\). Ces calculs sont en effet plus simples car ils permettent, via le théorème de transfert, d’annuler le terme \\(\\frac{1}{k+1}\\) de la probabilité \\(\\mathbb{P}(X=k)\\). Une fois qu’on a calculé \\(\\mathbb{E}(X)\\) et \\(\\mathbb{E}(X(X+1))\\), on en déduit facilement, par linéarité de l’espérance, \\(\\mathbb{E}(X)\\) et \\(\\mathbb{V}(X)\\). Ici, l’exercice est guidé, mais on pourrait très bien imaginer une question d’un sujet du concours dans laquelle il est directement demandé de calculer ces deux quantités, sans les indications fournies par la question 2. Dans ce cas, c’est au candidat de prendre l’initiative d’introduire les variables \\(X+1\\) et \\(X(X+1)\\). De façon générale, retenir que pour calculer l’espérance (ou la variance) d’une variable aléatoire, il est parfois avantageux de calculer au préalable l’espérance (ou la variance) d’une transformation (simple) de cette variable. Exercice 3.4 (Somme de deux VA). Soient \\(X\\) et \\(Y\\) deux variables aléatoires indépendantes, à valeurs dans \\(\\mathbb{N}\\) et telles que pour tout entier naturel \\(k\\) : \\[\\mathbb{P}(X=k)=\\mathbb{P}(Y=k)=pq^k\\] où \\(q=1-p\\) et \\(p\\in ]0\\,;\\,1[\\). 1. Calculer \\(\\mathbb{P}(X=Y)\\). 2. Calculer \\(\\mathbb{P}(X&lt;Y)\\). 3. Déterminer la loi de \\(X+Y\\). 1. \\[\\begin{align} \\mathbb{P}(X=Y)&amp;=\\sum\\limits_{k=0}^{+\\infty}\\mathbb{P}(X=k, Y=k) \\\\ &amp;=\\sum\\limits_{k=0}^{+\\infty}\\mathbb{P}(X=k)\\,\\mathbb{P}(Y=k) \\\\ &amp;\\text{(par indépendance de } X \\text{ et } Y \\text{)} \\\\ &amp;=\\sum\\limits_{k=0}^{+\\infty}p^2 q^{2k} \\\\ &amp;=p^2\\,\\frac{1}{1-q^2} \\\\ &amp;=\\frac{p^2}{(1-q)(1+q)} \\\\ &amp;=\\frac{p^2}{p(1+q)} \\\\ &amp;=\\frac{p}{1+q} \\\\ \\end{align}\\] 2. \\(X\\) et \\(Y\\) jouent des rôles symétriques, donc \\(\\mathbb{P}(X&lt;Y)=\\mathbb{P}(Y&gt;X)\\), et ces deux probabilités sont alors égales à \\(\\frac{1-\\mathbb{P}(X=Y)}{2}\\), soit : \\[\\mathbb{P}(X&lt;Y)=\\mathbb{P}(Y&lt;X)=\\frac{q}{1+q}\\] 3. \\(X\\) et \\(Y\\) étant indépendantes, la loi de \\(X+Y\\) s’obtient par produit de convolution. Son support est \\((X+Y)(\\Omega)=\\mathbb{N}\\), et pour tout entier naturel \\(k\\), on a \\[\\begin{align} \\mathbb{P}(X+Y=k)&amp;=\\sum\\limits_{i=0}^k\\mathbb{P}(X=i)\\,\\mathbb{P}(Y=k-i) \\\\ &amp;=\\sum\\limits_{i=0}^k pq^i pq^{k-i} \\\\ &amp;=p^2\\sum\\limits_{i=0}^k q^k \\\\ &amp;=(k+1)p^2 q^k \\\\ \\end{align}\\] Commentaire. Bien faire attention ici aux supports des variables \\(X\\) et \\(Y\\). Comme \\(Y\\) est positive, les événements \\((X=k)\\) et \\((Y=k-i)\\) ne peuvent être réalisés que si \\(i\\geq 0\\) et \\(k-i\\geq 0\\), soit \\(0\\leq i\\leq k\\). Exercice 3.5. Soit \\(X\\) une variable aléatoire suivant la loi binomiale \\(\\mathcal{B}(n,p)\\), avec \\(n\\geq 2\\) et \\(0&lt;p&lt;1\\). Soit \\(Y\\) une variable aléatoire telle que : pour tout entier \\(k\\) tel que \\(1\\leq k\\leq n\\), la réalisation de l’événement \\((X=k)\\) entraîne celle de l’événement \\((Y=k)\\) ; la loi de \\(Y\\) sachant \\((X=0)\\) est la loi uniforme sur \\(\\{1,2,\\dots, n\\}\\). 1. Déterminer la loi de probabilité de \\(Y\\). 2. Calculer \\(\\mathbb{E}(Y)\\). 3. Déterminer la loi de probabilité conditionnelle \\(\\mathcal{L}(Y|X\\neq 0)\\). Solution. 1. On a \\[\\left \\{ \\begin{array}{rcl} Y|X=0\\sim\\mathcal{U}_{[\\![1;n]\\!]} \\\\ \\forall k\\in [\\![1;n]\\!],\\, (X=k)\\subset (Y=k) \\\\ \\end{array} \\right.\\] Soit $k\\([\\![1;n]\\!]\\), alors, d’après la formule des probabilités totales : \\[\\begin{align} \\mathbb{P}(Y=k)&amp;=\\sum\\limits_{i=0}^n\\mathbb{P}(Y=k, X=i) \\\\ &amp;=\\underbrace{\\mathbb{P}(Y=k|X=0)}_{=\\frac{1}{n}}\\underbrace{\\mathbb{P}(X=0)}_{=(1-p)^n}+\\sum\\limits_{i=1}^n\\underbrace{\\mathbb{P}(Y=k, X=i)}_{=\\left \\{ \\begin{array}{rcl} 0&amp;;\\, \\text{ si } i\\neq k \\\\ \\mathbb{P}(X=k)&amp;;\\,\\text{ si } i=k \\\\ \\end{array} \\right.} \\\\ &amp;=\\frac{1}{n}(1-p)^n+\\mathbb{P}(X=k) \\\\ &amp;=\\frac{1}{n}(1-p)^n+\\binom{n}{k}p^k(1-p)^{n-k} \\\\ \\end{align}\\] Montrons enfin que \\(Y\\) a pour support \\(Y(\\Omega)[\\![1;n]\\!]=\\) : \\[\\begin{align} \\sum\\limits_{k=1}^n\\mathbb{P}(Y=k)&amp;=(1-p)^n+\\sum\\limits_{k=1}^n\\binom{n}{k}p^k (1-p)^{n-k} \\\\ &amp;=\\sum\\limits_{k=0}^n\\binom{n}{k}p^k (1-p)^{n-k} \\\\ &amp;=1 \\\\ \\end{align}\\] car on a sommé les probabilités \\(\\mathbb{P}(Z=k)\\), \\(k\\in [\\![0;n]\\!]\\), où \\(Z\\sim\\mathcal{B}(n\\,;\\,p)\\). On a donc \\(Y(\\Omega)=[\\![1;n]\\!]\\), ce qui permet de conclure cette question. Commentaire : Pour traiter rigoureusement cette question, il est important de ne pas éluder la question du support. On peut, comme on l’a fait, calculer \\(\\sum\\limits_{k=1}^n\\mathbb{P}(Y=k)\\), et conclure, si cette somme est bien égale à \\(1\\), que le support est égal à \\([\\![1;n]\\!]\\). Une autre approche aurait consisté à démontrer que \\(\\mathbb{P}(Y=x)=0\\) pour \\(x\\not\\in [\\![1;n]\\!]\\). 2. On peut écrire, pour tout \\(k\\in [\\![1;n]\\!]\\), \\(\\mathbb{P}(Y=k)=\\frac{1}{n}(1-p)^n+\\mathbb{P}(Z=k)\\), avec \\(Z\\sim\\mathcal{B}(n\\,;\\,p)\\). Donc : \\[\\begin{align} \\sum\\limits_{k=1}^n k\\mathbb{P}(Y=k)&amp;=\\frac{1}{n}(1-p)^n\\sum\\limits_{k=1}^n k+\\sum\\limits_{k=1}^n k\\mathbb{P}(Z=k) \\\\ &amp;=\\frac{n+1}{2}(1-p)^n+\\mathbb{E}(Z) \\\\ &amp;=\\frac{n+1}{2}(1-p)^n+np \\end{align}\\] 3. Conditionnellement à \\((X\\neq 0)\\), \\(Y\\) prend ses valeurs dans \\([\\![1;n]\\!]\\), et pour tout \\(k\\in [\\![1;n]\\!]\\) on a : \\[\\begin{align} \\mathbb{P}(Y=k|X\\neq 0)&amp;=\\frac{\\mathbb{P}(Y=k, X\\neq 0)}{\\mathbb{P}(X\\neq 0)} \\\\ &amp;=\\frac{\\mathbb{P}(Y=k)-\\mathbb{P}(Y=k, X=0)}{1-\\mathbb{P}(X=0)} \\\\ &amp;=\\frac{\\mathbb{P}(Y=k)-\\mathbb{P}(Y=k| X=0)\\mathbb{P}(X=0)}{1-\\mathbb{P}(X=0)} \\\\ &amp;=\\frac{\\frac{1}{n}(1-p)^n+\\binom{n}{k}p^k (1-p)^{n-k}-(1-p)^n\\frac{1}{n}}{1-(1-p)^n} \\\\ &amp;=\\frac{\\binom{n}{k}p^k (1-p)^{n-k}}{1-(1-p)^n} \\end{align}\\] Exercice 3.6. Une urne contient des boules numérotées \\(1\\), \\(2\\) et \\(3\\). On tire \\(n\\) boules une à une avec remise. Soient \\(X\\) la variable aléatoire représentant le plus petit des numéros obtenus et \\(Y\\) la variable aléatoire représentant le plus grand. 1. Calculer \\(\\mathbb{P}(X\\geq i)\\) pour \\(i\\in\\{1,2,3\\}\\). En déduire la loi de \\(X\\). 2. Calculer \\(\\mathbb{P}(Y\\leq j)\\) pour tout \\(j\\in\\{1,2,3\\}\\). En déduire la loi de \\(Y\\). 3. Les variables aléatoires \\(X\\) et \\(Y\\) sont-elles indépendantes ? Commentaire général sur cet exercice. Les questions 1 et 2 sont ultra classiques dans les sujets de probas-stat du concours. Il s’agit de calculer la probabilité que le \\(\\max\\) ou le \\(\\min\\) de \\(n\\) variables aléatoires indépendantes soient inférieures ou supérieures à une certaine valeur. Dans ce cas, le schéma est toujours le même : on exprime l’événement sur le max ou le min comme une intersection d’événements ; on utilise l’indépendance pour justifier que la probabilité de l’intersection est le produit des probabilités ; si de plus les variables aléatoires sont identiquement distribuées (elles sont alors i.i.d.) on utilise le fait que toutes ces probabilités sont égales.. Solution. 1. Pour \\(k\\in [\\![1;n]\\!]\\), on note \\(X_k\\in\\{1,2,3\\}\\) le numéro obtenu lors du tirage numéro \\(k\\). Ces tirages étant effectués avec remise, les variables \\(X_k\\) sont indépendantes. On a alors \\[\\left \\{ \\begin{array}{rcl} X&amp;=\\min(X_1,\\dots, X_n) \\\\ Y&amp;=\\max(X_1,\\dots X_n)\\\\ \\end{array} \\right.\\] \\(X\\) et \\(Y\\) sont à valeurs dans \\(\\{1,2,3\\}\\). Pour \\(i\\in\\{1,2,3\\}\\), on a \\[\\begin{align} \\mathbb{P}(X\\geq i)&amp;=\\mathbb{P}\\left(\\min(X_1,\\dots X_n)\\geq i \\right) \\\\ &amp;=\\mathbb{P}\\left(\\bigcap_{k=1}^n (X_k\\geq i) \\right) \\\\ &amp;=\\prod\\limits_{k=1}^n\\mathbb{P}(X_k\\geq i) \\\\ &amp;\\text{(par indépendance)} \\\\ &amp;=\\left(1-\\frac{i-1}{3}\\right)^n \\\\ &amp;=\\left(\\frac{4-i}{3}\\right)^n \\\\ \\end{align}\\] On a alors \\[\\begin{align} \\mathbb{P}(X=1)&amp;=\\mathbb{P}(X\\geq 1)-\\mathbb{P}(X\\geq 2) \\\\ &amp;=1-\\left(\\frac{2}{3}\\right)^n \\end{align}\\] \\[\\begin{align} \\mathbb{P}(X=2)&amp;=\\mathbb{P}(X\\geq 2)-\\mathbb{P}(X\\geq 3) \\\\ &amp;=\\left(\\frac{2}{3}\\right)^n-\\frac{1}{3^n} \\\\ &amp;=\\frac{2^n-1}{3^n} \\\\ \\end{align}\\] \\[\\begin{align} \\mathbb{P}(X=3)&amp;=\\mathbb{P}(X\\geq 3) \\\\ &amp;=\\frac{1}{3^n} \\end{align}\\] On vérifie de façon immédiate qu’on a bien \\(\\sum\\limits_{k=1}^3\\mathbb{P}(X=k)=1\\). Commentaire. On pourrait aussi calculer \\(\\mathbb{P}(X=3)\\) en utilisant la formule \\(\\mathbb{P}(X=3)=1-\\mathbb{P}(X=1)-\\mathbb{P}(X=2)\\). Cependant, dans cet exemple une telle approche ne rendrait pas le calcul plus rapide et présenterait de plus l’inconvénient de masquer d’éventuelles erreurs sur les calculs préalables de \\(\\mathbb{P}(X=1)\\) et \\(\\mathbb{P}(X=2)\\). 2. Soit \\(j\\in [\\![1;3]\\!]\\). On a \\[\\begin{align} \\mathbb{P}(Y\\leq j)&amp;=\\mathbb{P}\\left(\\max(X_1,\\dots X_n)\\leq j \\right) \\\\ &amp;=\\mathbb{P}\\left(\\bigcap_{k=1}^n (X_k\\leq j) \\right) \\\\ &amp;=\\prod\\limits_{k=1}^n\\mathbb{P}(X_k\\leq j) \\\\ &amp;\\text{(par indépendance)} \\\\ &amp;=\\left(\\frac{j}{3}\\right)^n \\\\ \\end{align}\\] pour tout \\(j\\in[\\![1;3]\\!]\\) on a donc \\[\\begin{align} \\mathbb{P}(Y=j)&amp;=\\mathbb{P}(Y\\leq j)-\\mathbb{P}(Y\\leq j-1) \\\\ &amp;=\\left(\\frac{j}{3}\\right)^n-\\left(\\frac{j-1}{3}\\right)^n \\end{align}\\] Commentaires. i. Si on veut être un tout petit peu plus explicite, on peut préciser que cette dernière expression est valable pour \\(j=2,3\\), mais qu’elle l’est aussi a posteriori pour \\(j=1\\) car \\(\\mathbb{P}(Y\\leq 1-1)=0=\\left(\\frac{1-1}{3}\\right)^n\\). ii. Dans la question précédente, on aurait très bien pu écrire le résultat de \\(\\mathbb{P}(X=i)\\) sous une forme générique comme on le fait ici. iii. Modulo un échange entre les symboles \\(\\min\\) et \\(\\max\\) d’une part, et \\(\\geq\\) et \\(\\leq\\) d’autre part, les questions 1 et 2 sont identiques. Dès lors qu’on a traité la question 1 soigneusement, on peut certainement se permettre de gagner un peu de temps et donner directement (si on le voit !) le résultat de la question 2, en évoquant par exemple un “raisonnement analogue” à celui de la question 1. De façon générale, il faut se dire qu’une rédaction efficace doit : - convaincre le correcteur qu’on a bien compris ce qu’on faisait et qu’on sait rédiger de façon rigoureuse (l’un impliquant généralement l’autre) ; - éviter les redites afin de gagner un peu de temps, sans trop sacrifier le point précédent. Sur un type de raisonnement que l’on a déjà rédigé proprement lors d’une question précédente, on peut probablement aller un peu plus vite sans s’attirer les foudres de la personne qui corrige la copie… 3. On a \\(X\\leq Y\\), donc \\(\\mathbb{P}(X=2, Y=1)=0\\neq\\mathbb{P}(X=2)\\mathbb{P}(Y=1)\\). Les variables \\(X\\) et \\(Y\\) ne sont donc pas indépendantes. Exercice 3.7. Soient \\(X\\) et \\(Y\\) deux variables aléatoires de Bernoulli indépendantes, de même paramètre \\(p\\in ]0\\,;\\,1[\\). On note \\(U=X+Y\\) et \\(V=X-Y\\). 1. Déterminer la loi du couple \\((U,V)\\). 2. Calculer le coefficient de corrélation \\(\\rho_{U,V}\\). 3. \\(U\\) et \\(V\\) sont-elles indépendantes ? Que peut-on conclure ? Solution. 1. \\(U(\\Omega)=[\\![0;2]\\!]\\) et \\(V(\\Omega)=[\\![-1;1]\\!]\\). Par ailleurs : \\[(U,V)(\\Omega)=\\left\\{(0;0),\\, (1;-1),\\,(1;0),\\,(2;0)\\right\\}\\] et, par indépendance de \\(X\\) et \\(Y\\) : \\[\\begin{align} \\mathbb{P}(U=0,V=0)&amp;=\\mathbb{P}(X=0,Y=0) \\\\ &amp;=\\mathbb{P}(X=0)\\mathbb{P}(Y=0) \\\\ &amp;=(1-p)^2 \\\\ \\end{align}\\] \\[\\begin{align} \\mathbb{P}(U=1,V=-1)&amp;=\\mathbb{P}(X=0,Y=1) \\\\ &amp;=\\mathbb{P}(X=0)\\mathbb{P}(Y=1) \\\\ &amp;=(1-p)p\\\\ \\end{align}\\] \\[\\begin{align} \\mathbb{P}(U=1,V=1)&amp;=\\mathbb{P}(X=1,Y=0) \\\\ &amp;=\\mathbb{P}(X=1)\\mathbb{P}(Y=0) \\\\ &amp;=p(1-p) \\\\ \\end{align}\\] \\[\\begin{align} \\mathbb{P}(U=2,V=0)&amp;=\\mathbb{P}(X=1,Y=1) \\\\ &amp;=\\mathbb{P}(X=1)\\mathbb{P}(Y=1) \\\\ &amp;=p^2 \\\\ \\end{align}\\] 2. Commentaire. La question précédente incite à utiliser la loi du couple \\((U,V)\\) pour : calculer \\(\\mathbb{E}(UV)\\) ; calculer les lois marginales de \\(U\\) et \\(V\\) et en déduire \\(\\mathbb{E}(U)\\), \\(\\mathbb{E}(V)\\), \\(\\sigma_U\\) et \\(\\sigma_V\\) ; en déduire enfin \\(cov(U,V)\\), puis \\(\\rho_{U,V}\\). Mais tout cela serait assez long, et il est bien plus efficace ici d’utiliser les propriétés de bilinéarité et de symétrie de la covariance… \\[\\begin{align} \\text{cov}(U,V)&amp;=\\text{cov}(X+Y,X-Y) \\\\ &amp;=\\mathbb{V}(X)-\\text{cov}(X,Y)+\\text{cov}(Y,X)-\\mathbb{V}(Y) \\\\ &amp;=\\mathbb{V}(X)-\\mathbb{V}(Y) \\\\ &amp;=0 \\end{align}\\] et donc \\[\\rho_{U,V}=0\\] 3. \\(\\mathbb{P}(U=2,V=1)=0\\), car \\((2,1)\\not\\in (U,V)(\\Omega)\\). Mais \\(\\mathbb{P}(U=2)\\mathbb{P}(V=1)\\neq 0\\) car aucun de ces facteurs n’est nul : \\[\\begin{align} \\mathbb{P}(U=2)&amp;=\\mathbb{P}(U=2,V=0) \\\\ &amp;=p^2 \\\\ &amp;\\neq 0 \\text{ car } p&gt;0 \\\\ \\end{align}\\] \\[\\begin{align} \\mathbb{P}(V=1)&amp;=\\mathbb{P}(U=1, V=1) \\\\ &amp;=p(1-p) \\\\ &amp;\\neq 0 \\text{ car } p\\not\\in\\{0,1\\} \\\\ \\end{align}\\] Les variables \\(U\\) et \\(V\\) ne sont donc pas indépendantes. On a donc construit un exemple de deux variables aléatoires non corrélées mais non indépendantes. Exercice 3.8. Soient \\(X\\) et \\(Y\\) deux variables aléatoires à valeurs dans \\(\\mathbb{N}\\). On donne la loi du couple \\((X,Y)\\) : \\[\\forall (i,j)\\in\\mathbb{N}^2, \\, \\mathbb{P}(X=i, Y=j)=\\frac{(i+j)\\lambda^{i+j}}{e\\,i!\\,j!}\\] 1. Déterminer \\(\\lambda\\). Donner la loi de \\(X\\). 2. Les variables \\(X\\) et \\(Y\\) sont-elles indépendantes ? Solution. 1. Heuristique. Pour déterminer \\(\\lambda\\), on doit résoudre l’équation \\[\\sum\\limits_{i=0}^{+\\infty}\\sum\\limits_{j=0}^{+\\infty}\\frac{(i+j)\\lambda^{i+j}}{e\\,i!j!}=1\\] Résoudre cette équation suppose d’étudier la convergence de la série double \\(\\sum\\limits_i\\sum\\limits_j\\frac{(i+j)\\lambda^{i+j}}{e\\,i! j!}\\). Comme souvent quand on a à montrer la convergence d’une série, le plus simple est de manipuler celle-ci au brouillon comme si on savait déjà qu’elle convergeait. De telles manipulations amènent souvent à voir émerger les conditions de sa convergence, et il ne reste plus alors qu’à transformer la suite de calculs informels en une démonstration rigoureuse. Ici, on a envie d’écrire : \\[\\sum\\limits_{i=0}\\sum\\limits_{j=0}\\frac{(i+j)\\lambda^{i+j}}{e\\,i!j!}=\\frac{1}{e}\\left(\\sum\\limits_{i=0}^{+\\infty}\\frac{i\\lambda^i}{i!}\\sum\\limits_{j=0}^{+\\infty}\\frac{\\lambda^{j}}{j!}+\\sum\\limits_{i=0}^{+\\infty}\\frac{\\lambda^i}{i!}\\sum\\limits_{j=0}^{+\\infty}\\frac{j\\lambda^{j}}{j!}\\right)\\] soit encore \\[\\sum\\limits_{i=0}\\sum\\limits_{j=0}\\frac{(i+j)\\lambda^{i+j}}{e\\,i!j!}=\\frac{1}{e}\\left(\\sum\\limits_{i=1}^{+\\infty}\\frac{\\lambda^i}{(i-1)!}\\sum\\limits_{j=0}^{+\\infty}\\frac{\\lambda^{j}}{j!}+\\sum\\limits_{i=0}^{+\\infty}\\frac{\\lambda^i}{i!}\\sum\\limits_{j=1}^{+\\infty}\\frac{\\lambda^{j}}{(j-1)!}\\right)\\] On ne peut pas écrire ces égalités telles quelles, justement car on ignore si la série relative au membre de gauche est convergente. En revanche, toutes les séries relatives au membre de droite sont bien convergentes pour tout réel \\(\\lambda\\), car toutes sont égales, à un éventuel facteur près, à la série \\(\\sum\\limits_i\\frac{\\lambda^i}{i!}\\), dont la somme vaut \\(\\sum\\limits_{=0}\\frac{\\lambda^i}{i!}=e^{\\lambda}\\). Mais puisque les séries du membre de droite sont convergentes, leur somme l’est aussi, autrement dit le membre de gauche est convergent, et donc par là même les égalités ci-dessus sont bien vraies. On rédige maintenant notre solution. Solution rédigée. Pour tout réel \\(\\lambda\\), la série \\(\\sum\\limits_i\\frac{\\lambda^i}{i!}\\) converge et \\(\\sum\\limits_{i=0}\\frac{\\lambda^i}{i!}=e^{\\lambda}\\). Donc, la série \\[\\sum\\limits_i\\sum\\limits_j\\frac{(i+j)\\lambda^{i+j}}{e\\,i!j!}=\\frac{1}{e}\\left(\\sum\\limits_{i}^{+\\infty}\\frac{\\lambda^i}{(i-1)!}\\sum\\limits_{j}^{+\\infty}\\frac{\\lambda^{j}}{j!}+\\sum\\limits_{i=0}^{+\\infty}\\frac{\\lambda^i}{i!}\\sum\\limits_{j=1}^{+\\infty}\\frac{\\lambda^{j}}{(j-1)!}\\right)\\] est convergente, et \\[\\begin{align} \\sum\\limits_{i=0}\\sum\\limits_{j=0}\\frac{(i+j)\\lambda^{i+j}}{e\\,i!j!}&amp;=\\frac{1}{e}\\left(\\sum\\limits_{i=1}^{+\\infty}\\frac{\\lambda^i}{(i-1)!}\\sum\\limits_{j=0}^{+\\infty}\\frac{\\lambda^{j}}{j!}+\\sum\\limits_{i=0}^{+\\infty}\\frac{\\lambda^i}{i!}\\sum\\limits_{j=1}^{+\\infty}\\frac{\\lambda^{j}}{(j-1)!}\\right) \\\\ &amp;=\\frac{1}{e}2\\lambda e^{2\\lambda} \\end{align}\\] On cherche donc un réel \\(\\lambda\\) tel que \\[2\\lambda e^{2\\lambda}=e\\] La fonction \\(x\\mapsto x e^x\\) est dérivable sur \\(\\mathbb{R}\\), et pour tout réel \\(x\\) on a \\(f&#39;(x)=(1+x)e^x\\). On en déduit que \\(f&#39;&lt;0\\) - et donc \\(f\\) est strictement décroissante - sur \\(]-\\infty\\,;\\,-1[\\) et \\(f&#39;&gt;0\\) - et donc \\(f\\) est strictement croissante - sur \\(]-1\\,;\\,+\\infty[\\). Comme par ailleurs \\(\\lim\\limits_{x\\to -\\infty}f(x)=0\\) et \\(\\lim\\limits_{x\\to +\\infty}f(x)=+\\infty\\), l’équation \\(f(x)=e\\) admet une unique solution sur \\(\\mathbb{R}\\). Or, \\(f(1)=e\\), et donc cette unique solution est \\(\\lambda=\\frac{1}{2}\\). Déterminons maintenant la loi de \\(X\\). Soit \\(i\\in\\mathbb{N}\\). On a \\[\\begin{align} \\mathbb{P}(X=i)&amp;=\\sum\\limits_{j=0}^{+\\infty}\\mathbb{P}(X=i, Y=j) \\\\ &amp;=\\sum\\limits_{j=0}^{+\\infty}\\frac{i+j}{e\\,i!j!}\\frac{1}{2^{i+j}} \\\\ &amp;=\\frac{i}{e\\,i!2^i}\\sum\\limits_{j=0}^{+\\infty}\\frac{1}{j!}\\frac{1}{2^j}+\\frac{1}{e\\,i!2^i}\\sum\\limits_{j=0}^{+\\infty}\\frac{j}{j!}\\frac{1}{2^j} \\\\ &amp;=\\frac{i}{e\\,i! 2^i}e^{\\frac{1}{2}}+\\frac{1}{e\\,i!2^i.2}e^{\\frac{1}{2}} \\\\ &amp;=\\frac{2i+1}{\\sqrt{e}\\,2^{i+1}\\,i!} \\\\ \\end{align}\\] 2. Par symétrie des rôles joués par \\(X\\) et \\(Y\\), on a \\[\\forall j\\in\\mathbb{N}, \\, \\mathbb{P}(Y=j)=\\frac{2j+1}{\\sqrt{e}\\,2^{j+1}\\,j!}\\] On en déduit que \\[\\mathbb{P}(X=0)\\mathbb{P}(Y=0)=\\frac{1}{4e}\\] alors que \\[\\mathbb{P}(X=0, Y=0)=0\\] Ainsi, \\(\\mathbb{P}(X=0, Y=0)\\neq\\mathbb{P}(X=0)\\mathbb{P}(Y=0)\\), et donc les variables aléatoires \\(X\\) et \\(Y\\) ne sont pas indépendantes. Exercice 3.9. Soient \\(X\\) et \\(Y\\) deux variables aléatoires indepéndantes de loi de Poisson de paramètres respectifs \\(\\lambda\\) et \\(\\mu\\). Déterminer la loi conditionnelle de \\(X\\) lorsque la somme \\(S=X+Y\\) a une valeur finie \\(S=s\\). En déduire l’expression de la fonction de régression de \\(X\\) sur \\(S\\) puis la valeur de \\(\\mathbb{E}(\\mathbb{E}(X|S))\\). Solution. \\(S(\\Omega)=\\mathbb{N}\\), on cherche donc la loi \\(\\mathcal{L}(X|X+Y=s)\\) pour \\(s\\in\\mathbb{N}\\). Conditionnellement à l’événement \\((X+Y=s)\\), \\(X\\) est à valeurs dans \\([\\![0\\,;\\,s]\\!]\\). Soit \\(k\\in[\\![0\\,;\\,s]\\!]\\), calculons \\(\\mathbb{P}(X=k|X+Y=s)\\). On a : \\[\\begin{align} \\mathbb{P}(X=k|X+Y=s)&amp;=\\frac{\\mathbb{P}(X=k, X+Y=s)}{\\mathbb{P}(X+Y=s)} \\\\ &amp;=\\frac{\\mathbb{P}(X=k, Y=s-k)}{\\mathbb{P}(X+Y=s)} \\\\ &amp;=\\frac{\\mathbb{P}(X=k)\\mathbb{P}(Y=s-k)}{\\mathbb{P}(X+Y=s)} \\\\ &amp;\\text{(car } X \\text{ et } Y \\text{ sont indépendantes)} \\\\ \\end{align}\\] On utilise le résultat classique suivant : Somme de deux lois de Poisson indépendantes. Soient \\(\\lambda\\) et \\(\\mu\\) deux réels strictement positifs et \\(X\\) et \\(Y\\) deux variables indépendantes telles que \\[X\\sim\\mathcal{P}(\\lambda)\\] \\[Y\\sim\\mathcal{P}(\\mu)\\] Alors : \\[X+Y\\sim\\mathcal{P}(\\lambda+\\mu)\\] Démonstration. \\(X+Y(\\Omega)=\\mathbb{N}\\). Comme \\(X\\) et \\(Y\\) sont indépendantes, la loi de \\(X+Y\\) est donnée par le produit de convolution entre \\(\\mathbb{P}_X\\) et \\(\\mathbb{P}_Y\\) : \\[\\mathbb{P}_{X+Y}=\\mathbb{P}_X*\\mathbb{P}_Y\\] Soit \\(k\\in\\mathbb{N}\\). On a donc \\[\\begin{align} \\mathbb{P}(X+Y=k)&amp;=\\sum\\limits_{i\\in\\mathbb{N}}\\mathbb{P}(X=i)\\,\\mathbb{P}(Y=k-i) \\\\ &amp;=\\sum\\limits_{i=0}^k\\mathbb{P}(X=i)\\,\\mathbb{P}(Y=k-i) \\\\ &amp;=\\sum\\limits_{i=0}^k\\frac{\\lambda^i}{i!}e^{-\\lambda}\\frac{\\mu^{k-i}}{(k-i)!}e^{-\\mu} \\\\ &amp;=e^{-(\\lambda+\\mu)}\\mu^k\\sum\\limits_{i=0}^k\\left(\\frac{\\lambda}{\\mu}\\right)^i\\frac{1}{i!}\\frac{1}{(k-i)!} \\\\ &amp;=e^{-(\\lambda+\\mu)}\\frac{\\mu^k}{k!}\\sum\\limits_{i=0}^k\\binom{k}{i}\\left(\\frac{\\lambda}{\\mu}\\right)^i \\\\ &amp;=e^{-(\\lambda+\\mu)}\\frac{\\mu^k}{k!}\\left(1+\\frac{\\lambda}{\\mu}\\right)^k \\\\ &amp;=e^{-(\\lambda+\\mu)}\\frac{(\\lambda+\\mu)^k}{k!} \\\\ \\end{align}\\] et on donc on a bien \\(X+Y\\sim\\mathcal{P}(\\lambda+\\mu)\\). \\(\\square\\) Commentaire. Ce résultat peut être considéré comme du cours. En pratique, il faut donc le connaître, mais aussi savoir le démontrer. On revient au calcul de \\(\\mathbb{P}(X=k|X+Y=s)\\). On a donc : \\[\\begin{align} \\mathbb{P}(X=k|X+Y=s)&amp;=\\frac{\\mathbb{P}(X=k)\\mathbb{P}(Y=s-k)}{\\mathbb{P}(X+Y=s)} \\\\ &amp;=\\frac{\\lambda^k e^{-\\lambda}\\,\\mu^{s-k} e^{-\\mu}}{k!\\,(s-k)!\\,\\frac{(\\lambda+\\mu)^s}{s!}e^{-(\\lambda+\\mu)}} \\\\ &amp;=\\frac{s!}{k!\\,(s-k)!}\\,\\frac{\\lambda^k\\,\\mu^{s-k}}{(\\lambda+\\mu)^s} \\\\ &amp;=\\binom{s}{k}\\left(\\frac{\\lambda}{\\lambda+\\mu}\\right)^k\\,\\left(\\frac{\\mu}{\\lambda+\\mu}\\right)^{s-k} \\\\ &amp;=\\binom{s}{k}p^k\\,(1-p)^{s-k}\\\\ \\end{align}\\] où \\(p=\\frac{\\lambda}{\\lambda+\\mu}\\). Ainsi, on a \\[X|X+Y=s\\sim\\mathcal{B}\\left(s\\,;\\,\\frac{\\lambda}{\\lambda+\\mu}\\right)\\] On en déduit que \\[\\forall s\\in\\mathbb{N},\\,\\mathbb{E}(X|X+Y=s)=\\frac{\\lambda}{\\lambda+\\mu}s\\] Par conséquent, \\(\\mathbb{E}(X|S)\\) est la variable aléatoire \\[\\mathbb{E}(X|S)=\\frac{\\lambda}{\\lambda+\\mu}S\\] Elle admet une espérance, et \\[\\begin{align} \\mathbb{E}\\left(\\mathbb{E}(X|S)\\right)&amp;=\\frac{\\lambda}{\\lambda+\\mu}\\mathbb{E}(S) \\\\ &amp;=\\frac{\\lambda}{\\lambda+\\mu}(\\lambda+\\mu) \\\\ &amp;=\\lambda \\end{align}\\] 9.3 Variables à densité Exercice 4.1. Soit \\(X\\) une variable aléatoire suivant une loi exponentielle de paramètre \\(\\frac{1}{2}\\). 1.a. Déterminer une densité de \\(Y=\\sqrt{X}\\). b. Déterminer l’espérance et la variance de \\(Y\\). 2. Déterminer une densité de \\(Z=X^2\\). Solution. Dans tout cet exercice, pour toute variable aléatoire \\(U\\) à densité, on notera \\(f_U\\) sa fonction de densité et \\(F_U\\) sa fonction de répartition. 1.a. \\(X\\) a pour densité la fonction \\(f_X\\) définie par \\[\\forall x\\in\\mathbb{R},\\,f_X(x)=\\frac{1}{2}e^{-\\frac{x}{2}}.\\mathbb{1}_{\\mathbb{R}_+}(x)\\] et pour fonction de répartition la fonction \\(F_X\\) définie par \\[\\forall x\\in\\mathbb{R},\\,F_x(x)=\\left(1-e^{-\\frac{x}{2}}\\right).\\mathbb{1}_{\\mathbb{R}_+}(x)\\] Soit \\(y\\) un réel. Si \\(y&lt;0\\) alors \\(F_Y(y)=0\\). Si \\(y\\geq 0\\), alors \\[\\begin{align} F_Y(y)&amp;=\\mathbb{P}(Y\\leq y) \\\\ &amp;=\\mathbb{P}(\\sqrt{X}\\leq y) \\\\ &amp;=\\mathbb{P}(0\\leq X\\leq y^2) \\\\ &amp;=F_X(y^2) \\text{ car } F_X(0)=0 \\\\ &amp;=1-e^{-\\frac{y^2}{2}} \\\\ \\end{align}\\] \\(F_Y\\) est dérivable sur \\(\\mathbb{R}_+^*\\) et donc \\(Y\\) admet une densité donnée par \\(f_Y=F_Y&#39;\\) : \\[\\begin{align} f_Y(y)&amp;=F_Y&#39;(y) \\\\ &amp;=y.e^{-\\frac{y^2}{2}} \\end{align}\\] D’où l’expression générale de \\(f_Y(y)\\), quel que soit le réel \\(y\\) : \\[f_y(y)=y.e^{-\\frac{y^2}{2}}.\\mathbb{1}_{\\mathbb{R}_+}(y)\\] b. \\(Y\\) admet une espérance si et seulement si l’intégrale \\(\\int_0^{+\\infty}y^2e^{-\\frac{y^2}{2}}\\,dy\\) est convergente. Or, si \\(U\\) suit une loi normale standard, la variance de \\(U\\) s’écrit comme l’intégrale \\(\\int_{-\\infty}^{+\\infty}\\frac{1}{\\sqrt{2\\pi}}y^2e^{-\\frac{y^2}{2}}\\,dy\\), autrement dit cette intégrale vaut \\(1\\). On en déduit que \\(Y\\) admet une espérance et que \\[\\mathbb{E}(Y)=\\sqrt{\\frac{\\pi}{2}}\\] De même, \\(Y\\) admet un moment d’ordre \\(2\\), et donc une variance, si et seulement si l’intégrale \\(\\int_{0}^{+\\infty}y^3e^{-\\frac{y^2}{2}}\\,dy\\) est convergente. Soit \\(A&gt;0\\). On a \\[\\int_0^A y^3e^{-\\frac{y^2}{2}}\\,dy=\\int_0^A y^2.\\left(ye^{-\\frac{y^2}{2}}\\right)\\,dy\\] Les fonctions \\(y\\mapsto y^2\\) et \\(y\\mapsto ye^{-\\frac{y^2}{2}}\\) sont de classe \\(\\mathcal{C}^1\\) sur le segment \\([0\\,;\\,A]\\), ce qui va nous permettre d’effectuer une intégration par parties : on dérive \\(u:y\\mapsto y^2\\) : \\(u&#39;(y)=2y\\) ; on intègre \\(v&#39;:y\\mapsto ye^{-\\frac{y^2}{2}}\\) en posant \\(v(y)=-e^{-\\frac{y^2}{2}}\\). On en déduit que \\[\\begin{align} \\int_0^A y^3e^{-\\frac{y^2}{2}}\\,dy&amp;=\\left[-y^2e^{-\\frac{y^2}{2}}\\right]_0^A+2\\int_0^A ye^{-\\frac{y^2}{2}}\\, dy \\\\ &amp;=-A^2e^{-\\frac{A^2}{2}}+2-2e^{-\\frac{A^2}{2}} \\\\ &amp;\\longrightarrow_{A\\to +\\infty} 2 \\end{align}\\] Ainsi, \\(Y\\) admet un moment d’ordre \\(2\\) et \\[\\mathbb{E}(Y^2)=2\\] D’après la formule de König-Huygens, on a alors \\[\\begin{align} \\mathbb{V}(Y)&amp;=\\mathbb{E}(Y^2)-\\mathbb{E}(Y)^2 \\\\ &amp;=2-\\frac{\\pi}{2} \\\\ \\end{align}\\] Commentaire. Dans le cadre du programme, l’intégration par parties s’applique sur un segment. Pour une intégrale impropre comme celle de cette question, on fixe donc une (ou deux) borne(s) finie(s), on transforme l’intégrale sur le segment constitué par ces deux bornes par intégration par parties, et seulement après on fait tendre cette (ou ces) borne(s) vers la (les) valeur(s) souhaitée(s). 2. \\(Z(\\Omega)=\\mathbb{R}_+\\) et, pour \\(z\\geq 0\\) on a \\[\\begin{align} F_Z(z)&amp;=\\mathbb{P}(Z\\leq z) \\\\ &amp;=\\mathbb{P}(X^2\\leq z) \\\\ &amp;=\\mathbb{P}\\left(0\\leq X\\leq\\sqrt{Z}\\right) \\\\ &amp;=F_X\\left(\\sqrt{z}\\right) \\, \\text{ car } F_X(0)=0 \\\\ &amp;=1-e^{-\\frac{\\sqrt{z}}{2}} \\\\ \\end{align}\\] \\(F_Z\\) est dérivable sur \\(\\mathbb{R}_+^*\\) et donc \\(Z\\) admet une densité, donnée par \\(f_Z=F_Z&#39;\\). Pour tout \\(z\\geq 0\\) on a \\[\\begin{align} f_Z(z)&amp;=F_Z&#39;(z) \\\\ &amp;=\\frac{1}{4\\sqrt{z}}e^{-\\frac{\\sqrt{z}}{2}} \\\\ \\end{align}\\] On a donc \\[f_Z(z)=\\frac{1}{4\\sqrt{z}}e^{-\\frac{\\sqrt{z}}{2}}.1_{\\mathbb{R}_+}(z)\\] Commentaire sur cet exercice. Dans la question 1.a., tout comme dans la question 2., on cherche la densité d’une transformation \\(\\varphi(X)\\) d’une variable aléatoire \\(X\\) dont on connaît la loi. Ce type de question revient très fréquemment dans les sujets de concours. La stratégie qui fonctionne souvent le mieux est de calculer d’abord la fonction de répartition, puis de la dériver afin d’obtenir la fonction de densité. Pour pouvoir appliquer cette méthode, on doit : s’assurer que \\(\\varphi\\) est strictement monotone sur \\(X(\\Omega)\\) : si c’est bien le cas, alors en particulier elle réalise une bijection de \\(\\Omega\\) sur \\(X(\\Omega)\\) ; calculer son inverse \\(\\varphi^{-1}\\) sur \\(X(\\Omega)\\). En pratique, ces deux points sont assez souvent immédiats, comme c’est le cas dans cet exercice. Alors, on peut transformer le calcul de la probabilité \\(\\mathbb{P}(\\varphi(X)\\leq a)\\) en \\(\\mathbb{P}(X\\leq\\varphi^{-1}(\\{a\\}))\\) si \\(X\\) est strictement croissante sur \\(X(\\Omega)\\) ; \\(\\mathbb{P}(X\\geq\\varphi^{-1}(\\{a\\}))\\) si \\(X\\) est strictement décroissante sur \\(X(\\Omega)\\). Exercice 4.2. Soient \\(X\\) et \\(U\\) deux variables aléatoires indépendantes, où \\(X\\sim\\mathcal{N}(0,1)\\) et \\(U\\sim\\mathcal{U}_{\\{-1,1\\}}\\). On définit \\(Y=UX\\). 1. Démontrer que, pour tout réel \\(x\\) : \\[\\mathbb{P}(Y\\leq x)=\\frac{1}{2}\\mathbb{P}(X\\leq x)+\\frac{1}{2}\\mathbb{P}(X\\geq -x)\\] 2. En déduire que \\(Y\\) suit une loi normale centrée réduite. 3. Calculer l’espérance de \\(U\\) puis démontrer que \\(\\mathbb{E}(XY)=0\\). 4. En déduire que \\(\\text{Cov}(X,Y)=0\\). Solution. 1. Comme \\(\\left\\{(U=-1), (U=1)\\right\\}\\) est un sysème complet d’événements, pour tout réel \\(x\\) on a, d’après la formule des probabilités totales : \\[\\begin{align} \\mathbb{P}(Y\\leq x)&amp;=\\mathbb{P}(Y\\leq x, U=1)+\\mathbb{P}(Y\\leq x, U=-1) \\\\ &amp;=\\mathbb{P}(Y\\leq x|U=1)\\,\\mathbb{P}(U=1)+\\mathbb{P}(Y\\leq x|U=-1)\\,\\mathbb{P}(U=-1) \\\\ &amp;=\\mathbb{P}(UX\\leq x|U=1)\\mathbb{P}(U=1)+\\mathbb{P}(UX\\leq x|U=-1)\\mathbb{P}(U=-1) \\\\ &amp;=\\mathbb{P}(X\\leq x|U=1).\\frac{1}{2}+\\mathbb{P}(X\\geq -x|U=-1).\\frac{1}{2} \\\\ &amp;=\\frac{1}{2}\\mathbb{P}(X\\leq x)+\\frac{1}{2}\\mathbb{P}(X\\geq -x) \\\\ &amp;\\text{(car } U \\text{ et } X \\text{ sont indépendantes)} \\end{align}\\] 2. On a donc, pour tout réel \\(x\\) : \\[F_Y(x)=\\frac{1}{2}F_X(x)+\\frac{1}{2}\\left(1-F_X(-x)\\right)\\] Comme \\(X\\) suit la loi normale centrée réduite, sa fonction de répartition vérifie la propriété de symétrie suivante : \\[\\forall x\\in\\mathbb{R}, F_X(x)=1-F_X(-x)\\] Avec la question précédente, on en déduit que \\(F_Y=F_X\\), autrement dit \\(Y\\) suit également la loi normale centrée réduite. 3. On a : \\[\\begin{align} \\mathbb{E}(U)&amp;=-1.\\mathbb{P}(U=-1)+1.\\mathbb{P}(U=1) \\\\ &amp;=-\\frac{1}{2}+\\frac{1}{2} \\\\ &amp;=0 \\\\ \\end{align}\\] On en déduit que \\[\\begin{align} \\mathbb{E}(XY)&amp;=\\mathbb{E}(X^2U) \\\\ &amp;=\\mathbb{E}(X^2)\\mathbb{E}(U) \\\\ &amp;\\text{(par indépendance de } X \\text{ et } \\text{U)} \\\\ &amp;=0 \\\\ \\end{align}\\] 4. Comme \\(\\mathbb{E}(X)=0\\) et \\(\\mathbb{E}(XY)=0\\), on obtient de façon immédiate que \\[\\begin{align} \\text{cov}(X,Y)&amp;=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y) \\\\ &amp;=0 \\\\ \\end{align}\\] Exercice 4.3. 1. Soient \\(X\\) et \\(Y\\) deux variables aléatoires indépendantes de loi commune \\(\\mathcal{E}(\\lambda)\\), avec \\(\\lambda&gt;0\\). Déterminer une densité de \\(X+Y\\). 2. Plus généralement, soient \\(X_1,\\dots,X_n\\) des variables aléatoires indépendantes de même loi \\(\\mathcal{E}(\\lambda)\\). Pour tout entier \\(n\\geq 1\\), on note \\[S_n=\\sum\\limits_{k=1}^n X_k\\] Démontrer qu’une densité de \\(S_n\\) est donnée par \\[f_n(x)=\\left \\{ \\begin{array}{c @{} c} \\frac{\\lambda^n}{(n-1)!}\\,x^{n-1}\\,e^{-\\lambda x} &amp; \\text{ si } x\\geq 0 \\\\ 0 &amp; \\text{ si }\\lambda&lt;0 \\end{array} \\right.\\] Exercice 4.4. Soient \\(X\\) et \\(Y\\) deux variables indépendantes de loi normale centrée réduite (de densité \\(\\varphi\\) et de fonction de répartition \\(\\Phi\\)). On pose \\[Z=\\text{sup}(X,Y)\\] 1.a. Démontrer que \\(Z\\) est une variable à densité. b. Vérifier que \\(Z\\) admet pour densité \\(f\\) définie, pour tout réel \\(x\\), par \\(f(x)=2\\,\\varphi(x)\\,\\Phi(x)\\). 2.a. Justifier la convergence et donner la valeur de \\(\\int_{-\\infty}^{+\\infty}e^{-t^2}\\,dt\\). b. En remarquant que \\(\\varphi&#39;(x)=-x\\,\\varphi(x)\\), démontrer que \\[\\int_{0}^{+\\infty}x\\,f(x)\\,dx=\\frac{1}{\\sqrt{2\\pi}}+\\frac{1}{\\pi}\\int_{0}^{+\\infty}e^{-x^2}\\,dx\\] c. Vérifier de même que \\[\\int_{-\\infty}^{0}x\\,f(x)\\,dx=-\\frac{1}{\\sqrt{2\\pi}}+\\frac{1}{\\pi}\\int_{-\\infty}^{0}e^{-x^2}\\,dx\\] En déduire que \\(Z\\) admet une espérance et donner sa valeur. 3.a. Démontrer que \\(X^2\\) et \\(Z^2\\) suivent la même loi. b. Déterminer \\(\\mathbb{E}(X^2)\\) puis donner la valeur de \\(\\mathbb{V}(Z)\\). Exercice 4.5. Soit \\((X,Y)\\) un couple de variables aléatoires, de densité \\[f(x,y)=\\left \\{ \\begin{array}{c @{} c} \\frac{k}{\\sqrt{xy}} &amp; \\text{ si } 0&lt;x\\leq y&lt;1 \\\\ 0 &amp; \\text{ sinon } \\end{array} \\right.\\] 1. Déterminer la valeur de \\(k\\) ainsi que la fonction de répartition du couple \\((X,Y)\\). 2. Déterminer les lois marginales de \\(X\\) et \\(Y\\). Ces variables sont-elles indépendantes ? 3. Déterminer les lois conditionnelles de \\(X|Y=y\\) et de \\(Y|X=x\\). En déduire l’expression de la fonction de régression \\(x\\mapsto\\mathbb{E}(Y|X=x)\\), puis calculer \\(\\mathbb{E}(\\mathbb{E}(Y|X))\\). 9.4 Annales des oraux 9.4.1 Interne, 2016 Solution. 1. \\[\\begin{align} \\mathbb{P}(Y&gt;X) &amp;= \\mathbb{E}(1_{Y&gt;X}) \\\\ &amp;= \\int_{\\mathbb{R}_+^2} 1_{y&gt;x}\\lambda e^{-\\lambda x}\\lambda e^{-\\lambda y} \\, \\text{dy dx (par indépendance)} \\\\ &amp;= \\lambda\\int_{0}^{+\\infty}e^{-\\lambda x}\\int_{x}^{+\\infty}\\lambda e^{-\\lambda y}\\,\\text{dy dx} \\\\ &amp;= \\int_{0}^{+\\infty} e^{-\\lambda x}\\,\\lambda e^{-\\lambda x}\\,\\text{dx} \\\\ &amp;= \\int_{0}^{+\\infty} e^{-\\lambda y} f_{\\lambda}(y)\\text{ dy} \\text{ (où } f_{\\lambda} \\text{ est la densité de la loi } \\mathcal{E}(\\lambda) \\text{)} \\\\ &amp;= \\mathbb{E}(e^{-\\lambda X}) \\\\ \\end{align}\\] "]]
